<!DOCTYPE html>
<!-- START: inst/pkgdown/templates/layout.html --><!-- Generated by pkgdown: do not edit by hand --><html lang="en" data-bs-theme="auto">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<title>Intro to AWS SageMaker for Common ML/AI Procedures: All in One View</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="../assets/themetoggle.js"></script><link rel="stylesheet" type="text/css" href="../assets/styles.css">
<script src="../assets/scripts.js" type="text/javascript"></script><!-- mathjax --><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      config: ["MMLorHTML.js"],
      jax: ["input/TeX","input/MathML","output/HTML-CSS","output/NativeMML", "output/PreviewHTML"],
      extensions: ["tex2jax.js","mml2jax.js","MathMenu.js","MathZoom.js", "fast-preview.js", "AssistiveMML.js", "a11y/accessibility-menu.js"],
      TeX: {
        extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
      },
      tex2jax: {
        inlineMath: [['\\(', '\\)']],
        displayMath: [ ['$$','$$'], ['\\[', '\\]'] ],
        processEscapes: true
      }
    });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><!-- Responsive Favicon for The Carpentries --><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="manifest" href="../site.webmanifest">
<link rel="mask-icon" href="../safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" media="(prefers-color-scheme: light)" content="white">
<meta name="theme-color" media="(prefers-color-scheme: dark)" content="black">
</head>
<body>
    <header id="top" class="navbar navbar-expand-md top-nav incubator"><svg xmlns="http://www.w3.org/2000/svg" class="d-none"><symbol id="check2" viewbox="0 0 16 16"><path d="M13.854 3.646a.5.5 0 0 1 0 .708l-7 7a.5.5 0 0 1-.708 0l-3.5-3.5a.5.5 0 1 1 .708-.708L6.5 10.293l6.646-6.647a.5.5 0 0 1 .708 0z"></path></symbol><symbol id="circle-half" viewbox="0 0 16 16"><path d="M8 15A7 7 0 1 0 8 1v14zm0 1A8 8 0 1 1 8 0a8 8 0 0 1 0 16z"></path></symbol><symbol id="moon-stars-fill" viewbox="0 0 16 16"><path d="M6 .278a.768.768 0 0 1 .08.858 7.208 7.208 0 0 0-.878 3.46c0 4.021 3.278 7.277 7.318 7.277.527 0 1.04-.055 1.533-.16a.787.787 0 0 1 .81.316.733.733 0 0 1-.031.893A8.349 8.349 0 0 1 8.344 16C3.734 16 0 12.286 0 7.71 0 4.266 2.114 1.312 5.124.06A.752.752 0 0 1 6 .278z"></path><path d="M10.794 3.148a.217.217 0 0 1 .412 0l.387 1.162c.173.518.579.924 1.097 1.097l1.162.387a.217.217 0 0 1 0 .412l-1.162.387a1.734 1.734 0 0 0-1.097 1.097l-.387 1.162a.217.217 0 0 1-.412 0l-.387-1.162A1.734 1.734 0 0 0 9.31 6.593l-1.162-.387a.217.217 0 0 1 0-.412l1.162-.387a1.734 1.734 0 0 0 1.097-1.097l.387-1.162zM13.863.099a.145.145 0 0 1 .274 0l.258.774c.115.346.386.617.732.732l.774.258a.145.145 0 0 1 0 .274l-.774.258a1.156 1.156 0 0 0-.732.732l-.258.774a.145.145 0 0 1-.274 0l-.258-.774a1.156 1.156 0 0 0-.732-.732l-.774-.258a.145.145 0 0 1 0-.274l.774-.258c.346-.115.617-.386.732-.732L13.863.1z"></path></symbol><symbol id="sun-fill" viewbox="0 0 16 16"><path d="M8 12a4 4 0 1 0 0-8 4 4 0 0 0 0 8zM8 0a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 0zm0 13a.5.5 0 0 1 .5.5v2a.5.5 0 0 1-1 0v-2A.5.5 0 0 1 8 13zm8-5a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2a.5.5 0 0 1 .5.5zM3 8a.5.5 0 0 1-.5.5h-2a.5.5 0 0 1 0-1h2A.5.5 0 0 1 3 8zm10.657-5.657a.5.5 0 0 1 0 .707l-1.414 1.415a.5.5 0 1 1-.707-.708l1.414-1.414a.5.5 0 0 1 .707 0zm-9.193 9.193a.5.5 0 0 1 0 .707L3.05 13.657a.5.5 0 0 1-.707-.707l1.414-1.414a.5.5 0 0 1 .707 0zm9.193 2.121a.5.5 0 0 1-.707 0l-1.414-1.414a.5.5 0 0 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .707zM4.464 4.465a.5.5 0 0 1-.707 0L2.343 3.05a.5.5 0 1 1 .707-.707l1.414 1.414a.5.5 0 0 1 0 .708z"></path></symbol></svg><a class="visually-hidden-focusable skip-link" href="#main-content">Skip to main content</a>
  <div class="container-fluid top-nav-container">
    <div class="col-md-8">
      <div class="large-logo">
        <img id="incubator-logo" alt="Learn how to run common AI/ML procedures (train, tune, etc.) using AWS SageMaker. These examples focus on narrow &amp;quot;predictive ML/AI&amp;quot; cases, where models are trained to perform a single function (contrasing with &amp;quot;foundation&amp;quot; model use via AWS Bedrock). These materials are directed towards participants of the 2024 Machine Learning Marathon, and some instructions may pertain only to that group. A more general purpose version of this workshop will be made available in future months." src="../assets/images/incubator-logo.svg"><span class="badge text-bg-danger">
          <abbr title="This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.">
            <a href="https://cdh.carpentries.org/the-lesson-life-cycle.html#early-development-pre-alpha-through-alpha" class="external-link alert-link">
              <i aria-hidden="true" class="icon" data-feather="alert-octagon" style="border-radius: 5px"></i>
              Pre-Alpha
            </a>
            <span class="visually-hidden">This lesson is in the pre-alpha phase, which means that it is in early development, but has not yet been taught.</span>
          </abbr>
        </span>

      </div>
    </div>
    <div class="selector-container">
      <div id="theme-selector">
        <li class="nav-item dropdown" id="theme-button-list">
          <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
            <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="bd-theme-text">
<li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                Light
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                Dark
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
            <li>
              <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                Auto
                <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
            </li>
          </ul>
</li>
      </div>

      <div class="dropdown" id="instructor-dropdown">
        <button class="btn btn-secondary dropdown-toggle bordered-button" type="button" id="dropdownMenu1" data-bs-toggle="dropdown" aria-expanded="false">
          <i aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View <i data-feather="chevron-down"></i>
        </button>
        <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
<li><button class="dropdown-item" type="button" onclick="window.location.href='../aio.html';">Learner View</button></li>
        </ul>
</div>
    </div>
  </div>
  <hr></header><nav class="navbar navbar-expand-xl bottom-nav incubator" aria-label="Main Navigation"><div class="container-fluid nav-container">
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle Navigation">
      <span class="navbar-toggler-icon"></span>
      <span class="menu-title">Menu</span>
    </button>
    <div class="nav-logo">
      <img class="small-logo" alt="Learn how to run common AI/ML procedures (train, tune, etc.) using AWS SageMaker. These examples focus on narrow &amp;quot;predictive ML/AI&amp;quot; cases, where models are trained to perform a single function (contrasing with &amp;quot;foundation&amp;quot; model use via AWS Bedrock). These materials are directed towards participants of the 2024 Machine Learning Marathon, and some instructions may pertain only to that group. A more general purpose version of this workshop will be made available in future months." src="../assets/images/incubator-logo-sm.svg">
</div>
    <div class="lesson-title-md">
      Intro to AWS SageMaker for Common ML/AI Procedures
    </div>
    <div class="search-icon-sm">
      <!-- TODO: do not show until we have search
        <i role="img" aria-label="Search the All In One page" data-feather="search"></i>
      -->
    </div>
    <div class="desktop-nav">
      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item">
          <span class="lesson-title">
            Intro to AWS SageMaker for Common ML/AI Procedures
          </span>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/key-points.html">Key Points</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/instructor-notes.html">Instructor Notes</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../instructor/images.html">Extract All Images</a>
        </li>
        <li class="nav-item dropdown">
          <button class="nav-link dropdown-toggle" id="navbarDropdown" data-bs-toggle="dropdown" aria-expanded="false">
            More <i data-feather="chevron-down"></i>
          </button>
          <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
<hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
          </ul>
</li>
      </ul>
</div>
    <!--
    <form class="d-flex col-md-2 search-form">
      <fieldset disabled>
      <input class="form-control me-2 searchbox" type="search" placeholder="" aria-label="">
        <button class="btn btn-outline-success tablet-search-button"  type="submit">
          <i class="search-icon" data-feather="search" role="img" aria-label="Search the All In One page"></i>
        </button>
      </fieldset>
    </form>
    -->
    <a id="search-button" class="btn btn-primary" href="../instructor/aio.html" role="button" aria-label="Search the All In One page">Search the All In One page</a>
  </div>
<!--/div.container-fluid -->
</nav><div class="col-md-12 mobile-title">
  Intro to AWS SageMaker for Common ML/AI Procedures
</div>

<aside class="col-md-12 lesson-progress"><div style="width: %" class="percentage">
    %
  </div>
  <div class="progress incubator">
    <div class="progress-bar incubator" role="progressbar" style="width: %" aria-valuenow="" aria-label="Lesson Progress" aria-valuemin="0" aria-valuemax="100">
    </div>
  </div>
</aside><div class="container">
      <div class="row">
        <!-- START: inst/pkgdown/templates/navbar.html -->
<div id="sidebar-col" class="col-lg-4">
  <div id="sidebar" class="sidebar">
      <nav aria-labelledby="flush-headingEleven"><button role="button" aria-label="close menu" alt="close menu" aria-expanded="true" aria-controls="sidebar" class="collapse-toggle" data-collapse="Collapse " data-episodes="Episodes ">
          <i class="search-icon" data-feather="x" role="img"></i>
        </button>
        <div class="sidebar-inner">
          <div class="row mobile-row" id="theme-row-mobile">
            <div class="col" id="theme-selector">
              <li class="nav-item dropdown" id="theme-button-list">
                <button class="btn btn-link nav-link px-0 px-lg-2 dropdown-toggle d-flex align-items-center" id="bd-theme" type="button" aria-expanded="false" data-bs-toggle="dropdown" data-bs-display="static" aria-label="Toggle theme (auto)">
                  <svg class="bi my-1 theme-icon-active"><use href="#circle-half"></use></svg><span class="d-lg-none ms-1" id="bd-theme-text">Toggle Theme</span>
                </button>
                <ul class="dropdown-menu dropdown-menu-right" aria-labelledby="bd-theme-text">
<li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="light" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#sun-fill"></use></svg>
                      Light
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center" data-bs-theme-value="dark" aria-pressed="false">
                      <svg class="bi me-2 theme-icon"><use href="#moon-stars-fill"></use></svg>
                      Dark
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                  <li>
                    <button type="button" class="btn dropdown-item d-flex align-items-center active" data-bs-theme-value="auto" aria-pressed="true">
                      <svg class="bi me-2 theme-icon"><use href="#circle-half"></use></svg>
                      Auto
                      <svg class="bi ms-auto d-none"><use href="#check2"></use></svg></button>
                  </li>
                </ul>
</li>
            </div>
          </div>
          <div class="row mobile-row">
            <div class="col">
              <div class="sidenav-view-selector">
                <div class="accordion accordion-flush" id="accordionFlush9">
                  <div class="accordion-item">
                    <h2 class="accordion-header" id="flush-headingNine">
                      <button class="accordion-button collapsed" id="instructor" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseNine" aria-expanded="false" aria-controls="flush-collapseNine">
                        <i id="eye" aria-hidden="true" class="icon" data-feather="eye"></i> Instructor View
                      </button>
                    </h2>
                    <div id="flush-collapseNine" class="accordion-collapse collapse" aria-labelledby="flush-headingNine" data-bs-parent="#accordionFlush2">
                      <div class="accordion-body">
                        <a href="../aio.html">Learner View</a>
                      </div>
                    </div>
                  </div>
<!--/div.accordion-item-->
                </div>
<!--/div.accordion-flush-->
              </div>
<!--div.sidenav-view-selector -->
            </div>
<!--/div.col -->

            <hr>
</div>
<!--/div.mobile-row -->

          <div class="accordion accordion-flush" id="accordionFlush11">
            <div class="accordion-item">

              <button id="chapters" class="accordion-button show" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseEleven" aria-expanded="false" aria-controls="flush-collapseEleven">
                <h2 class="accordion-header chapters" id="flush-headingEleven">
                  EPISODES
                </h2>
              </button>
              <div id="flush-collapseEleven" class="accordion-collapse show collapse" aria-labelledby="flush-headingEleven" data-bs-parent="#accordionFlush11">

                <div class="accordion-body">
                  <div class="accordion accordion-flush" id="accordionFlush1">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading1">
        <a href="index.html">Summary and Schedule</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush2">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading2">
        <a href="SageMaker-overview.html">1. Overview of Amazon SageMaker</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush3">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading3">
        <a href="Data-storage-setting-up-S3.html">2. Data Storage: Setting up S3</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush4">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading4">
        <a href="SageMaker-notebooks-as-controllers.html">3. Notebooks as Controllers</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush5">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading5">
        <a href="Accessing-S3-via-SageMaker-notebooks.html">4. Accessing and Managing Data in S3 with SageMaker Notebooks</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush6">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading6">
        <a href="Interacting-with-code-repo.html">5. Using a GitHub Personal Access Token (PAT) to Push/Pull from a SageMaker Notebook</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

<div class="accordion accordion-flush" id="accordionFlush7">
  <div class="accordion-item">
    <div class="accordion-header" id="flush-heading7">
        <a href="Training-models-in-SageMaker-notebooks.html">6. Training models in SageMaker notebooks</a>
    </div>
<!--/div.accordion-header-->

  </div>
<!--/div.accordion-item-->
</div>
<!--/div.accordion-flush-->

                </div>
              </div>
            </div>

            <hr class="half-width">
<div class="accordion accordion-flush lesson-resources" id="accordionFlush12">
              <div class="accordion-item">
                <h2 class="accordion-header" id="flush-headingTwelve">
                  <button class="accordion-button collapsed" id="lesson-resources" type="button" data-bs-toggle="collapse" data-bs-target="#flush-collapseTwelve" aria-expanded="false" aria-controls="flush-collapseTwelve">
                    RESOURCES
                  </button>
                </h2>
                <div id="flush-collapseTwelve" class="accordion-collapse collapse" aria-labelledby="flush-headingTwelve" data-bs-parent="#accordionFlush12">
                  <div class="accordion-body">
                    <ul>
<li>
                        <a href="../instructor/key-points.html">Key Points</a>
                      </li>
                      <li>
                        <a href="../instructor/instructor-notes.html">Instructor Notes</a>
                      </li>
                      <li>
                        <a href="../instructor/images.html">Extract All Images</a>
                      </li>
                      <hr>
<li><a class="dropdown-item" href="reference.html">Reference</a></li>
                    </ul>
</div>
                </div>
              </div>
            </div>
            <hr class="half-width lesson-resources">
<a href="../instructor/aio.html">See all in one page</a>


            <hr class="d-none d-sm-block d-md-none">
<div class="d-grid gap-1">

            </div>
          </div>
<!-- /div.accordion -->
        </div>
<!-- /div.sidebar-inner -->
      </nav>
</div>
<!-- /div.sidebar -->
  </div>
<!-- /div.sidebar-col -->
<!-- END:   inst/pkgdown/templates/navbar.html-->

        <!-- START: inst/pkgdown/templates/content-extra.html -->
  <div class="col-xl-8 col-lg-12 primary-content">
    <main id="main-content" class="main-content"><div class="container lesson-content">
        
        
<section id="aio-SageMaker-overview"><p>Content from <a href="SageMaker-overview.html">Overview of Amazon SageMaker</a></p>
<hr>
<p>Last updated on 2024-11-05 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/SageMaker-overview.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 10 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<p>Amazon SageMaker is a comprehensive machine learning platform that
empowers users to build, train, tune, and deploy models at scale.
Designed to streamline the ML workflow, SageMaker supports data
scientists and researchers in tackling complex machine learning problems
without needing to manage underlying infrastructure. This allows you to
focus on developing and refining your models while leveraging AWS’s
robust computing resources for efficient training and deployment.</p>
<div class="section level3">
<h3 id="why-use-sagemaker-for-machine-learning">Why use SageMaker for machine learning?<a class="anchor" aria-label="anchor" href="#why-use-sagemaker-for-machine-learning"></a>
</h3>
<p>SageMaker provides several features that make it an ideal choice for
researchers and ML practitioners:</p>
<ul>
<li><p><strong>End-to-end workflow</strong>: SageMaker covers the entire
ML pipeline, from data preprocessing to model deployment. This unified
environment reduces the need to switch between platforms or tools,
enabling users to set up, train, tune, and deploy models
seamlessly.</p></li>
<li><p><strong>Flexible compute options</strong>: SageMaker lets you
easily select instance types tailored to your project needs. For
compute-intensive tasks, such as training deep learning models, you can
switch to GPU instances for faster processing. SageMaker’s scalability
also supports parallelized training, enabling you to distribute large
training jobs across multiple instances, which can significantly speed
up training time for large datasets and complex models.</p></li>
<li><p><strong>Efficient hyperparameter tuning</strong>: SageMaker
provides powerful tools for automated hyperparameter tuning, allowing
users to perform complex cross-validation (CV) searches with a single
chunk of code. This feature enables you to explore a wide range of
parameters and configurations efficiently, helping you find optimal
models without manually managing multiple training runs.</p></li>
<li><p><strong>Support for Custom Scripts</strong>: While SageMaker
offers built-in algorithms, it also allows users to bring their own
customized scripts. This flexibility is crucial for researchers
developing unique models or custom algorithms. SageMaker’s support for
Docker containers allows you to deploy fully customized code for
training, tuning, and inference on scalable AWS infrastructure.</p></li>
<li><p><strong>Cost management and monitoring</strong>: SageMaker
includes built-in monitoring tools to help you track and manage costs,
ensuring you can scale up efficiently without unnecessary expenses. With
thoughtful usage, SageMaker can be very affordable—for example, training
roughly 100 models on a small dataset (under 1GB) can cost less than
$20, making it accessible for many research projects.</p></li>
</ul>
<p>SageMaker is designed to support machine learning at any scale,
making it a strong choice for projects ranging from small experiments to
large research deployments. With robust tools for every step of the ML
process, it empowers researchers and practitioners to bring their models
from development to production efficiently and effectively.</p>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div></section><section id="aio-Data-storage-setting-up-S3"><p>Content from <a href="Data-storage-setting-up-S3.html">Data Storage: Setting up S3</a></p>
<hr>
<p>Last updated on 2024-11-05 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/Data-storage-setting-up-S3.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 20 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I store and manage data effectively in AWS for SageMaker
workflows?</li>
<li>What are the best practices for using S3 versus EC2 storage for
machine learning projects?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Explain data storage options in AWS for machine learning
projects.</li>
<li>Describe the advantages of S3 for large datasets and multi-user
workflows.</li>
<li>Outline steps to set up an S3 bucket and manage data within
SageMaker.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="step-1-data-storage">Step 1: Data storage<a class="anchor" aria-label="anchor" href="#step-1-data-storage"></a>
</h2>
<hr class="half-width">
<blockquote>
<p><strong>Hackathon Attendees</strong>: All data uploaded to AWS must
relate to your specific Kaggle challenge, except for auxiliary datasets
for transfer learning or pretraining. <strong>DO NOT upload any
restricted or sensitive data to AWS.</strong></p>
</blockquote>
</section><section><h2 class="section-heading" id="options-for-storage-ec2-instance-or-s3">Options for storage: EC2 Instance or S3<a class="anchor" aria-label="anchor" href="#options-for-storage-ec2-instance-or-s3"></a>
</h2>
<hr class="half-width">
<p>When working with SageMaker and other AWS services, you have options
for data storage, primarily <strong>EC2 instances</strong> or
<strong>S3</strong>.</p>
<div class="section level4">
<h4 id="what-is-an-ec2-instance">What is an EC2 instance?<a class="anchor" aria-label="anchor" href="#what-is-an-ec2-instance"></a>
</h4>
<p>An Amazon EC2 (Elastic Compute Cloud) instance is a virtual server
environment where you can run applications, process data, and store data
temporarily. EC2 instances come in various types and sizes to meet
different computing and memory needs, making them versatile for tasks
ranging from light web servers to intensive machine learning workloads.
In SageMaker, the notebook instance itself is an EC2 instance configured
to run Jupyter notebooks, enabling direct data processing.</p>
</div>
<div class="section level4">
<h4 id="when-to-store-data-directly-on-ec2">When to store data directly on EC2<a class="anchor" aria-label="anchor" href="#when-to-store-data-directly-on-ec2"></a>
</h4>
<p>Using an EC2 instance for data storage can be useful for temporary or
small datasets, especially during processing within a Jupyter notebook.
However, this storage is not persistent; if the instance is stopped or
terminated, the data is erased. Therefore, EC2 is ideal for one-off
experiments or intermediate steps in data processing.</p>
<div id="limitations-of-ec2-storage" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="limitations-of-ec2-storage" class="callout-inner">
<h3 class="callout-title">Limitations of EC2 storage</h3>
<div class="callout-content">
<ul>
<li>
<strong>Scalability</strong>: EC2 storage is limited to the
instance’s disk capacity, so it may not be ideal for very large
datasets.</li>
<li>
<strong>Cost</strong>: EC2 storage can be more costly for long-term
use compared to S3.</li>
<li>
<strong>Data Persistence</strong>: EC2 data may be lost if the
instance is stopped or terminated, unless using Elastic Block Store
(EBS) for persistent storage.</li>
</ul>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="what-is-an-s3-bucket">What is an S3 bucket?<a class="anchor" aria-label="anchor" href="#what-is-an-s3-bucket"></a>
</h3>
<p>Storing data in an <strong>S3 bucket</strong> is generally preferred
for machine learning workflows on AWS, especially when using SageMaker.
An S3 bucket is a container in Amazon S3 (Simple Storage Service) where
you can store, organize, and manage data files. Buckets act as the
top-level directory within S3 and can hold a virtually unlimited number
of files and folders, making them ideal for storing large datasets,
backups, logs, or any files needed for your project. You access objects
in a bucket via a unique <strong>S3 URI</strong> (e.g.,
<code>s3://your-bucket-name/your-file.csv</code>), which you can use to
reference data across various AWS services like EC2 and SageMaker.</p>
<div id="benefits-of-using-s3-recommended-for-sagemaker-and-ml-workflows" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="benefits-of-using-s3-recommended-for-sagemaker-and-ml-workflows" class="callout-inner">
<h3 class="callout-title">Benefits of using S3 (recommended for SageMaker and ML workflows)</h3>
<div class="callout-content">
<p>For flexibility, scalability, and cost efficiency, store data in S3
and load it into EC2 as needed. This setup allows:</p>
<ul>
<li>
<strong>Scalability</strong>: S3 handles large datasets efficiently,
enabling storage beyond the limits of an EC2 instance’s disk space.</li>
<li>
<strong>Cost efficiency</strong>: S3 storage costs are generally
lower than expanding EC2 disk volumes. You only pay for the storage you
use.</li>
<li>
<strong>Separation of storage and compute</strong>: You can start
and stop EC2 instances without losing access to data stored in S3.</li>
<li>
<strong>Integration with AWS services</strong>: SageMaker can read
directly from and write back to S3, making it ideal for AWS-based
workflows.</li>
<li>
<strong>Easy data sharing</strong>: Datasets in S3 are easier to
share with team members or across projects compared to EC2 storage.</li>
<li>
<strong>Cost-effective data transfer</strong>: When S3 and EC2 are
in the same region, data transfer between them is free.</li>
</ul>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="recommended-approach-s3-buckets">Recommended approach: S3 buckets<a class="anchor" aria-label="anchor" href="#recommended-approach-s3-buckets"></a>
</h2>
<hr class="half-width">
<p><strong>Hackathon attendees</strong>: When you setup your bucket for
your actual project, note that you will only need one bucket for your
whole team. Team members will have the proper permissions to access
buckets on our shared account.</p>
<div class="section level3">
<h3 id="summary-steps-to-access-s3-and-upload-your-dataset">Summary steps to access S3 and upload your dataset<a class="anchor" aria-label="anchor" href="#summary-steps-to-access-s3-and-upload-your-dataset"></a>
</h3>
<ol style="list-style-type: decimal">
<li>Log in to AWS Console and navigate to S3.</li>
<li>Create a new bucket or use an existing one.</li>
<li>Upload your dataset files.</li>
<li>Use the object URL to reference your data in future
experiments.</li>
</ol>
</div>
<div class="section level3">
<h3 id="detailed-procedure">Detailed procedure<a class="anchor" aria-label="anchor" href="#detailed-procedure"></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<p><strong>Sign in to the AWS Management Console</strong></p>
<ul>
<li>Log in to AWS Console using your credentials.</li>
</ul>
</li>
<li>
<p><strong>Navigate to S3</strong></p>
<ul>
<li>Type “S3” in the search bar</li>
<li>Protip: select the star icon to save S3 as a bookmark in your AWS
toolbar</li>
<li>Select <strong>S3 - Scalable Storage in the Cloud</strong>
</li>
</ul>
</li>
<li>
<p><strong>Create a new bucket</strong></p>
<ul>
<li>Click <strong>Create Bucket</strong> and enter a unique name, and
note that bucket name must not contain uppercase characters.
<strong>Hackathon participants</strong>: Use the following convention
for your bucket name: <code>teamname_datasetname</code> (e.g.,
<code>myawesometeam-titanic</code>).</li>
<li>
<strong>Region</strong>: Leave as is (likely <code>us-east-1</code>
(US East N. Virginia))</li>
<li>
<strong>Access Control</strong>: Disable ACLs (recommended).</li>
<li>
<strong>Public Access</strong>: Turn on “Block all public
access”.</li>
<li>
<strong>Versioning</strong>: Disable unless you need multiple
versions of objects.</li>
<li>
<strong>Tags</strong>: Adding tags to your S3 buckets is a great way
to track project-specific costs and usage over time, especially as data
and resources scale up. While tags are required for hackathon
participants, we suggest that all users apply tags to easily identify
and analyze costs later. <strong>Hackathon participants</strong>: Use
the following convention for your bucket name
<ul>
<li>
<strong>Name</strong>: Your Name</li>
<li>
<strong>ProjectName</strong>: Your team’s name</li>
<li>
<strong>Purpose</strong>: Dataset name (e.g., titanic if you’re
following along with this workshop) <img src="https://raw.githubusercontent.com/UW-Madison-DataScience/ml-with-aws-sagemaker/main/images/bucket_tags.PNG" alt="Screenshot showing required tags for an S3 bucket" class="figure">
</li>
</ul>
</li>
<li>Click <strong>Create Bucket</strong> at the bottom once everything
above has been configured</li>
</ul>
</li>
<li>
<p><strong>Edit bucket policy</strong> Once the bucket is created,
you’ll be brought to a page that shows all of your current buckets (and
those on our shared account). We’ll have to edit our bucket’s policy to
allow ourselves proper access to any files stored there (e.g., read from
bucket, write to bucket). To set these permissions…</p>
<ol style="list-style-type: decimal">
<li>Click on the name of your bucket to bring up additional options and
settings.</li>
<li>Click the Permissions tab</li>
<li>Scroll down to Bucket policy and click Edit. Paste the following
policy, <strong>editing the bucket name “myawesometeam-titanic”</strong>
to reflect your bucket’s name</li>
</ol>
</li>
</ol>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">JSON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode json" tabindex="0"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>	<span class="dt">"Version"</span><span class="fu">:</span> <span class="st">"2012-10-17"</span><span class="fu">,</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>	<span class="dt">"Statement"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>		<span class="fu">{</span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>			<span class="dt">"Effect"</span><span class="fu">:</span> <span class="st">"Allow"</span><span class="fu">,</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>			<span class="dt">"Principal"</span><span class="fu">:</span> <span class="fu">{</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>				<span class="dt">"AWS"</span><span class="fu">:</span> <span class="st">"arn:aws:iam::183295408236:role/ml-sagemaker-use"</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>			<span class="fu">},</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>			<span class="dt">"Action"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>				<span class="st">"s3:GetObject"</span><span class="ot">,</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>				<span class="st">"s3:PutObject"</span><span class="ot">,</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>				<span class="st">"s3:DeleteObject"</span><span class="ot">,</span></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a>				<span class="st">"s3:ListMultipartUploadParts"</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>			<span class="ot">]</span><span class="fu">,</span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a>			<span class="dt">"Resource"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a>				<span class="st">"arn:aws:s3:::myawesometeam-titanic"</span><span class="ot">,</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>				<span class="st">"arn:aws:s3:::myawesometeam-titanic/*"</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>			<span class="ot">]</span></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>		<span class="fu">}</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>	<span class="ot">]</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a><span class="fu">}</span></span></code></pre>
</div>
<p>For hackathon attendees, this policy grants the
<code>ml-sagemaker-use</code> IAM role access to specific S3 bucket
actions, ensuring they can use the bucket for reading, writing,
deleting, and listing parts during multipart uploads. Attendees should
apply this policy to their buckets to enable SageMaker to operate on
stored data.</p>
<div id="general-guidance-for-setting-up-permissions-outside-the-hackathon" class="callout">
<div class="callout-square">
<i class="callout-icon" data-feather="bell"></i>
</div>
<div id="general-guidance-for-setting-up-permissions-outside-the-hackathon" class="callout-inner">
<h3 class="callout-title">General guidance for setting up permissions outside the hackathon</h3>
<div class="callout-content">
<p>For those not participating in the hackathon, it’s essential to
create a similar IAM role (such as <code>ml-sagemaker-use</code>) with
policies that provide controlled access to S3 resources, ensuring only
the necessary actions are permitted for security and
cost-efficiency.</p>
<ol style="list-style-type: lower-alpha">
<li><p><strong>Create an IAM role</strong>: Set up an IAM role for
SageMaker to assume, with necessary S3 access permissions, such as
<code>s3:GetObject</code>, <code>s3:PutObject</code>,
<code>s3:DeleteObject</code>, and
<code>s3:ListMultipartUploadParts</code>, as shown in the policy
above.</p></li>
<li><p><strong>Attach permissions to S3 buckets</strong>: Attach bucket
policies that specify this role as the principal, as in the hackathon
example.</p></li>
<li><p><strong>More information</strong>: For a detailed guide on
setting up roles and policies for SageMaker, refer to the <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html" class="external-link">AWS
SageMaker documentation on IAM roles and policies</a>. This resource
explains role creation, permission setups, and policy best practices
tailored for SageMaker’s operations with S3 and other AWS
services.</p></li>
</ol>
<p>This setup ensures that your SageMaker operations will have the
access needed without exposing the bucket to unnecessary permissions or
external accounts.</p>
</div>
</div>
</div>
<ol start="5" style="list-style-type: decimal">
<li>
<strong>Upload files to the bucket</strong>
<ul>
<li>Navigate to the Objects tab of your bucket, then
<strong>Upload</strong>.</li>
<li>
<strong>Add Files</strong> (e.g., <code>titanic_train.csv</code>,
<code>titanic_test.csv</code>) and click <strong>Upload</strong> to
complete.</li>
</ul>
</li>
<li>
<strong>Take note of S3 URI for your data</strong>
<ul>
<li>After uploading, click on a file to find its <strong>Object
URI</strong> (e.g., <code>s3://titanic-dataset-test/test.csv</code>).
We’ll use this URI to load data into SageMaker later.</li>
</ul>
</li>
</ol>
</div>
</section><section><h2 class="section-heading" id="s3-bucket-costs">S3 bucket costs<a class="anchor" aria-label="anchor" href="#s3-bucket-costs"></a>
</h2>
<hr class="half-width">
<p>S3 bucket storage incurs costs based on data storage, data transfer,
and request counts.</p>
<div class="section level3">
<h3 id="storage-costs">Storage costs<a class="anchor" aria-label="anchor" href="#storage-costs"></a>
</h3>
<ul>
<li>Storage is charged per GB per month. Typical: Storing 10 GB costs
approximately $0.23/month in S3 Standard (us-east-1).</li>
<li>Pricing Tiers: S3 offers multiple storage classes (Standard,
Intelligent-Tiering, Glacier, etc.), with different costs based on
access frequency and retrieval times. Standard S3 fits most purposes. If
you’re curious about other tiers, refer to AWS’s <a href="https://aws.amazon.com/s3/pricing/" class="external-link">S3 Pricing
Information</a>.</li>
<li>To calculate specific costs based on your needs, storage class, and
region, refer to AWS’s <a href="https://aws.amazon.com/s3/pricing/" class="external-link">S3
Pricing Information</a>.</li>
</ul>
</div>
<div class="section level3">
<h3 id="data-transfer-costs">Data transfer costs<a class="anchor" aria-label="anchor" href="#data-transfer-costs"></a>
</h3>
<ul>
<li>
<strong>Uploading</strong> data to S3 is free.</li>
<li>
<strong>Downloading</strong> data (out of S3) incurs charges
(~$0.09/GB). Be sure to take note of this fee, as it can add up fast for
large datasets.</li>
<li>
<strong>In-region transfer</strong> (e.g., S3 to EC2) is free, while
cross-region data transfer is charged (~$0.02/GB).</li>
</ul>
<blockquote>
<p><strong><a href="https://aws.amazon.com/s3/pricing/" class="external-link">Data transfer
pricing</a></strong></p>
</blockquote>
</div>
<div class="section level3">
<h3 id="request-costs">Request costs<a class="anchor" aria-label="anchor" href="#request-costs"></a>
</h3>
<ul>
<li>GET requests are $0.0004 per 1,000 requests. In the context of
Amazon S3, “GET” requests refer to the action of retrieving or
downloading data from an S3 bucket. Each time a file or object is
accessed in S3, it incurs a small cost per request. This means that if
you have code that reads data from S3 frequently, such as loading
datasets repeatedly, each read operation counts as a GET request.</li>
</ul>
<blockquote>
<p><strong><a href="https://aws.amazon.com/s3/pricing/" class="external-link">Request
Pricing</a></strong></p>
</blockquote>
<div id="challenge-exercise-calculate-your-projects-data-costs" class="callout discussion">
<div class="callout-square">
<i class="callout-icon" data-feather="message-circle"></i>
</div>
<div id="challenge-exercise-calculate-your-projects-data-costs" class="callout-inner">
<h3 class="callout-title">Challenge Exercise: Calculate Your Project’s Data Costs</h3>
<div class="callout-content">
<p>Estimate the total cost of storing your project data in S3 for one
month, using the following dataset sizes and assuming:</p>
<ul>
<li>Storage duration: 1 month</li>
<li>Storage region: us-east-1</li>
<li>Storage class: S3 Standard</li>
<li>Data will be retrieved 100 times for model training
(<code>GET</code> requests)</li>
<li>Data will be deleted after the project concludes, incurring data
retrieval and deletion costs</li>
</ul>
<p>Dataset sizes to consider:</p>
<ul>
<li>1 GB</li>
<li>10 GB</li>
<li>100 GB</li>
<li>1 TB</li>
</ul>
<p><strong>Hints</strong></p>
<ul>
<li>S3 storage cost: $0.023 per GB per month (us-east-1)</li>
<li>Data transfer cost (retrieval/deletion): $0.09 per GB (us-east-1 out
to internet)</li>
<li>
<code>GET</code> requests cost: $0.0004 per 1,000 requests (each
model training will incur one <code>GET</code> request)</li>
</ul>
<p>Check the <a href="https://aws.amazon.com/s3/pricing/" class="external-link">AWS S3
Pricing</a> page for more details.</p>
</div>
</div>
</div>
<div id="accordionSolution1" class="accordion challenge-accordion accordion-flush">
<div class="accordion-item">
<button class="accordion-button solution-button collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#collapseSolution1" aria-expanded="false" aria-controls="collapseSolution1">
  <h4 class="accordion-header" id="headingSolution1"> Show me the solution </h4>
</button>
<div id="collapseSolution1" class="accordion-collapse collapse" data-bs-parent="#accordionSolution1" aria-labelledby="headingSolution1">
<div class="accordion-body">
<p>Using the S3 Standard rate in us-east-1:</p>
<ol style="list-style-type: decimal">
<li>
<strong>1 GB</strong>:
<ul>
<li>
<strong>Storage</strong>: 1 GB * $0.023 = $0.023</li>
<li>
<strong>Retrieval/Deletion</strong>: 1 GB * $0.09 = $0.09<br>
</li>
<li>
<strong>GET Requests</strong>: 100 requests * $0.0004 per 1,000 =
$0.00004<br>
</li>
<li>
<strong>Total Cost</strong>: <strong>$0.11304</strong>
</li>
</ul>
</li>
<li>
<strong>10 GB</strong>:
<ul>
<li>
<strong>Storage</strong>: 10 GB * $0.023 = $0.23</li>
<li>
<strong>Retrieval/Deletion</strong>: 10 GB * $0.09 = $0.90<br>
</li>
<li>
<strong>GET Requests</strong>: 100 requests * $0.0004 per 1,000 =
$0.00004<br>
</li>
<li>
<strong>Total Cost</strong>: <strong>$1.13004</strong>
</li>
</ul>
</li>
<li>
<strong>100 GB</strong>:
<ul>
<li>
<strong>Storage</strong>: 100 GB * $0.023 = $2.30</li>
<li>
<strong>Retrieval/Deletion</strong>: 100 GB * $0.09 = $9.00<br>
</li>
<li>
<strong>GET Requests</strong>: 100 requests * $0.0004 per 1,000 =
$0.00004<br>
</li>
<li>
<strong>Total Cost</strong>: <strong>$11.30004</strong>
</li>
</ul>
</li>
<li>
<strong>1 TB (1024 GB)</strong>:
<ul>
<li>
<strong>Storage</strong>: 1024 GB * $0.023 = $23.55</li>
<li>
<strong>Retrieval/Deletion</strong>: 1024 GB * $0.09 = $92.16<br>
</li>
<li>
<strong>GET Requests</strong>: 100 requests * $0.0004 per 1,000 =
$0.00004<br>
</li>
<li>
<strong>Total Cost</strong>: <strong>$115.71004</strong>
</li>
</ul>
</li>
</ol>
<p>These costs assume no additional request charges beyond those for
retrieval, storage, and <code>GET</code> requests for training.</p>
</div>
</div>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="removing-unused-data">Removing unused data<a class="anchor" aria-label="anchor" href="#removing-unused-data"></a>
</h2>
<hr class="half-width">
<p>Choose one of these options:</p>
<div class="section level3">
<h3 id="option-1-delete-data-only">Option 1: Delete data only<a class="anchor" aria-label="anchor" href="#option-1-delete-data-only"></a>
</h3>
<ul>
<li>
<strong>When to Use</strong>: You plan to reuse the bucket.</li>
<li>
<strong>Steps</strong>:
<ul>
<li>Go to S3, navigate to the bucket.</li>
<li>Select files to delete, then <strong>Actions &gt;
Delete</strong>.</li>
<li>
<strong>CLI</strong> (optional):
<code>!aws s3 rm s3://your-bucket-name --recursive</code>
</li>
</ul>
</li>
</ul>
</div>
<div class="section level3">
<h3 id="option-2-delete-the-s3-bucket-entirely">Option 2: Delete the S3 bucket entirely<a class="anchor" aria-label="anchor" href="#option-2-delete-the-s3-bucket-entirely"></a>
</h3>
<ul>
<li>
<strong>When to Use</strong>: You no longer need the bucket or
data.</li>
<li>
<strong>Steps</strong>:
<ul>
<li>Select the bucket, click <strong>Actions &gt; Delete</strong>.</li>
<li>Type the bucket name to confirm deletion.</li>
</ul>
</li>
</ul>
<p>Deleting the bucket stops all costs associated with storage,
requests, and data transfer.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Use S3 for scalable, cost-effective, and flexible storage.</li>
<li>EC2 storage is fairly uncommon, but may be suitable for small,
temporary datasets.</li>
<li>Track your S3 storage costs, data transfer, and requests to manage
expenses.</li>
<li>Regularly delete unused data or buckets to avoid ongoing costs.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-SageMaker-notebooks-as-controllers"><p>Content from <a href="SageMaker-notebooks-as-controllers.html">Notebooks as Controllers</a></p>
<hr>
<p>Last updated on 2024-11-06 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/SageMaker-notebooks-as-controllers.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How do you set up and use SageMaker notebooks for machine learning
tasks?</li>
<li>How can you manage compute resources efficiently using SageMaker’s
controller notebook approach?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Describe how to use SageMaker notebooks for ML workflows.</li>
<li>Set up a Jupyter notebook instance as a controller to manage compute
tasks.</li>
<li>Use SageMaker SDK to launch training and tuning jobs on scalable
instances.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="step-2-running-python-code-with-sagemaker-notebooks">Step 2: Running Python code with SageMaker notebooks<a class="anchor" aria-label="anchor" href="#step-2-running-python-code-with-sagemaker-notebooks"></a>
</h2>
<hr class="half-width">
<p>Amazon SageMaker provides a managed environment to simplify the
process of building, training, and deploying machine learning models. By
using SageMaker, you can focus on model development without needing to
manually provision resources or set up environments. In this episode,
we’ll guide you through setting up a <strong>SageMaker notebook
instance</strong>—a Jupyter notebook hosted on AWS specifically for
running SageMaker jobs. This setup allows you to efficiently manage and
monitor machine learning workflows directly from a lightweight notebook
controller. We’ll also cover loading data in preparation for model
training and tuning in future episodes, using the Titanic dataset stored
in S3.</p>
<blockquote>
<p><strong>Note for hackathon attendees</strong>: We’ll use SageMaker
notebook instances (not the full SageMaker Studio environment) for
simpler instance management and streamlined resource usage, ideal for
collaborative projects or straightforward ML tasks.</p>
</blockquote>
</section><section><h2 class="section-heading" id="using-the-notebook-as-a-controller">Using the notebook as a controller<a class="anchor" aria-label="anchor" href="#using-the-notebook-as-a-controller"></a>
</h2>
<hr class="half-width">
<p>In this setup, the notebook instance functions as a
<strong>controller</strong> to manage more resource-intensive compute
tasks. By selecting a minimal instance (e.g., <code>ml.t3.medium</code>)
for the notebook, you can perform lightweight operations and leverage
the <strong>SageMaker Python SDK</strong> to launch more powerful,
scalable compute instances when needed for model training, batch
processing, or hyperparameter tuning. This approach minimizes costs by
keeping your controller instance lightweight while accessing the full
power of SageMaker for demanding tasks.</p>
</section><section><h2 class="section-heading" id="summary-of-key-steps">Summary of key steps<a class="anchor" aria-label="anchor" href="#summary-of-key-steps"></a>
</h2>
<hr class="half-width">
<ol style="list-style-type: decimal">
<li>Navigate to SageMaker in AWS.</li>
<li>Create a Jupyter notebook instance as a controller.</li>
<li>Set up the Python environment within the notebook.</li>
<li>Load the Titanic dataset from S3.</li>
<li>Use SageMaker SDK to launch training and tuning jobs on powerful
instances (covered in next episodes).</li>
<li>View and monitor training/tuning progress (covered in next
episodes).</li>
</ol></section><section><h2 class="section-heading" id="detailed-procedure">Detailed procedure<a class="anchor" aria-label="anchor" href="#detailed-procedure"></a>
</h2>
<hr class="half-width">
<div class="section level3">
<h3 id="navigate-to-sagemaker">1. Navigate to SageMaker<a class="anchor" aria-label="anchor" href="#navigate-to-sagemaker"></a>
</h3>
<ul>
<li>In the AWS Console, search for <strong>SageMaker</strong>.</li>
<li>Protip: select the star icon to save SageMaker as a bookmark in your
AWS toolbar</li>
<li>Select <strong>SageMaker - Build, Train, and Deploy
Models</strong>.</li>
</ul>
</div>
<div class="section level3">
<h3 id="create-a-new-notebook-instance">2. Create a new notebook instance<a class="anchor" aria-label="anchor" href="#create-a-new-notebook-instance"></a>
</h3>
<ul>
<li>In the SageMaker left-side menu, click on
<strong>Notebooks</strong>, then click <strong>Create notebook
instance</strong>.</li>
<li>
<strong>Notebook name</strong>: Enter a name that reflects your
notebook’s primary user (your name), dataset (titanic), purpose
(train-tune), and models utilized (XGBoost-NN). <strong>Hackathon
attendees must use the following convention</strong>:
TeamName-YourName-Dataset-NotebookPurpose(s)-Model(s) (e.g.,
<code>MyAwesomeTeam-ChrisEndemann-Titanic-Train-Tune-XGBoost-NN</code>).</li>
<li>
<strong>Instance type</strong>: Start with a small instance type,
such as <code>ml.t3.medium</code>. You can scale up later as needed for
intensive tasks, which will be managed by launching separate training
jobs from this notebook. For guidance on common instances for ML
procedures, refer to this <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">spreadsheet</a>.</li>
<li>
<strong>Platform identifier</strong>: You can leave this as the
default.</li>
<li>
<strong>Permissions and encryption</strong>:
<ul>
<li>
<strong>IAM role</strong>: Choose an existing role or create a new
one. <strong>Hackathon attendees should select
‘ml-sagemaker-use’</strong>. The role should include the
<code>AmazonSageMakerFullAccess</code> policy to enable access to AWS
services like S3.</li>
<li>
<strong>Root access</strong>: Enable root access to notebook.</li>
<li>
<strong>Encryption key (optional)</strong>: Specify a KMS key for
encrypting data at rest if needed. Otherwise, leave it blank.</li>
</ul>
</li>
<li>
<strong>Network (optional)</strong>: Networking settings are
optional. Configure them if you’re working within a specific VPC or need
network customization.</li>
<li>
<strong>Git repositories configuration (optional)</strong>: You
don’t need to complete this configuration. Instead, we’ll run a clone
command from our notebook later to get our repo setup. This approach is
a common strategy (allowing some flexiblity in which repo you use for
the notebook.</li>
<li>
<strong>Tags (required for hackathon attendees)</strong>: Adding
tags helps track and organize resources for billing and management. This
is particularly useful when you need to break down expenses by project,
task, or team. Please use the tags found in the below image to track
your notebook’s resource usage.</li>
</ul>
<figure><img src="https://raw.githubusercontent.com/UW-Madison-DataScience/ml-with-aws-sagemaker/main/images/notebook_tags.PNG" alt="Tag Setup Example" class="figure mx-auto d-block"><div class="figcaption">Tag Setup Example</div>
</figure><ul>
<li>Click <strong>Create notebook instance</strong>. It may take a few
minutes for the instance to start. Once its status is
<strong>InService</strong>, you can open the notebook instance and start
coding.</li>
</ul>
</div>
<div class="section level3">
<h3 id="managing-training-and-tuning-with-the-controller-notebook">Managing training and tuning with the controller notebook<a class="anchor" aria-label="anchor" href="#managing-training-and-tuning-with-the-controller-notebook"></a>
</h3>
<p>In the next couple expisodes, we’ll use the <strong>SageMaker Python
SDK</strong> within the notebook to launch compute-heavy tasks on more
powerful instances as needed. Examples of tasks to launch include:</p>
<ul>
<li>
<strong>Training a model</strong>: Use the SDK to submit a training
job, specifying a higher-powered instance (e.g.,
<code>ml.p2.xlarge</code> or <code>ml.m5.4xlarge</code>) based on your
model’s resource requirements.</li>
<li>
<strong>Hyperparameter tuning</strong>: Configure and launch tuning
jobs, allowing SageMaker to automatically manage multiple powerful
instances for optimal tuning.</li>
</ul>
<p>This setup allows you to control costs by keeping the notebook
instance minimal and only incurring costs for larger instances when they
are actively training or tuning models. Detailed guidance on training,
tuning, and batch processing will follow in later episodes.</p>
<p>For more details, refer to the <a href="https://sagemaker.readthedocs.io/" class="external-link">SageMaker Python SDK
documentation</a> for example code on launching and managing remote
training jobs.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Use a minimal SageMaker notebook instance as a controller to manage
larger, resource-intensive tasks.</li>
<li>Launch training and tuning jobs on scalable instances using the
SageMaker SDK.</li>
<li>Tags can help track costs effectively, especially in multi-project
or team settings.</li>
<li>Use the SageMaker SDK documentation to explore additional options
for managing compute resources in AWS.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-Accessing-S3-via-SageMaker-notebooks"><p>Content from <a href="Accessing-S3-via-SageMaker-notebooks.html">Accessing and Managing Data in S3 with SageMaker Notebooks</a></p>
<hr>
<p>Last updated on 2024-11-06 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/Accessing-S3-via-SageMaker-notebooks.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I load data from S3 into a SageMaker notebook?</li>
<li>How do I monitor storage usage and costs for my S3 bucket?</li>
<li>What steps are involved in pushing new data back to S3 from a
notebook?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Read data directly from an S3 bucket into memory in a SageMaker
notebook.</li>
<li>Check storage usage and estimate costs for data in an S3
bucket.</li>
<li>Upload new files from the SageMaker environment back to the S3
bucket.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="section level3">
<h3 id="setup">Setup<a class="anchor" aria-label="anchor" href="#setup"></a>
</h3>
<div class="section level4">
<h4 id="open--ipynb-notebook">Open .ipynb notebook<a class="anchor" aria-label="anchor" href="#open--ipynb-notebook"></a>
</h4>
<p>Once your newly created notebook <em>instance</em> (“SageMaker
notebook”) shows as <code>InService</code>, open the instance in Jupyter
Lab. From there, we will select the standard python3 environment
(conda_python3) to start our first .ipynb notebook (“Jupyter notebook”).
You can name your Jupyter notebook something along the lines of,
<code>Interacting-with-S3.ipynb</code>.</p>
<p>We can use the standard conda_python3 environment since we aren’t
doing any training/tuning just yet. #### Set up AWS environment To begin
each SageMaker notebook, it’s important to set up an AWS environment
that will allow seamless access to the necessary cloud resources. Here’s
what we’ll do to get started:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Define the Role</strong>: We’ll use
<code>get_execution_role()</code> to retrieve the IAM role associated
with the SageMaker instance. This role specifies the permissions needed
for interacting with AWS services like S3, which allows SageMaker to
securely read from and write to storage buckets.</p></li>
<li><p><strong>Initialize the SageMaker Session</strong>: Next, we’ll
create a <code>sagemaker.Session()</code> object, which will help manage
and track the resources and operations we use in SageMaker, such as
training jobs and model artifacts. The session acts as a bridge between
the SageMaker SDK commands in our notebook and AWS services.</p></li>
<li><p><strong>Set Up an S3 Client</strong>: Using <code>boto3</code>,
we’ll initialize an S3 client for accessing S3 buckets directly. This
client enables us to handle data storage, retrieve datasets, and manage
files in S3, which will be essential as we work through various machine
learning tasks.</p></li>
</ol>
<p>Starting with these initializations prepares our notebook environment
to efficiently interact with AWS resources for model development, data
management, and deployment.</p>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> boto3</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> sagemaker</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">from</span> sagemaker <span class="im">import</span> get_execution_role</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co"># Initialize the SageMaker role and session</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># Define the SageMaker role and session</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>role <span class="op">=</span> sagemaker.get_execution_role() <span class="co"># specifies your permissions to use AWS tools</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>session <span class="op">=</span> sagemaker.Session() </span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a>s3 <span class="op">=</span> boto3.client(<span class="st">'s3'</span>)</span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="reading-data-from-s3">Reading data from S3<a class="anchor" aria-label="anchor" href="#reading-data-from-s3"></a>
</h3>
<p>You can either read data from S3 into memory or download a copy of
your S3 data into your notebook’s instance. While loading into memory
can save on storage resources, it can be convenient at times to have a
local copy. We’ll show you both strategies in this upcoming section.
Here’s a more detailed look at the pros and cons of each strategy:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Reading data directly from S3 into memory</strong>:
<ul>
<li>
<strong>Pros</strong>:
<ul>
<li>
<strong>Storage efficiency</strong>: By keeping data in memory, you
avoid taking up local storage on your notebook instance, which can be
particularly beneficial for larger datasets or instances with limited
storage.</li>
<li>
<strong>Simple data management</strong>: Accessing data directly
from S3 avoids the need to manage or clean up local copies after
processing.</li>
</ul>
</li>
<li>
<strong>Cons</strong>:
<ul>
<li>
<strong>Performance for frequent reads</strong>: Accessing S3 data
repeatedly can introduce latency and slow down workflows, as each read
requires a network request. This approach works best if you only need to
load data once or infrequently.</li>
<li>
<strong>Potential cost for high-frequency access</strong>: Multiple
GET requests to S3 can accumulate charges over time, especially if your
workflow requires repeated access to the same data.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Downloading a copy of data from S3 to local
storage</strong>:
<ul>
<li>
<strong>Pros</strong>:
<ul>
<li>
<strong>Better performance for intensive workflows</strong>: If you
need to access the dataset multiple times during processing, working
from a local copy avoids repeated network requests, making operations
faster and more efficient.</li>
<li>
<strong>Offline access</strong>: Once downloaded, you can access the
data without a persistent internet connection, which can be helpful for
handling larger data transformations.</li>
</ul>
</li>
<li>
<strong>Cons</strong>:
<ul>
<li>
<strong>Storage costs</strong>: Local storage on the instance may
come with additional costs or limitations, especially if your instance
type has constrained storage capacity.</li>
<li>
<strong>Data management overhead</strong>: You’ll need to manage
local data copies and ensure that they are properly cleaned up to free
resources once processing is complete.</li>
</ul>
</li>
</ul>
</li>
</ol>
</div>
<div class="section level3">
<h3 id="choosing-between-the-two-strategies">Choosing between the two strategies<a class="anchor" aria-label="anchor" href="#choosing-between-the-two-strategies"></a>
</h3>
<p>If your workflow requires only a single read of the dataset for
processing, reading directly into memory can be a quick and
resource-efficient solution. However, for cases where you’ll perform
extensive or iterative processing, downloading a local copy of the data
will typically be more performant and may incur fewer request-related
costs.</p>
</div>
<section><h2 class="section-heading" id="a--read-data-from-s3-into-memory">1A. Read data from S3 into memory<a class="anchor" aria-label="anchor" href="#a--read-data-from-s3-into-memory"></a>
</h2>
<hr class="half-width">
<p>Our data is stored on an S3 bucket called ‘titanic-dataset-test’. We
can use the following code to read data directly from S3 into memory in
the Jupyter notebook environment, without actually downloading a copy of
train.csv as a local file.</p>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co"># Define the S3 bucket and object key</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>bucket_name <span class="op">=</span> <span class="st">'myawesometeam-titanic'</span>  <span class="co"># replace with your S3 bucket name</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co"># Read the train data from S3</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>key <span class="op">=</span> <span class="st">'titanic_train.csv'</span>  <span class="co"># replace with your object key</span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>response <span class="op">=</span> s3.get_object(Bucket<span class="op">=</span>bucket_name, Key<span class="op">=</span>key)</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>train_data <span class="op">=</span> pd.read_csv(response[<span class="st">'Body'</span>])</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co"># Read the test data from S3</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>key <span class="op">=</span> <span class="st">'titanic_test.csv'</span>  <span class="co"># replace with your object key</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>response <span class="op">=</span> s3.get_object(Bucket<span class="op">=</span>bucket_name, Key<span class="op">=</span>key)</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a>test_data <span class="op">=</span> pd.read_csv(response[<span class="st">'Body'</span>])</span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="co"># check shape</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="bu">print</span>(train_data.shape)</span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="bu">print</span>(test_data.shape)</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co"># Inspect the first few rows of the DataFrame</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>train_data.head()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="ex">sagemaker.config</span> INFO <span class="at">-</span> Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="ex">sagemaker.config</span> INFO <span class="at">-</span> Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="kw">(</span><span class="ex">712,</span> 12<span class="kw">)</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="kw">(</span><span class="ex">179,</span> 12<span class="kw">)</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="b--download-copy-into-notebook-environment">1B. Download copy into notebook environment<a class="anchor" aria-label="anchor" href="#b--download-copy-into-notebook-environment"></a>
</h2>
<hr class="half-width">
<p>Download data from S3 to notebook environment. You may need to hit
refresh on the file explorer panel to the left to see this file. If you
get any permission issues…</p>
<ul>
<li>check that you have selected the appropriate policy for this
notebook</li>
<li>check that your bucket has the appropriate policy permissions</li>
</ul>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># Define the S3 bucket and file location</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>key <span class="op">=</span> <span class="st">"titanic_train.csv"</span>  <span class="co"># Path to your file in the S3 bucket</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>local_file_path <span class="op">=</span> <span class="st">"./titanic_train.csv"</span>  <span class="co"># Local path to save the file</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co"># Initialize the S3 client and download the file</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>s3.download_file(bucket_name, key, local_file_path)</span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="op">!</span>ls</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="ex">File</span> downloaded: ./titanic_train.csv</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="check-current-size-and-storage-costs-of-bucket">2. Check current size and storage costs of bucket<a class="anchor" aria-label="anchor" href="#check-current-size-and-storage-costs-of-bucket"></a>
</h2>
<hr class="half-width">
<p>It’s a good idea to periodically check how much storage you have used
in your bucket. You can do this from a Jupyter notebook in SageMaker by
using the <strong>Boto3</strong> library, which is the AWS SDK for
Python. This will allow you to calculate the total size of objects
within a specified bucket. Here’s how you can do it…</p>
<div class="section level3">
<h3 id="step-1-set-up-the-s3-client-and-calculate-bucket-size">Step 1: Set up the S3 Client and Calculate Bucket Size<a class="anchor" aria-label="anchor" href="#step-1-set-up-the-s3-client-and-calculate-bucket-size"></a>
</h3>
<p>The code below will calculate your bucket size for you. Here is a
breakdown of the important pieces in the next code section:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Paginator</strong>: Since S3 buckets can contain many
objects, we use a paginator to handle large listings.</li>
<li>
<strong>Size calculation</strong>: We sum the <code>Size</code>
attribute of each object in the bucket.</li>
<li>
<strong>Unit conversion</strong>: The size is given in bytes, so
dividing by <code>1024 ** 2</code> converts it to megabytes (MB).</li>
</ol>
<blockquote>
<p><strong>Note</strong>: If your bucket has very large objects or you
want to check specific folders within a bucket, you may want to refine
this code to only fetch certain objects or folders.</p>
</blockquote>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Initialize the total size counter</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>total_size_bytes <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co"># List and sum the size of all objects in the bucket</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>paginator <span class="op">=</span> s3.get_paginator(<span class="st">'list_objects_v2'</span>)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="cf">for</span> page <span class="kw">in</span> paginator.paginate(Bucket<span class="op">=</span>bucket_name):</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>    <span class="cf">for</span> obj <span class="kw">in</span> page.get(<span class="st">'Contents'</span>, []):</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>        total_size_bytes <span class="op">+=</span> obj[<span class="st">'Size'</span>]</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co"># Convert the total size to gigabytes for cost estimation</span></span>
<span id="cb6-11"><a href="#cb6-11" tabindex="-1"></a>total_size_gb <span class="op">=</span> total_size_bytes <span class="op">/</span> (<span class="dv">1024</span> <span class="op">**</span> <span class="dv">3</span>)</span>
<span id="cb6-12"><a href="#cb6-12" tabindex="-1"></a><span class="co"># print(f"Total size of bucket '{bucket_name}': {total_size_gb:.2f} GB") # can uncomment this if you want GB reported</span></span>
<span id="cb6-13"><a href="#cb6-13" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" tabindex="-1"></a><span class="co"># Convert the total size to megabytes for readability</span></span>
<span id="cb6-15"><a href="#cb6-15" tabindex="-1"></a>total_size_mb <span class="op">=</span> total_size_bytes <span class="op">/</span> (<span class="dv">1024</span> <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb6-16"><a href="#cb6-16" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total size of bucket '</span><span class="sc">{</span>bucket_name<span class="sc">}</span><span class="ss">': </span><span class="sc">{</span>total_size_mb<span class="sc">:.2f}</span><span class="ss"> MB"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="ex">Total</span> size of bucket <span class="st">'myawesometeam-titanic'</span>: 0.06 MB</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="using-helper-functions-from-lesson-repo">Using helper functions from lesson repo<a class="anchor" aria-label="anchor" href="#using-helper-functions-from-lesson-repo"></a>
</h3>
<p>We have added code to calculate bucket size to a helper function
called <code>get_s3_bucket_size(bucket_name)</code> for your
convenience. There are also some other helper functions in that repo to
assist you with common AWS/SageMaker workflows. We’ll show you how to
clone this code into your notebook environment.</p>
<p><strong>Note</strong>: Make sure you have already forked the lesson
repo as described on the <a href="https://uw-madison-datascience.github.io/ML_with_Amazon_SageMaker/#workshop-repository-setup" class="external-link">setup
page</a>. Replace “username” below with your GitHub username.</p>
<div class="section level4">
<h4 id="directory-setup">Directory setup<a class="anchor" aria-label="anchor" href="#directory-setup"></a>
</h4>
<p>Let’s make sure we’re starting in the root directory of this
instance, so that we all have our AWS_helpers.py file located in the
same path (/test_AWS/scripts/AWS_helpers.py)</p>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="op">%</span>cd <span class="op">/</span>home<span class="op">/</span>ec2<span class="op">-</span>user<span class="op">/</span>SageMaker<span class="op">/</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="ex">/home/ec2-user/SageMaker</span></span></code></pre>
</div>
<p>To clone the repo to our Jupyter notebook, use the following
code.</p>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="op">!</span>git clone https:<span class="op">//</span>github.com<span class="op">/</span>username<span class="op">/</span>AWS_helpers.git <span class="co"># downloads AWS_helpers folder/repo (refresh file explorer to see)</span></span></code></pre>
</div>
<p>Our AWS_helpers.py file can be found in
<code>AWS_helpers/helpers.py</code>. With this file downloaded, you can
call this function via…</p>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="im">import</span> AWS_helpers.helpers <span class="im">as</span> helpers</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a>helpers.get_s3_bucket_size(bucket_name)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="ex">{</span><span class="st">'size_mb'</span><span class="ex">:</span> 0.060057640075683594, <span class="st">'size_gb'</span>: 5.865003913640976e-05}</span></code></pre>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="check-storage-costs-of-bucket">3: Check storage costs of bucket<a class="anchor" aria-label="anchor" href="#check-storage-costs-of-bucket"></a>
</h2>
<hr class="half-width">
<p>To estimate the storage cost of your Amazon S3 bucket directly from a
Jupyter notebook in SageMaker, you can use the following approach. This
method calculates the total size of the bucket and estimates the monthly
storage cost based on AWS S3 pricing.</p>
<p><strong>Note</strong>: AWS S3 pricing varies by region and storage
class. The example below uses the S3 Standard storage class pricing for
the US East (N. Virginia) region as of November 1, 2024. Please verify
the current pricing for your specific region and storage class on the <a href="https://aws.amazon.com/s3/pricing/" class="external-link">AWS S3 Pricing page</a>.</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># AWS S3 Standard Storage pricing for US East (N. Virginia) region</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co"># Pricing tiers as of November 1, 2024</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>first_50_tb_price_per_gb <span class="op">=</span> <span class="fl">0.023</span>  <span class="co"># per GB for the first 50 TB</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a>next_450_tb_price_per_gb <span class="op">=</span> <span class="fl">0.022</span>  <span class="co"># per GB for the next 450 TB</span></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a>over_500_tb_price_per_gb <span class="op">=</span> <span class="fl">0.021</span>  <span class="co"># per GB for storage over 500 TB</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="co"># Calculate the cost based on the size</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="cf">if</span> total_size_gb <span class="op">&lt;=</span> <span class="dv">50</span> <span class="op">*</span> <span class="dv">1024</span>:</span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>    <span class="co"># Total size is within the first 50 TB</span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a>    cost <span class="op">=</span> total_size_gb <span class="op">*</span> first_50_tb_price_per_gb</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a><span class="cf">elif</span> total_size_gb <span class="op">&lt;=</span> <span class="dv">500</span> <span class="op">*</span> <span class="dv">1024</span>:</span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a>    <span class="co"># Total size is within the next 450 TB</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>    cost <span class="op">=</span> (<span class="dv">50</span> <span class="op">*</span> <span class="dv">1024</span> <span class="op">*</span> first_50_tb_price_per_gb) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a>           ((total_size_gb <span class="op">-</span> <span class="dv">50</span> <span class="op">*</span> <span class="dv">1024</span>) <span class="op">*</span> next_450_tb_price_per_gb)</span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a>    <span class="co"># Total size is over 500 TB</span></span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a>    cost <span class="op">=</span> (<span class="dv">50</span> <span class="op">*</span> <span class="dv">1024</span> <span class="op">*</span> first_50_tb_price_per_gb) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb13-18"><a href="#cb13-18" tabindex="-1"></a>           (<span class="dv">450</span> <span class="op">*</span> <span class="dv">1024</span> <span class="op">*</span> next_450_tb_price_per_gb) <span class="op">+</span> <span class="op">\</span></span>
<span id="cb13-19"><a href="#cb13-19" tabindex="-1"></a>           ((total_size_gb <span class="op">-</span> <span class="dv">500</span> <span class="op">*</span> <span class="dv">1024</span>) <span class="op">*</span> over_500_tb_price_per_gb)</span>
<span id="cb13-20"><a href="#cb13-20" tabindex="-1"></a></span>
<span id="cb13-21"><a href="#cb13-21" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated monthly storage cost: $</span><span class="sc">{</span>cost<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb13-22"><a href="#cb13-22" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated annual storage cost: $</span><span class="sc">{</span>cost<span class="op">*</span><span class="dv">12</span><span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="ex">Estimated</span> monthly storage cost: <span class="va">$0</span>.0000</span></code></pre>
</div>
<p>For your convenience, we have also added this code to a helper
function.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>monthly_cost, storage_size_gb <span class="op">=</span> helpers.calculate_s3_storage_cost(bucket_name)</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated monthly cost (</span><span class="sc">{</span>storage_size_gb<span class="sc">:.4f}</span><span class="ss"> GB): $</span><span class="sc">{</span>monthly_cost<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Estimated annual cost (</span><span class="sc">{</span>storage_size_gb<span class="sc">:.4f}</span><span class="ss"> GB): $</span><span class="sc">{</span>monthly_cost<span class="op">*</span><span class="dv">12</span><span class="sc">:.5f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="ex">Estimated</span> monthly cost <span class="er">(</span><span class="ex">0.0001</span> GB<span class="kw">)</span><span class="bu">:</span> <span class="va">$0</span>.00000</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="ex">Estimated</span> annual cost <span class="er">(</span><span class="ex">0.0001</span> GB<span class="kw">)</span><span class="bu">:</span> <span class="va">$0</span>.00002</span></code></pre>
</div>
<p><strong>Important Considerations</strong>:</p>
<ul>
<li>
<strong>Pricing Tiers</strong>: AWS S3 pricing is tiered. The first
50 TB per month is priced at <code>$0.023 per GB</code>, the next 450 TB
at <code>$0.022 per GB</code>, and storage over 500 TB at
<code>$0.021 per GB</code>. Ensure you apply the correct pricing tier
based on your total storage size.</li>
<li>
<strong>Region and Storage Class</strong>: Pricing varies by AWS
region and storage class. The example above uses the S3 Standard storage
class pricing for the US East (N. Virginia) region. Adjust the pricing
variables if your bucket is in a different region or uses a different
storage class.</li>
<li>
<strong>Additional Costs</strong>: This estimation covers storage
costs only. AWS S3 may have additional charges for requests, data
retrievals, and data transfers. For a comprehensive cost analysis,
consider these factors as well.</li>
</ul>
<p>For detailed and up-to-date information on AWS S3 pricing, please
refer to the <a href="https://aws.amazon.com/s3/pricing/" class="external-link">AWS S3 Pricing
page</a>.</p>
</section><section><h2 class="section-heading" id="pushing-new-files-from-notebook-environment-to-bucket">4. Pushing new files from notebook environment to bucket<a class="anchor" aria-label="anchor" href="#pushing-new-files-from-notebook-environment-to-bucket"></a>
</h2>
<hr class="half-width">
<p>As your analysis generates new files, you can upload to your bucket
as demonstrated below. For this demo, you can create a blank
<code>results.txt</code> file to upload to your bucket. To do so, go to
<strong>File</strong> -&gt; <strong>New</strong> -&gt; <strong>Text
file</strong>, and save it out as <code>results.txt</code>.</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="co"># Define the S3 bucket name and the file paths</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>train_file_path <span class="op">=</span> <span class="st">"results.txt"</span> <span class="co"># assuming your file is in root directory of jupyter notebook (check file explorer tab)</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="co"># Upload the training file to a new folder called "results". You can also just place it in the bucket's root directory if you prefer (remove results/ in code below).</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a>s3.upload_file(train_file_path, bucket_name, <span class="st">"results/results.txt"</span>)</span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Files uploaded successfully."</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="ex">Files</span> uploaded successfully.</span></code></pre>
</div>
<p>After uploading, we can view the objects/files available on our
bucket using…</p>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># List and print all objects in the bucket</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>response <span class="op">=</span> s3.list_objects_v2(Bucket<span class="op">=</span>bucket_name)</span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="co"># Check if there are objects in the bucket</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="cf">if</span> <span class="st">'Contents'</span> <span class="kw">in</span> response:</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a>    <span class="cf">for</span> obj <span class="kw">in</span> response[<span class="st">'Contents'</span>]:</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a>        <span class="bu">print</span>(obj[<span class="st">'Key'</span>])  <span class="co"># Print the object's key (its path in the bucket)</span></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The bucket is empty or does not exist."</span>)</span></code></pre>
</div>
<p>Alternatively, we can substitute this for a helper function call as
well.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>file_list <span class="op">=</span> helpers.list_S3_objects(bucket_name)</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a>file_list</span></code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Load data from S3 into memory for efficient storage and
processing.</li>
<li>Periodically check storage usage and costs to manage S3
budgets.</li>
<li>Use SageMaker to upload analysis results and maintain an organized
workflow.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</section></section><section id="aio-Interacting-with-code-repo"><p>Content from <a href="Interacting-with-code-repo.html">Using a GitHub Personal Access Token (PAT) to Push/Pull from a SageMaker Notebook</a></p>
<hr>
<p>Last updated on 2024-11-06 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/Interacting-with-code-repo.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 35 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I securely push/pull code to and from GitHub within a
SageMaker notebook?</li>
<li>What steps are necessary to set up a GitHub PAT for authentication
in SageMaker?</li>
<li>How can I convert notebooks to <code>.py</code> files and ignore
<code>.ipynb</code> files in version control?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Configure Git in a SageMaker notebook to use a GitHub Personal
Access Token (PAT) for HTTPS-based authentication.</li>
<li>Securely handle credentials in a notebook environment using
<code>getpass</code>.</li>
<li>Convert <code>.ipynb</code> files to <code>.py</code> files for
better version control practices in collaborative projects.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="step-0-initial-setup">Step 0: Initial setup<a class="anchor" aria-label="anchor" href="#step-0-initial-setup"></a>
</h2>
<hr class="half-width">
<p>In the previous episode, we cloned our fork that we created during
the <a href="https://uw-madison-datascience.github.io/ML_with_Amazon_SageMaker/#workshop-repository-setup" class="external-link">workshop
setup</a>. In this episode, we’ll see how to push our code to this fork.
Complete these three setup steps before moving foward.</p>
<ol style="list-style-type: decimal">
<li><p>Clone the fork if you haven’t already. See previous
episode.</p></li>
<li><p>Start a new Jupyter notebook, and name it something along the
lines of “Interacting-with-git.ipynb”. We can use the standard
conda_python3 environment since we aren’t doing any training/tuning just
yet.</p></li>
<li><p>Let’s make sure we’re starting at the same directory. Cd to the
root directory of this instance before going further.</p></li>
</ol>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="op">%</span>cd <span class="op">/</span>home<span class="op">/</span>ec2<span class="op">-</span>user<span class="op">/</span>SageMaker<span class="op">/</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="ex">/home/ec2-user/SageMaker</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="step-1-using-a-github-personal-access-token-pat-to-pushpull-from-a-sagemaker-notebook">Step 1: Using a GitHub personal access token (PAT) to push/pull from
a SageMaker notebook<a class="anchor" aria-label="anchor" href="#step-1-using-a-github-personal-access-token-pat-to-pushpull-from-a-sagemaker-notebook"></a>
</h2>
<hr class="half-width">
<p>When working in SageMaker notebooks, you may often need to push code
updates to GitHub repositories. However, SageMaker notebooks are
typically launched with temporary instances that don’t persist
configurations, including SSH keys, across sessions. This makes
HTTPS-based authentication, secured with a GitHub Personal Access Token
(PAT), a practical solution. PATs provide flexibility for authentication
and enable seamless interaction with both public and private
repositories directly from your notebook.</p>
<blockquote>
<p><strong>Important Note</strong>: Personal access tokens are powerful
credentials that grant specific permissions to your GitHub account. To
ensure security, only select the minimum necessary permissions and
handle the token carefully.</p>
</blockquote>
<div class="section level4">
<h4 id="generate-a-personal-access-token-pat-on-github">Generate a personal access token (PAT) on GitHub<a class="anchor" aria-label="anchor" href="#generate-a-personal-access-token-pat-on-github"></a>
</h4>
<ol style="list-style-type: decimal">
<li>Go to <strong>Settings &gt; Developer settings &gt; Personal access
tokens</strong> on GitHub.</li>
<li>Click <strong>Generate new token</strong>, select
<strong>Classic</strong>.</li>
<li>Give your token a descriptive name (e.g., “SageMaker Access Token”)
and set an expiration date if desired for added security.</li>
<li>
<strong>Select the minimum permissions needed</strong>:
<ul>
<li>
<strong>For public repositories</strong>: Choose only
<strong><code>public_repo</code></strong>.</li>
<li>
<strong>For private repositories</strong>: Choose
<strong><code>repo</code></strong> (full control of private
repositories).</li>
<li>Optional permissions, if needed:
<ul>
<li>
<strong><code>repo:status</code></strong>: Access commit status (if
checking status checks).</li>
<li>
<strong><code>workflow</code></strong>: Update GitHub Actions
workflows (only if working with GitHub Actions).</li>
</ul>
</li>
</ul>
</li>
<li>Generate the token and <strong>copy it</strong> (you won’t be able
to see it again).</li>
</ol>
<blockquote>
<p><strong>Caution</strong>: Treat your PAT like a password. Avoid
sharing it or exposing it in your code. Store it securely (e.g., via a
password manager like LastPass) and consider rotating it regularly.</p>
</blockquote>
</div>
<div class="section level4">
<h4 id="use-getpass-to-prompt-for-username-and-pat">Use <code>getpass</code> to prompt for username and PAT<a class="anchor" aria-label="anchor" href="#use-getpass-to-prompt-for-username-and-pat"></a>
</h4>
<p>The <code>getpass</code> library allows you to input your GitHub
username and PAT without exposing them in the notebook. This approach
ensures you’re not hardcoding sensitive information.</p>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="im">import</span> getpass</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co"># Prompt for GitHub username and PAT securely</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>username <span class="op">=</span> <span class="bu">input</span>(<span class="st">"GitHub Username: "</span>)</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>token <span class="op">=</span> getpass.getpass(<span class="st">"GitHub Personal Access Token (PAT): "</span>)</span></code></pre>
</div>
<p><strong>Note</strong>: After running, you may want to comment out the
above code so that you don’t have to enter in your login every time you
run your whole notebook</p>
</div>
</section><section><h2 class="section-heading" id="step-2-configure-git-settings">Step 2: Configure Git settings<a class="anchor" aria-label="anchor" href="#step-2-configure-git-settings"></a>
</h2>
<hr class="half-width">
<p>In your SageMaker or Jupyter notebook environment, run the following
commands to set up your Git user information.</p>
<p>Setting this globally (<code>--global</code>) will ensure the
configuration persists across all repositories in the environment. If
you’re working in a temporary environment, you may need to re-run this
configuration after a restart.</p>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="op">!</span>git config <span class="op">--</span><span class="kw">global</span> user.name <span class="st">"Your name"</span> <span class="co"># This is your GitHub username (or just your name), which will appear in the commit history as the author of the changes.</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="op">!</span>git config <span class="op">--</span><span class="kw">global</span> user.email your_email<span class="op">@</span>wisc.edu <span class="co"># This should match the email associated with your GitHub account so that commits are properly linked to your profile.</span></span></code></pre>
</div>
</section><section><h2 class="section-heading" id="step-3-convert-json--ipynb-files-to--py">Step 3: Convert json .ipynb files to .py<a class="anchor" aria-label="anchor" href="#step-3-convert-json--ipynb-files-to--py"></a>
</h2>
<hr class="half-width">
<p>We’d like to track our notebook files within our AWS_helpers fork.
However, to avoid tracking ipynb files directly, which are formatted as
json, we may want to convert our notebook to .py first (plain text).
Converting notebooks to <code>.py</code> files helps maintain code (and
version-control) readability and minimizes potential issues with
notebook-specific metadata in Git history.</p>
<div class="section level4">
<h4 id="benefits-of-converting-to--py-before-committing">Benefits of converting to <code>.py</code> before Committing<a class="anchor" aria-label="anchor" href="#benefits-of-converting-to--py-before-committing"></a>
</h4>
<ul>
<li>
<strong>Cleaner version control</strong>: <code>.py</code> files
have cleaner diffs and are easier to review and merge in Git.</li>
<li>
<strong>Script compatibility</strong>: Python files are more
compatible with other environments and can run easily from the command
line.</li>
<li>
<strong>Reduced repository size</strong>: <code>.py</code> files are
generally lighter than <code>.ipynb</code> files since they don’t store
outputs or metadata.</li>
</ul>
<p>Here’s how to convert <code>.ipynb</code> files to <code>.py</code>
in SageMaker without needing to export or download files.</p>
<ol style="list-style-type: decimal">
<li>First, install Jupytext.</li>
</ol>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="op">!</span>pip install jupytext</span></code></pre>
</div>
<ol start="2" style="list-style-type: decimal">
<li>Then, run the following command in a notebook cell to convert both
of our notebooks to <code>.py</code> files</li>
</ol>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="co"># Adjust filename(s) if you used something different</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="op">!</span>jupytext <span class="op">--</span>to py Interacting<span class="op">-</span><span class="cf">with</span><span class="op">-</span>S3.ipynb</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="ex">[jupytext]</span> Reading Interacting-with-S3.ipynb in format ipynb</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="ex">[jupytext]</span> Writing Interacting-with-S3.py</span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>If you have multiple notebooks to convert, you can automate the
conversion process by running this code, which converts all
<code>.ipynb</code> files in the current directory to <code>.py</code>
files:</li>
</ol>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co"># List all .ipynb files in the directory</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a>notebooks <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> os.listdir() <span class="cf">if</span> f.endswith(<span class="st">'.ipynb'</span>)]</span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co"># Convert each notebook to .py using jupytext</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="cf">for</span> notebook <span class="kw">in</span> notebooks:</span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a>    output_file <span class="op">=</span> notebook.replace(<span class="st">'.ipynb'</span>, <span class="st">'.py'</span>)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a>    subprocess.run([<span class="st">"jupytext"</span>, <span class="st">"--to"</span>, <span class="st">"py"</span>, notebook, <span class="st">"--output"</span>, output_file])</span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Converted </span><span class="sc">{</span>notebook<span class="sc">}</span><span class="ss"> to </span><span class="sc">{</span>output_file<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<p>For convenience, we have placed this code inside a
<code>convert_files()</code> function in <code>helpers.py</code>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="im">import</span> AWS_helpers.helpers <span class="im">as</span> helpers</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a>helpers.convert_files(direction<span class="op">=</span><span class="st">"notebook_to_python"</span>)</span></code></pre>
</div>
<p><strong>Once converted, we can move our .py files to the AWS_helpers
folder using the file explorer panel in Jupyter Lab.</strong></p>
</div>
</section><section><h2 class="section-heading" id="step-4--add-and-commit--py-files">Step 4. Add and commit .py files<a class="anchor" aria-label="anchor" href="#step-4--add-and-commit--py-files"></a>
</h2>
<hr class="half-width">
<ol style="list-style-type: decimal">
<li>Check status of repo. Make sure you’re in the repo folder before
running the next step.</li>
</ol>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="op">!</span>git status</span></code></pre>
</div>
<p>On branch main Your branch is up to date with ‘origin/main’.</p>
<p>Untracked files: (use “git add <file>…” to include in what will be
committed) Interacting-with-S3.py Interacting-with-git.py</file></p>
<p>nothing added to commit but untracked files present (use “git add” to
track)</p>
<ol start="2" style="list-style-type: decimal">
<li>Add and commit changes</li>
</ol>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="op">!</span>git add . <span class="co"># you may also add files one at a time, for further specificity over the associated commit message</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="op">!</span>git commit <span class="op">-</span>m <span class="st">"Updates from Jupyter notebooks"</span> <span class="co"># in general, your commit message should be more specific!</span></span></code></pre>
</div>
<ol start="3" style="list-style-type: decimal">
<li>Check status</li>
</ol>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="op">!</span>git status</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="ex">On</span> branch main</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="ex">Your</span> branch is ahead of <span class="st">'origin/main'</span> by 1 commit.</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>  <span class="kw">(</span><span class="ex">use</span> <span class="st">"git push"</span> to publish your local commits<span class="kw">)</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="ex">nothing</span> to commit, working tree clean</span></code></pre>
</div>
</section><section><h2 class="section-heading" id="step-5--adding--ipynb-to-gitigore">Step 5. Adding .ipynb to gitigore<a class="anchor" aria-label="anchor" href="#step-5--adding--ipynb-to-gitigore"></a>
</h2>
<hr class="half-width">
<p>Adding <code>.ipynb</code> files to <code>.gitignore</code> is a good
practice if you plan to only commit <code>.py</code> scripts. This will
prevent accidental commits of Jupyter Notebook files across all
subfolders in the repository.</p>
<p>Here’s how to add <code>.ipynb</code> files to
<code>.gitignore</code> to ignore them project-wide:</p>
<ol style="list-style-type: decimal">
<li>
<p><strong>Cd to git repo folder</strong> First make sure we’re in
the repo folder</p>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="op">%</span>cd AWS_helpers</span></code></pre>
</div>
</li>
<li><p><strong>Create the <code>.gitignore</code> file</strong>: This
file will be hidden in Jupyter (since it starts with “.”), but you can
verify it exists using <code>ls</code>.
<code>python     !touch .gitignore     !ls -a</code></p></li>
<li>
<p><strong>Add <code>.ipynb</code> files to
<code>.gitignore</code></strong>:</p>
<p>You can add this line using a command within your notebook:</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">".gitignore"</span>, <span class="st">"a"</span>) <span class="im">as</span> gitignore:</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>    gitignore.write(<span class="st">"</span><span class="ch">\n</span><span class="st"># Ignore all Jupyter Notebook files</span><span class="ch">\n</span><span class="st">*.ipynb</span><span class="ch">\n</span><span class="st">"</span>)</span></code></pre>
</div>
<p>View file contents <code>python  !cat .gitignore</code></p>
</li>
<li>
<p><strong>Ignore other common temp files</strong> While we’re at
it, let’s ignore other common files that can clutter repos, such as
cache folders and temporary files.
<code>python     with open(".gitignore", "a") as gitignore:         gitignore.write("\n# Ignore cache and temp files\n__pycache__/\n*.tmp\n*.log\n")</code></p>
<p>View file contents <code>python  !cat .gitignore</code></p>
</li>
<li>
<p><strong>Add and commit the <code>.gitignore</code>
file</strong>:</p>
<p>Add and commit the updated <code>.gitignore</code> file to ensure
it’s applied across the repository.
<code>python  !git add .gitignore  !git commit -m "Add .ipynb files to .gitignore to ignore notebooks"</code></p>
</li>
</ol>
<p>This setup will: - Prevent all <code>.ipynb</code> files from being
tracked by Git. - Keep your repository cleaner, containing only
<code>.py</code> scripts for easier version control and reduced
repository size.</p>
</section><section><h2 class="section-heading" id="step-6--merging-local-changes-with-remotegithub">Step 6. Merging local changes with remote/GitHub<a class="anchor" aria-label="anchor" href="#step-6--merging-local-changes-with-remotegithub"></a>
</h2>
<hr class="half-width">
<p>Our local changes have now been committed, and we can begin the
process of mergining with the remoate main branch. Before we try to push
our changes, it’s good practice to first to a pull. This is critical
when working on a collaborate repo with multiple users, so that you
don’t miss any updates from other team members.</p>
<div class="section level3">
<h3 id="pull-the-latest-changes-from-the-main-branch">1. Pull the latest changes from the main branch<a class="anchor" aria-label="anchor" href="#pull-the-latest-changes-from-the-main-branch"></a>
</h3>
<p>There are a few different options for pulling the remote code into
your local version. The best pull strategy depends on your workflow and
the history structure you want to maintain. Here’s a breakdown to help
you decide:</p>
<ul>
<li>Merge (pull.rebase false): Combines the remote changes into your
local branch as a merge commit.
<ul>
<li>
<strong>Use if</strong>: You’re okay with having merge commits in
your history, which indicate where you pulled in remote changes. This is
the default and is usually the easiest for team collaborations,
especially if conflicts arise.</li>
</ul>
</li>
<li>Rebase (pull.rebase true): Replays your local changes on top of the
updated main branch, resulting in a linear history.
<ul>
<li>
<strong>Use if</strong>: You prefer a clean, linear history without
merge commits. Rebase is useful if you like to keep your branch history
as if all changes happened sequentially.</li>
</ul>
</li>
<li>Fast-forward only (pull.ff only): Only pulls if the local branch can
fast-forward to the remote without diverging (no new commits locally).
<ul>
<li>
<strong>Use if</strong>: You only want to pull updates if no
additional commits have been made locally. This can be helpful to avoid
unintended merges when your branch hasn’t diverged.</li>
</ul>
</li>
</ul>
<div class="section level4">
<h4 id="recommended-for-most-users">Recommended for Most Users<a class="anchor" aria-label="anchor" href="#recommended-for-most-users"></a>
</h4>
<p>If you’re collaborating and want simplicity, <strong>merge
(pull.rebase false)</strong> is often the most practical option. This
will ensure you get remote changes with a merge commit that captures the
history of integration points. For those who prefer a more streamlined
history and are comfortable with Git, <strong>rebase (pull.rebase
true)</strong> can be ideal but may require more careful conflict
handling.</p>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="op">!</span>git config pull.rebase false <span class="co"># Combines the remote changes into your local branch as a merge commit.</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="op">!</span>git pull origin main</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="ex">From</span> https://github.com/qualiaMachine/AWS_helpers</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a> <span class="ex">*</span> branch            main       <span class="at">-</span><span class="op">&gt;</span> FETCH_HEAD</span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="ex">Already</span> up to date.</span></code></pre>
</div>
<p>If you get merge conflicts, be sure to resolve those before moving
forward (e.g., use git checkout -&gt; add -&gt; commit). You can skip
the below code if you don’t have any conflicts.</p>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="co"># Keep your local changes in one conflicting file</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="co"># !git checkout --ours Interacting-with-git.py</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="co"># Keep remote version for the other conflicting file</span></span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a><span class="co"># !git checkout --theirs Interacting-with-git.py</span></span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" tabindex="-1"></a><span class="co"># # Stage the files to mark the conflicts as resolved</span></span>
<span id="cb18-8"><a href="#cb18-8" tabindex="-1"></a><span class="co"># !git add Interacting-with-git.py</span></span>
<span id="cb18-9"><a href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" tabindex="-1"></a><span class="co"># # Commit the merge result</span></span>
<span id="cb18-11"><a href="#cb18-11" tabindex="-1"></a><span class="co"># !git commit -m "Resolved merge conflicts by keeping local changes"</span></span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="push-changes-using-pat-creditials">2. Push changes using PAT creditials<a class="anchor" aria-label="anchor" href="#push-changes-using-pat-creditials"></a>
</h3>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="co"># Push with embedded credentials from getpass (avoids interactive prompt)</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>github_url <span class="op">=</span> <span class="st">'github.com/username/AWS_helpers.git'</span> <span class="co"># replace username with your own. THe full address for your fork can be found under Code -&gt; Clone -&gt; HTTPS (remote the https:// before the rest of the address)</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a><span class="op">!</span>git push https:<span class="op">//</span>{username}:{token}<span class="op">@</span>{github_url} main</span></code></pre>
</div>
<p>After pushing, you should navigate back to your fork on GitHub to
verify everything worked (e.g., <a href="https://github.com/username/AWS_helpers/tree/main" class="external-link uri">https://github.com/username/AWS_helpers/tree/main</a>)</p>
</div>
</section><section><h2 class="section-heading" id="step-7-pulling--py-files-and-converting-back-to-notebook-format">Step 7: Pulling .py files and converting back to notebook
format<a class="anchor" aria-label="anchor" href="#step-7-pulling--py-files-and-converting-back-to-notebook-format"></a>
</h2>
<hr class="half-width">
<p>Let’s assume you’ve taken a short break from your work, and you would
like to start again by pulling in your code repo. If you’d like to work
with notebook files again, you can again use jupytext to convert your
<code>.py</code> files back to <code>.ipynb</code></p>
<p>This command will create
<code>03_Data-storage-and-access-via-buckets-test.ipynb</code> in the
current directory, converting the Python script to a Jupyter Notebook
format. Jupytext handles the conversion gracefully without expecting the
<code>.py</code> file to be in JSON format.</p>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="co"># Replace 'your_script.py' with your actual filename</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="op">!</span>jupytext <span class="op">--</span>to notebook Data<span class="op">-</span>storage<span class="op">-</span><span class="kw">and</span><span class="op">-</span>access<span class="op">-</span>via<span class="op">-</span>buckets.py <span class="op">--</span>output Data<span class="op">-</span>storage<span class="op">-</span><span class="kw">and</span><span class="op">-</span>access<span class="op">-</span>via<span class="op">-</span>buckets<span class="op">-</span>test.ipynb</span></code></pre>
</div>
<div class="section level3">
<h3 id="applying-to-all--py-files">Applying to all .py files<a class="anchor" aria-label="anchor" href="#applying-to-all--py-files"></a>
</h3>
<p>To convert all of your .py files to notebooks, you can use the
following code:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="im">import</span> subprocess</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="co"># List all .py files in the directory</span></span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>scripts <span class="op">=</span> [f <span class="cf">for</span> f <span class="kw">in</span> os.listdir() <span class="cf">if</span> f.endswith(<span class="st">'.py'</span>)]</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a><span class="co"># Convert each .py file to .ipynb using jupytext</span></span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a><span class="cf">for</span> script <span class="kw">in</span> scripts:</span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>    output_file <span class="op">=</span> script.replace(<span class="st">'.py'</span>, <span class="st">'.ipynb'</span>)</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>    subprocess.run([<span class="st">"jupytext"</span>, <span class="st">"--to"</span>, <span class="st">"notebook"</span>, script, <span class="st">"--output"</span>, output_file])</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Converted </span><span class="sc">{</span>script<span class="sc">}</span><span class="ss"> to </span><span class="sc">{</span>output_file<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>Use a GitHub PAT for HTTPS-based authentication in temporary
SageMaker notebook instances.</li>
<li>Securely enter sensitive information in notebooks using
<code>getpass</code>.</li>
<li>Converting <code>.ipynb</code> files to <code>.py</code> files helps
with cleaner version control and easier review of changes.</li>
<li>Adding <code>.ipynb</code> files to <code>.gitignore</code> keeps
your repository organized and reduces storage.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section><section id="aio-Training-models-in-SageMaker-notebooks"><p>Content from <a href="Training-models-in-SageMaker-notebooks.html">Training models in SageMaker notebooks</a></p>
<hr>
<p>Last updated on 2024-11-03 |

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/episodes/Training-models-in-SageMaker-notebooks.md" class="external-link">Edit this page <i aria-hidden="true" data-feather="edit"></i></a></p>
<p>Estimated time: <i aria-hidden="true" data-feather="clock"></i> 30 minutes</p>
<div class="text-end">
          <button role="button" aria-pressed="false" tabindex="0" id="expand-code" class="pull-right" data-expand="Expand All Solutions " data-collapse="Collapse All Solutions "> Expand All Solutions <i aria-hidden="true" data-feather="plus"></i></button>
        </div>
<div class="overview card">
<h2 class="card-header">Overview</h2>
<div class="row g-0">
<div class="col-md-4">
<div class="card-body">
<div class="inner">
<h3 class="card-title">Questions</h3>
<ul>
<li>How can I initialize the SageMaker environment and set up data in
S3?</li>
<li>What are the differences between local training and
SageMaker-managed training?</li>
<li>How do Estimator classes in SageMaker streamline the training
process for various frameworks?</li>
</ul>
</div>
</div>
</div>
<div class="col-md-8">
<div class="card-body">
<div class="inner bordered">
<h3 class="card-title">Objectives</h3>
<ul>
<li>Set up and initialize the SageMaker environment, including roles,
sessions, and S3 data.</li>
<li>Understand the difference between training locally in a SageMaker
notebook and using SageMaker’s managed infrastructure.</li>
<li>Learn to configure and use SageMaker’s Estimator classes for
different frameworks (e.g., XGBoost, PyTorch, SKLearn).</li>
<li>Compare performance, cost, and setup between custom scripts and
built-in images in SageMaker.</li>
<li>Conduct training with data stored in S3 and monitor training job
status using the SageMaker console.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<section><h2 class="section-heading" id="initialize-sagemaker-environment">Initialize SageMaker environment<a class="anchor" aria-label="anchor" href="#initialize-sagemaker-environment"></a>
</h2>
<hr class="half-width">
<p>This code initializes the AWS SageMaker environment by defining the
SageMaker role, session, and S3 client. It also specifies the S3 bucket
and key for accessing the Titanic training dataset stored in an S3
bucket.</p>
<div class="section level4">
<h4 id="boto3-api">Boto3 API<a class="anchor" aria-label="anchor" href="#boto3-api"></a>
</h4>
<blockquote>
<p>Boto3 is the official AWS SDK for Python, allowing developers to
interact programmatically with AWS services like S3, EC2, and Lambda. It
provides both high-level and low-level APIs, making it easy to manage
AWS resources and automate tasks. With built-in support for paginators,
waiters, and session management, Boto3 simplifies working with AWS
credentials, regions, and IAM permissions. It’s ideal for automating
cloud operations and integrating AWS services into Python
applications.</p>
</blockquote>
<div class="codewrapper sourceCode" id="cb1">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="im">import</span> boto3</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="im">import</span> sagemaker</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="im">from</span> sagemaker <span class="im">import</span> get_execution_role</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co"># Initialize the SageMaker role (will reflect notebook instance's policy)</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a>role <span class="op">=</span> sagemaker.get_execution_role()</span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'role = </span><span class="sc">{</span>role<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a><span class="co"># Create a SageMaker session to manage interactions with Amazon SageMaker, such as training jobs, model deployments, and data input/output.</span></span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a>session <span class="op">=</span> sagemaker.Session()</span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a><span class="co"># Initialize an S3 client to interact with Amazon S3, allowing operations like uploading, downloading, and managing objects and buckets.</span></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a>s3 <span class="op">=</span> boto3.client(<span class="st">'s3'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="co"># Define the S3 bucket that we will load from</span></span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a>bucket <span class="op">=</span> <span class="st">'titanic-dataset-test'</span>  <span class="co"># replace with your S3 bucket name</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a><span class="co"># Define train/test filenames</span></span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>train_filename <span class="op">=</span> <span class="st">'titanic_train.csv'</span></span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>test_filename <span class="op">=</span> <span class="st">'titanic_test.csv'</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb2">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="ex">sagemaker.config</span> INFO <span class="at">-</span> Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="ex">sagemaker.config</span> INFO <span class="at">-</span> Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="ex">role</span> = arn:aws:iam::183295408236:role/ml-sagemaker-use</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="download-copy-into-notebook-environment">Download copy into notebook environment<a class="anchor" aria-label="anchor" href="#download-copy-into-notebook-environment"></a>
</h3>
<p>If you have larger dataset (&gt; 1GB), you may want to skip this step
and always read directly into memory. However, for smaller datasets, it
can be convenient to have a “local” copy (i.e., one that you store in
your notebook’s instance).</p>
<p>Download data from S3 to notebook environment. You may need to hit
refresh on the file explorer panel to the left to see this file. If you
get any permission issues…</p>
<ul>
<li>check that you have selected the appropriate policy for this
notebook</li>
<li>check that your bucket has the appropriate policy permissions</li>
</ul>
<div class="codewrapper sourceCode" id="cb3">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="co"># Define the S3 bucket and file location</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>file_key <span class="op">=</span> <span class="ss">f"data/</span><span class="sc">{</span>train_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Path to your file in the S3 bucket</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>local_file_path <span class="op">=</span> <span class="ss">f"./</span><span class="sc">{</span>train_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Local path to save the file</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co"># Download the file using the s3 client variable we initialized earlier</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>s3.download_file(bucket, file_key, local_file_path)</span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"File downloaded:"</span>, local_file_path)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb4">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="ex">File</span> downloaded: ./titanic_train.csv</span></code></pre>
</div>
<p>We can do the same for the test set.</p>
<div class="codewrapper sourceCode" id="cb5">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># Define the S3 bucket and file location</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>file_key <span class="op">=</span> <span class="ss">f"data/</span><span class="sc">{</span>test_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Path to your file in the S3 bucket. W</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>local_file_path <span class="op">=</span> <span class="ss">f"./</span><span class="sc">{</span>test_filename<span class="sc">}</span><span class="ss">"</span>  <span class="co"># Local path to save the file</span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co"># Initialize the S3 client and download the file</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>s3.download_file(bucket, file_key, local_file_path)</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"File downloaded:"</span>, local_file_path)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb6">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="ex">File</span> downloaded: ./titanic_test.csv</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="get-code-train-and-tune-scripts-from-git-repo-">Get code (train and tune scripts) from git repo.<a class="anchor" aria-label="anchor" href="#get-code-train-and-tune-scripts-from-git-repo-"></a>
</h3>
<p>We recommend you DO NOT put data inside your code repo, as version
tracking for data files takes up unnecessary storage in this notebook
instance. Instead, store your data in a separte S3 bucket. We have a
data folder in our repo only as a means to initially hand you the data
for this tutorial.</p>
<p>Check to make sure we’re in our EC2 root folder
(<code>/home/ec2-user/SageMaker</code>).</p>
<div class="codewrapper sourceCode" id="cb7">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="op">!</span>pwd</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb8">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="ex">/home/ec2-user/SageMaker/test_AWS</span></span></code></pre>
</div>
<p>If not, change directory using <code>%cd</code>.</p>
<div class="codewrapper sourceCode" id="cb9">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="op">%</span>cd <span class="op">/</span>home<span class="op">/</span>ec2<span class="op">-</span>user<span class="op">/</span>SageMaker<span class="op">/</span></span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="op">!</span>pwd</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb10">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="ex">/home/ec2-user/SageMaker</span></span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="ex">/home/ec2-user/SageMaker</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb11">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="op">!</span>git clone https:<span class="op">//</span>github.com<span class="op">/</span>UW<span class="op">-</span>Madison<span class="op">-</span>DataScience<span class="op">/</span>test_AWS.git</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb12">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="ex">fatal:</span> destination path <span class="st">'test_AWS'</span> already exists and is not an empty directory.</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="testing-train-py-on-this-notebooks-instance">Testing train.py on this notebook’s instance<a class="anchor" aria-label="anchor" href="#testing-train-py-on-this-notebooks-instance"></a>
</h3>
<p>Notebook instances in SageMaker allow us allocate more powerful
instances (or many instances) to machine learning jobs that require
extra power, GPUs, or benefit from parallelization. Before we try
exploiting this extra power, it is essential that we test our code
thoroughly. We don’t want to waste unnecessary compute cycles and
resources on jobs that produce bugs instead of insights. If you need to,
you can use a subset of your data to run quicker tests. You can also
select a slightly better instance resource if your current instance
insn’t meeting your needs. See the <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">Instances
for ML spreadsheet</a> for guidance.</p>
<div class="section level4">
<h4 id="logging-runtime-instance-info">Logging runtime &amp; instance info<a class="anchor" aria-label="anchor" href="#logging-runtime-instance-info"></a>
</h4>
<p>To compare our local runtime with future experiments, we’ll need to
know what instance was used, as this will greatly impact runtime in many
cases. We can extract the instance name for this notebook using…</p>
<div class="codewrapper sourceCode" id="cb13">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a><span class="co"># Replace with your notebook instance name.</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="co"># This does NOT refer to specific ipynb fils, but to the notebook instance opened from SageMaker.</span></span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>notebook_instance_name <span class="op">=</span> <span class="st">'Titanic-ML-Notebook'</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="co"># Initialize SageMaker client</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a>sagemaker_client <span class="op">=</span> boto3.client(<span class="st">'sagemaker'</span>)</span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="co"># Describe the notebook instance</span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a>response <span class="op">=</span> sagemaker_client.describe_notebook_instance(NotebookInstanceName<span class="op">=</span>notebook_instance_name)</span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a><span class="co"># Display the status and instance type</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Notebook Instance '</span><span class="sc">{</span>notebook_instance_name<span class="sc">}</span><span class="ss">' status: </span><span class="sc">{</span>response[<span class="st">'NotebookInstanceStatus'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a>local_instance <span class="op">=</span> response[<span class="st">'InstanceType'</span>]</span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Instance Type: </span><span class="sc">{</span>local_instance<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb14">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a><span class="ex">Notebook</span> Instance <span class="st">'Titanic-ML-Notebook'</span> status: InService</span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="ex">Instance</span> Type: ml.t3.medium</span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="helper-get_notebook_instance_info">Helper: <code>get_notebook_instance_info()</code>
<a class="anchor" aria-label="anchor" href="#helper-get_notebook_instance_info"></a>
</h4>
<p>You can also use the <code>get_notebook_instance_info()</code>
function found in <code>AWS_helpers.py</code> to retrieve this info for
your own project.</p>
<div class="codewrapper sourceCode" id="cb15">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="im">from</span> test_AWS.scripts.AWS_helpers <span class="im">import</span> get_notebook_instance_info</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a>get_notebook_instance_info(notebook_instance_name)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb16">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a><span class="ex">{</span><span class="st">'Status'</span><span class="ex">:</span> <span class="st">'InService'</span>, <span class="st">'InstanceType'</span>: <span class="st">'ml.t3.medium'</span>}</span></code></pre>
</div>
<p>Test train.py on this notebook’s instance (or when possible, on your
own machine) before doing anything more complicated (e.g.,
hyperparameter tuning on multiple instances)</p>
<div class="codewrapper sourceCode" id="cb17">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a><span class="op">!</span>pip install xgboost <span class="co"># need to add this to environment to run train.py</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb18">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a><span class="ex">Requirement</span> already satisfied: xgboost in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages <span class="er">(</span><span class="ex">2.1.2</span><span class="kw">)</span></span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a><span class="ex">Requirement</span> already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages <span class="er">(</span><span class="ex">from</span> xgboost<span class="kw">)</span> <span class="kw">(</span><span class="ex">1.26.4</span><span class="kw">)</span></span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a><span class="ex">Requirement</span> already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages <span class="er">(</span><span class="ex">from</span> xgboost<span class="kw">)</span> <span class="kw">(</span><span class="ex">1.14.1</span><span class="kw">)</span></span></code></pre>
</div>
<p>Here’s what each argument does in detail for the below call to
train_xgboost.py:</p>
<ul>
<li><p><code>--max_depth 5</code>: Sets the maximum depth of each tree
in the model to 5. Limiting tree depth helps control model complexity
and can reduce overfitting, especially on small datasets.</p></li>
<li><p><code>--eta 0.1</code>: Sets the learning rate to 0.1, which
scales the contribution of each tree to the final model. A smaller
learning rate often requires more rounds to converge but can lead to
better performance.</p></li>
<li><p><code>--subsample 0.8</code>: Specifies that 80% of the training
data will be randomly sampled to build each tree. Subsampling can help
with model robustness by preventing overfitting and increasing
variance.</p></li>
<li><p><code>--colsample_bytree 0.8</code>: Specifies that 80% of the
features will be randomly sampled for each tree, enhancing the model’s
ability to generalize by reducing feature reliance.</p></li>
<li><p><code>--num_round 100</code>: Sets the number of boosting rounds
(trees) to 100. More rounds typically allow for a more refined model,
but too many rounds can lead to overfitting.</p></li>
<li><p><code>--train ./train.csv</code>: Points to the location of the
training data, <code>train.csv</code>, which will be used to train the
model.</p></li>
</ul>
<div class="codewrapper sourceCode" id="cb19">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a><span class="im">import</span> time <span class="im">as</span> t <span class="co"># we'll use the time package to measure runtime</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>start_time <span class="op">=</span> t.time()</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="co"># Run the script and pass arguments directly</span></span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="op">%</span>run test_AWS<span class="op">/</span>scripts<span class="op">/</span>train_xgboost.py <span class="op">--</span>max_depth <span class="dv">5</span> <span class="op">--</span>eta <span class="fl">0.1</span> <span class="op">--</span>subsample <span class="fl">0.8</span> <span class="op">--</span>colsample_bytree <span class="fl">0.8</span> <span class="op">--</span>num_round <span class="dv">100</span> <span class="op">--</span>train .<span class="op">/</span>titanic_train.csv</span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="co"># Measure and print the time taken</span></span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total local runtime: </span><span class="sc">{</span>t<span class="sc">.</span>time() <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type = </span><span class="sc">{</span>local_instance<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb20">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a><span class="ex">Train</span> size: <span class="er">(</span><span class="ex">569,</span> 8<span class="kw">)</span></span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="ex">Val</span> size: <span class="er">(</span><span class="ex">143,</span> 8<span class="kw">)</span></span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="ex">Training</span> time: 0.06 seconds</span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="ex">Model</span> saved to ./xgboost-model</span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="ex">Total</span> local runtime: 1.01 seconds, instance_type = ml.t3.medium</span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a><span class="ex">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:265:</span> FutureWarning: Your system has an old version of glibc <span class="er">(</span><span class="op">&lt;</span> 2.28<span class="kw">)</span><span class="bu">.</span> We will stop supporting Linux distros with glibc older than 2.28 after <span class="pp">**</span>May 31, 2025<span class="pp">**</span>. Please upgrade to a recent Linux distro <span class="er">(</span><span class="ex">with</span> glibc 2.28+<span class="kw">)</span> <span class="ex">to</span> use future versions of XGBoost.</span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a><span class="ex">Note:</span> You have installed the <span class="st">'manylinux2014'</span> variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the <span class="st">'manylinux_2_28'</span> variant.</span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a>  <span class="ex">warnings.warn</span><span class="er">(</span></span></code></pre>
</div>
<p>Training on this relatively small dataset should take less than a
minute, but as we scale up with larger datasets and more complex models
in SageMaker, tracking both training time and total runtime becomes
essential for efficient debugging and resource management.</p>
<p><strong>Note</strong>: Our training script includes print statements
to monitor dataset size and track time spent specifically on training,
which provides insights into resource usage for model development. We
recommend incorporating similar logging to track not only training time
but also total runtime, which includes additional steps like data
loading, evaluation, and saving results. Tracking both can help you
pinpoint bottlenecks and optimize your workflow as projects grow in size
and complexity, especially when scaling with SageMaker’s distributed
resources.</p>
</div>
</div>
</section><section><h2 class="section-heading" id="training-via-sagemaker-using-notebook-as-controller---custom-train-py-script">Training via SageMaker (using notebook as controller) - custom
train.py script<a class="anchor" aria-label="anchor" href="#training-via-sagemaker-using-notebook-as-controller---custom-train-py-script"></a>
</h2>
<hr class="half-width">
<p>Unlike “local” training (using this notebook), this next approach
leverages SageMaker’s managed infrastructure to handle resources,
parallelism, and scalability. By specifying instance parameters, such as
instance_count and instance_type, you can control the resources
allocated for training.</p>
<p>In this example, we start with one ml.m5.large instance, which is
suitable for small- to medium-sized datasets and simpler models. Using a
single instance is often cost-effective and sufficient for initial
testing, allowing for straightforward scaling up to more powerful
instance types or multiple instances if training takes too long. See
here for further guidance on selecting an appropriate instance for your
data/model: <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">EC2
Instances for ML</a></p>
<div class="section level3">
<h3 id="overview-of-estimator-classes-in-sagemaker">Overview of Estimator Classes in SageMaker<a class="anchor" aria-label="anchor" href="#overview-of-estimator-classes-in-sagemaker"></a>
</h3>
<p>In SageMaker, <strong>Estimator</strong> classes streamline the
configuration and training of models on managed instances. Each
Estimator can work with custom scripts and be enhanced with additional
dependencies by specifying a <code>requirements.txt</code> file, which
is automatically installed at the start of training. Here’s a breakdown
of some commonly used Estimator classes in SageMaker:</p>
<div class="section level4">
<h4 id="estimator-base-class">1. <strong><code>Estimator</code> (Base Class)</strong>
<a class="anchor" aria-label="anchor" href="#estimator-base-class"></a>
</h4>
<ul>
<li>
<strong>Purpose</strong>: General-purpose for custom Docker
containers or defining an image URI directly.</li>
<li>
<strong>Configuration</strong>: Requires specifying an
<code>image_uri</code> and custom entry points.</li>
<li>
<strong>Dependencies</strong>: You can use
<code>requirements.txt</code> to install Python packages or configure a
custom Docker container with pre-baked dependencies.</li>
<li>
<strong>Ideal Use Cases</strong>: Custom algorithms or models that
need tailored environments not covered by built-in containers.</li>
</ul>
</div>
<div class="section level4">
<h4 id="xgboost-estimator">2. <strong><code>XGBoost</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#xgboost-estimator"></a>
</h4>
<ul>
<li>
<strong>Purpose</strong>: Provides an optimized container
specifically for XGBoost models.</li>
<li>
<strong>Configuration</strong>:
<ul>
<li>
<code>entry_point</code>: Path to a custom script, useful for
additional preprocessing or unique training workflows.</li>
<li>
<code>framework_version</code>: Select XGBoost version, e.g.,
<code>"1.5-1"</code>.</li>
<li>
<code>dependencies</code>: Specify additional packages through
<code>requirements.txt</code> to enhance preprocessing capabilities or
incorporate auxiliary libraries.</li>
</ul>
</li>
<li>
<strong>Ideal Use Cases</strong>: Tabular data modeling using
gradient-boosted trees; cases requiring custom preprocessing or tuning
logic.</li>
</ul>
</div>
<div class="section level4">
<h4 id="pytorch-estimator">3. <strong><code>PyTorch</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#pytorch-estimator"></a>
</h4>
<ul>
<li>
<strong>Purpose</strong>: Configures training jobs with PyTorch for
deep learning tasks.</li>
<li>
<strong>Configuration</strong>:
<ul>
<li>
<code>entry_point</code>: Training script with model architecture
and training loop.</li>
<li>
<code>instance_type</code>: e.g., <code>ml.p3.2xlarge</code> for GPU
acceleration.</li>
<li>
<code>framework_version</code> and <code>py_version</code>: Define
specific versions.</li>
<li>
<code>dependencies</code>: Install any required packages via
<code>requirements.txt</code> to support advanced data processing, data
augmentation, or custom layer implementations.</li>
</ul>
</li>
<li>
<strong>Ideal Use Cases</strong>: Deep learning models, particularly
complex networks requiring GPUs and custom layers.</li>
</ul>
</div>
<div class="section level4">
<h4 id="sklearn-estimator">4. <strong><code>SKLearn</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#sklearn-estimator"></a>
</h4>
<ul>
<li>
<strong>Purpose</strong>: Supports scikit-learn workflows for data
preprocessing and classical machine learning.</li>
<li>
<strong>Configuration</strong>:
<ul>
<li>
<code>entry_point</code>: Python script to handle feature
engineering, preprocessing, or training.</li>
<li>
<code>framework_version</code>: Version of scikit-learn, e.g.,
<code>"1.0-1"</code>.</li>
<li>
<code>dependencies</code>: Use <code>requirements.txt</code> to
install any additional Python packages required by the training
script.</li>
</ul>
</li>
<li>
<strong>Ideal Use Cases</strong>: Classical ML workflows, extensive
preprocessing, or cases where additional libraries (e.g., pandas, numpy)
are essential.</li>
</ul>
</div>
<div class="section level4">
<h4 id="tensorflow-estimator">5. <strong><code>TensorFlow</code> Estimator</strong>
<a class="anchor" aria-label="anchor" href="#tensorflow-estimator"></a>
</h4>
<ul>
<li>
<strong>Purpose</strong>: Designed for training and deploying
TensorFlow models.</li>
<li>
<strong>Configuration</strong>:
<ul>
<li>
<code>entry_point</code>: Script for model definition and training
process.</li>
<li>
<code>instance_type</code>: Select based on dataset size and
computational needs.</li>
<li>
<code>dependencies</code>: Additional dependencies can be listed in
<code>requirements.txt</code> to install TensorFlow add-ons, custom
layers, or preprocessing libraries.</li>
</ul>
</li>
<li>
<strong>Ideal Use Cases</strong>: NLP, computer vision, and transfer
learning applications in TensorFlow.</li>
</ul>
</div>
<div class="section level4">
<h4 id="configuring-custom-environments-with-requirements-txt">Configuring Custom Environments with
<code>requirements.txt</code>
<a class="anchor" aria-label="anchor" href="#configuring-custom-environments-with-requirements-txt"></a>
</h4>
<p>For all these Estimators, adding a <code>requirements.txt</code> file
under <code>dependencies</code> ensures that additional packages are
installed before training begins. This approach allows the use of
specific libraries that may be critical for custom preprocessing,
feature engineering, or model modifications. Here’s how to include
it:</p>
<div class="codewrapper sourceCode" id="cb21">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>sklearn_estimator <span class="op">=</span> SKLearn(</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">"train_script.py"</span>,</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a>    instance_count<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb21-5"><a href="#cb21-5" tabindex="-1"></a>    instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span>,</span>
<span id="cb21-6"><a href="#cb21-6" tabindex="-1"></a>    output_path<span class="op">=</span><span class="st">"s3://your-bucket/output"</span>,</span>
<span id="cb21-7"><a href="#cb21-7" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.0-1"</span>,</span>
<span id="cb21-8"><a href="#cb21-8" tabindex="-1"></a>    dependencies<span class="op">=</span>[<span class="st">'requirements.txt'</span>],  <span class="co"># Adding custom dependencies</span></span>
<span id="cb21-9"><a href="#cb21-9" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb21-10"><a href="#cb21-10" tabindex="-1"></a>        <span class="st">"max_depth"</span>: <span class="dv">5</span>,</span>
<span id="cb21-11"><a href="#cb21-11" tabindex="-1"></a>        <span class="st">"eta"</span>: <span class="fl">0.1</span>,</span>
<span id="cb21-12"><a href="#cb21-12" tabindex="-1"></a>        <span class="st">"subsample"</span>: <span class="fl">0.8</span>,</span>
<span id="cb21-13"><a href="#cb21-13" tabindex="-1"></a>        <span class="st">"num_round"</span>: <span class="dv">100</span></span>
<span id="cb21-14"><a href="#cb21-14" tabindex="-1"></a>    }</span>
<span id="cb21-15"><a href="#cb21-15" tabindex="-1"></a>)</span></code></pre>
</div>
<p>This setup simplifies training, allowing you to maintain custom
environments directly within SageMaker’s managed containers, without
needing to build and manage your own Docker images.</p>
</div>
</div>
<div class="section level3">
<h3 id="more-information-on-pre-built-environments">More information on pre-built environments<a class="anchor" aria-label="anchor" href="#more-information-on-pre-built-environments"></a>
</h3>
<p>he <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html" class="external-link">AWS
SageMaker Documentation</a> provides lists of pre-built container images
for each framework and their standard libraries, including details on
pre-installed packages.</p>
<p>For this deployment, we configure the “XGBoost” estimator with a
custom training script, train_xgboost.py, and define hyperparameters
directly within the SageMaker setup. Here’s the full code, with some
additional explanation following the code.</p>
<div class="codewrapper sourceCode" id="cb22">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a><span class="im">from</span> sagemaker.inputs <span class="im">import</span> TrainingInput</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a><span class="im">from</span> sagemaker.xgboost.estimator <span class="im">import</span> XGBoost</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="co"># Define instance type/count we'll use for training</span></span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a>instance_count<span class="op">=</span><span class="dv">1</span> <span class="co"># always start with 1. Rarely is parallelized training justified with data &lt; 50 GB.</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a><span class="co"># Define S3 paths for input and output</span></span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a>train_s3_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/data/</span><span class="sc">{</span>train_filename<span class="sc">}</span><span class="ss">'</span></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" tabindex="-1"></a><span class="co"># we'll store all results in a subfolder called xgboost on our bucket. This folder will automatically be created if it doesn't exist already.</span></span>
<span id="cb22-12"><a href="#cb22-12" tabindex="-1"></a>output_folder <span class="op">=</span> <span class="st">'xgboost'</span></span>
<span id="cb22-13"><a href="#cb22-13" tabindex="-1"></a>output_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>output_folder<span class="sc">}</span><span class="ss">/'</span> </span>
<span id="cb22-14"><a href="#cb22-14" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" tabindex="-1"></a><span class="co"># Set up the SageMaker XGBoost Estimator with custom script</span></span>
<span id="cb22-16"><a href="#cb22-16" tabindex="-1"></a>xgboost_estimator <span class="op">=</span> XGBoost(</span>
<span id="cb22-17"><a href="#cb22-17" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">'train_xgboost.py'</span>,      <span class="co"># Custom script path</span></span>
<span id="cb22-18"><a href="#cb22-18" tabindex="-1"></a>    source_dir<span class="op">=</span><span class="st">'test_AWS/scripts'</span>,               <span class="co"># Directory where your script is located</span></span>
<span id="cb22-19"><a href="#cb22-19" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb22-20"><a href="#cb22-20" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,</span>
<span id="cb22-21"><a href="#cb22-21" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb22-22"><a href="#cb22-22" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb22-23"><a href="#cb22-23" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb22-24"><a href="#cb22-24" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.5-1"</span>,           <span class="co"># Use latest supported version for better compatibility</span></span>
<span id="cb22-25"><a href="#cb22-25" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb22-26"><a href="#cb22-26" tabindex="-1"></a>        <span class="st">'train'</span>: <span class="st">'titanic_train.csv'</span>,</span>
<span id="cb22-27"><a href="#cb22-27" tabindex="-1"></a>        <span class="st">'max_depth'</span>: <span class="dv">5</span>,</span>
<span id="cb22-28"><a href="#cb22-28" tabindex="-1"></a>        <span class="st">'eta'</span>: <span class="fl">0.1</span>,</span>
<span id="cb22-29"><a href="#cb22-29" tabindex="-1"></a>        <span class="st">'subsample'</span>: <span class="fl">0.8</span>,</span>
<span id="cb22-30"><a href="#cb22-30" tabindex="-1"></a>        <span class="st">'colsample_bytree'</span>: <span class="fl">0.8</span>,</span>
<span id="cb22-31"><a href="#cb22-31" tabindex="-1"></a>        <span class="st">'num_round'</span>: <span class="dv">100</span></span>
<span id="cb22-32"><a href="#cb22-32" tabindex="-1"></a>    }</span>
<span id="cb22-33"><a href="#cb22-33" tabindex="-1"></a>)</span>
<span id="cb22-34"><a href="#cb22-34" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" tabindex="-1"></a><span class="co"># Define input data</span></span>
<span id="cb22-36"><a href="#cb22-36" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(train_s3_path, content_type<span class="op">=</span><span class="st">'csv'</span>)</span>
<span id="cb22-37"><a href="#cb22-37" tabindex="-1"></a></span>
<span id="cb22-38"><a href="#cb22-38" tabindex="-1"></a><span class="co"># Measure and start training time</span></span>
<span id="cb22-39"><a href="#cb22-39" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb22-40"><a href="#cb22-40" tabindex="-1"></a>xgboost_estimator.fit({<span class="st">'train'</span>: train_input})</span>
<span id="cb22-41"><a href="#cb22-41" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb22-42"><a href="#cb22-42" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb23">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a><span class="ex">INFO:sagemaker:Creating</span> training-job with name: sagemaker-xgboost-2024-11-03-21-10-03-577</span></code></pre>
</div>
<div class="section level4">
<h4 id="hyperparameters">Hyperparameters<a class="anchor" aria-label="anchor" href="#hyperparameters"></a>
</h4>
<blockquote>
<p>The <code>hyperparameters</code> section in this code defines key
parameters for the XGBoost model, such as <code>max_depth</code>,
<code>eta</code>, <code>subsample</code>, <code>colsample_bytree</code>,
and <code>num_round</code>, which control aspects of the model like tree
depth, learning rate, and data sampling, directly impacting model
performance and training time.</p>
<p>Additionally, we define a <code>train_file</code> hyperparameter to
pass the dataset’s S3 path to <code>train_xgboost.py</code>, allowing
the script to access this path directly. When running the training job,
SageMaker passes these values to <code>train_xgboost.py</code> as
command-line arguments, making them accessible in the script via
<code>argparse</code> or similar methods. This setup enables flexible
tuning of model parameters and data paths directly from the training
configuration, without needing modifications in the script itself.</p>
</blockquote>
</div>
<div class="section level4">
<h4 id="why-do-we-need-a-train-hyperparameter-in-addition-to-traininginput">Why do we need a train hyperparameter in addition to
TrainingInput?<a class="anchor" aria-label="anchor" href="#why-do-we-need-a-train-hyperparameter-in-addition-to-traininginput"></a>
</h4>
<blockquote>
<p>The <code>TrainingInput</code> in SageMaker isn’t just about
providing the data path for your script. It actually sets up a
<strong>data channel</strong> that allows SageMaker to manage, validate,
and automatically transfer your data from S3 to the training instance.
Here’s how it works: 1. <strong>Data Download</strong>: SageMaker uses
<code>TrainingInput</code> to download your dataset from S3 to a
temporary location on the training instance. This location is mounted
and managed by SageMaker and can be accessed by the training job if
needed. 2. <strong>Environment Setup</strong>: Using
<code>TrainingInput</code> also configures the job environment. For
example, the path specified in <code>TrainingInput</code> (e.g., under
<code>'train'</code>) becomes an environment variable
(<code>SM_CHANNEL_TRAIN</code>), which points to the downloaded data
location on the training instance. 3. <strong>Data Management</strong>:
SageMaker can manage and track data inputs independently of your script,
which is especially useful for distributed training or when using
managed algorithms. ##### Why Use Both? If your script is designed to
handle the data directly (e.g., by downloading it from an S3 path), the
<strong>data path you pass as a hyperparameter</strong> can handle this.
However, SageMaker still needs <code>TrainingInput</code> to manage and
configure the environment and data resources properly. -
<strong><code>TrainingInput</code></strong>: Required by SageMaker for
managing the data channel, downloading data to the instance, and setting
up the training environment. - <strong>Hyperparameter with S3
Path</strong>: Necessary for your custom script to handle the dataset
directly.</p>
</blockquote>
</div>
<div class="section level4">
<h4 id="model-results">Model results<a class="anchor" aria-label="anchor" href="#model-results"></a>
</h4>
<blockquote>
<p>With this code, the training results and model artifacts are saved in
a subfolder called <code>xgboost</code> in your specified S3 bucket.
This folder (<code>s3://{bucket}/xgboost/</code>) will be automatically
created if it doesn’t already exist, and will contain:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Model Artifacts</strong>: The trained model file (often a
<code>.tar.gz</code> file) that SageMaker saves in the
<code>output_path</code>.</li>
<li>
<strong>Logs and Metrics</strong>: Any metrics and logs related to
the training job, stored in the same <code>xgboost</code> folder.</li>
</ol>
<p>This setup allows for convenient access to both the trained model and
related output for later evaluation or deployment.</p>
</blockquote>
</div>
</div>
<div class="section level3">
<h3 id="extracting-trained-model-from-s3-for-final-evaluation">Extracting trained model from S3 for final evaluation<a class="anchor" aria-label="anchor" href="#extracting-trained-model-from-s3-for-final-evaluation"></a>
</h3>
<p>To evaluate the model on a test set after training, we’ll go through
these steps:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Download the trained model from S3</strong>.</li>
<li>
<strong>Load and preprocess</strong> the test dataset.</li>
<li>
<strong>Evaluate</strong> the model on the test data.</li>
</ol>
<p>Here’s how you can implement this in your SageMaker notebook. The
following code will:</p>
<ul>
<li>Download the <code>model.tar.gz</code> file containing the trained
model from S3.</li>
<li>Load the <code>test.csv</code> data from S3 and preprocess it as
needed.</li>
<li>Use the XGBoost model to make predictions on the test set and then
compute accuracy or other metrics on the results.</li>
</ul>
<p>If additional metrics or custom evaluation steps are needed, you can
add them in place of or alongside accuracy.</p>
<div class="codewrapper sourceCode" id="cb24">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="co"># Model results are saved in auto-generated folders. Use xgboost_estimator.latest_training_job.name to get the folder name</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a>model_s3_path <span class="op">=</span> <span class="ss">f'</span><span class="sc">{</span>output_folder<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>xgboost_estimator<span class="sc">.</span>latest_training_job<span class="sc">.</span>name<span class="sc">}</span><span class="ss">/output/model.tar.gz'</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="bu">print</span>(model_s3_path)</span>
<span id="cb24-4"><a href="#cb24-4" tabindex="-1"></a>local_model_path <span class="op">=</span> <span class="st">'model.tar.gz'</span></span>
<span id="cb24-5"><a href="#cb24-5" tabindex="-1"></a></span>
<span id="cb24-6"><a href="#cb24-6" tabindex="-1"></a><span class="co"># Download the trained model from S3</span></span>
<span id="cb24-7"><a href="#cb24-7" tabindex="-1"></a>s3.download_file(bucket, model_s3_path, local_model_path)</span>
<span id="cb24-8"><a href="#cb24-8" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" tabindex="-1"></a><span class="co"># Extract the model file</span></span>
<span id="cb24-10"><a href="#cb24-10" tabindex="-1"></a><span class="im">import</span> tarfile</span>
<span id="cb24-11"><a href="#cb24-11" tabindex="-1"></a><span class="cf">with</span> tarfile.<span class="bu">open</span>(local_model_path) <span class="im">as</span> tar:</span>
<span id="cb24-12"><a href="#cb24-12" tabindex="-1"></a>    tar.extractall()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb25">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a><span class="ex">xgboost/sagemaker-xgboost-2024-11-03-21-10-03-577/output/model.tar.gz</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb26">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a><span class="co"># Load the test set. We downloaded this earlier from our S3 bucket.</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>test_data <span class="op">=</span> pd.read_csv(test_filename)</span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>test_data.head()</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb27">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" tabindex="-1"></a><span class="co"># Preprocess the test set to match the training setup</span></span>
<span id="cb27-2"><a href="#cb27-2" tabindex="-1"></a><span class="im">from</span> test_AWS.scripts.train_xgboost <span class="im">import</span> preprocess_data</span>
<span id="cb27-3"><a href="#cb27-3" tabindex="-1"></a>X_test, y_test <span class="op">=</span> preprocess_data(test_data)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb28">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a><span class="im">import</span> joblib</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb28-6"><a href="#cb28-6" tabindex="-1"></a></span>
<span id="cb28-7"><a href="#cb28-7" tabindex="-1"></a><span class="co"># Load the trained model using joblib</span></span>
<span id="cb28-8"><a href="#cb28-8" tabindex="-1"></a>model <span class="op">=</span> joblib.load(<span class="st">"xgboost-model"</span>)</span>
<span id="cb28-9"><a href="#cb28-9" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" tabindex="-1"></a><span class="co"># Assume X_test and y_test are defined</span></span>
<span id="cb28-11"><a href="#cb28-11" tabindex="-1"></a><span class="co"># Create DMatrix for X_test for XGBoost prediction compatibility</span></span>
<span id="cb28-12"><a href="#cb28-12" tabindex="-1"></a>dmatrix_test <span class="op">=</span> xgb.DMatrix(X_test)</span>
<span id="cb28-13"><a href="#cb28-13" tabindex="-1"></a></span>
<span id="cb28-14"><a href="#cb28-14" tabindex="-1"></a><span class="co"># Make predictions on the test set</span></span>
<span id="cb28-15"><a href="#cb28-15" tabindex="-1"></a>preds <span class="op">=</span> model.predict(dmatrix_test)</span>
<span id="cb28-16"><a href="#cb28-16" tabindex="-1"></a>predictions <span class="op">=</span> np.<span class="bu">round</span>(preds)  <span class="co"># Round to 0 or 1 for classification</span></span>
<span id="cb28-17"><a href="#cb28-17" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" tabindex="-1"></a><span class="co"># Calculate accuracy or any other relevant metrics</span></span>
<span id="cb28-19"><a href="#cb28-19" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_test, predictions)</span>
<span id="cb28-20"><a href="#cb28-20" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Test Set Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb29">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="ex">Test</span> Set Accuracy: 0.7933</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" tabindex="-1"></a><span class="ex">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158:</span> UserWarning: <span class="pp">[</span><span class="ss">21:13:21</span><span class="pp">]</span> WARNING: /workspace/src/collective/../data/../common/error_msg.h:80: If you are loading a serialized model <span class="er">(</span><span class="ex">like</span> pickle in Python, RDS in R<span class="kw">)</span> <span class="ex">or</span></span>
<span id="cb29-5"><a href="#cb29-5" tabindex="-1"></a><span class="ex">configuration</span> generated by an older version of XGBoost, please export the model by calling</span>
<span id="cb29-6"><a href="#cb29-6" tabindex="-1"></a><span class="kw">`</span><span class="ex">Booster.save_model</span><span class="kw">`</span> from that version first, then load it back in current version. See:</span>
<span id="cb29-7"><a href="#cb29-7" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" tabindex="-1"></a>    <span class="ex">https://xgboost.readthedocs.io/en/stable/tutorials/saving_model.html</span></span>
<span id="cb29-9"><a href="#cb29-9" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" tabindex="-1"></a><span class="cf">for</span> more <span class="ex">details</span> about differences between saving model and serializing.</span>
<span id="cb29-11"><a href="#cb29-11" tabindex="-1"></a></span>
<span id="cb29-12"><a href="#cb29-12" tabindex="-1"></a>  <span class="ex">warnings.warn</span><span class="er">(</span><span class="ex">smsg,</span> UserWarning<span class="kw">)</span></span>
<span id="cb29-13"><a href="#cb29-13" tabindex="-1"></a><span class="ex">/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/xgboost/core.py:158:</span> UserWarning: <span class="pp">[</span><span class="ss">21:13:21</span><span class="pp">]</span> WARNING: /workspace/src/learner.cc:872: Found JSON model saved before XGBoost 1.6, please save the model using current version again. The support for old JSON model will be discontinued in XGBoost 2.3.</span>
<span id="cb29-14"><a href="#cb29-14" tabindex="-1"></a>  <span class="ex">warnings.warn</span><span class="er">(</span><span class="ex">smsg,</span> UserWarning<span class="kw">)</span></span></code></pre>
</div>
<p>Now that we’ve covered training using a custom script with the
<code>XGBoost</code> estimator, let’s examine the built-in image-based
approach. Using SageMaker’s pre-configured XGBoost image streamlines the
setup by eliminating the need to manage custom scripts for common
workflows, and it can also provide optimization advantages. Below, we’ll
discuss both the code and pros and cons of the image-based setup
compared to the custom script approach.</p>
</div>
<div class="section level3">
<h3 id="training-with-sagemakers-built-in-xgboost-image">Training with SageMaker’s Built-in XGBoost Image<a class="anchor" aria-label="anchor" href="#training-with-sagemakers-built-in-xgboost-image"></a>
</h3>
<p>With the SageMaker-provided XGBoost container, you can bypass custom
script configuration if your workflow aligns with standard XGBoost
training. This setup is particularly useful when you need quick, default
configurations without custom preprocessing or additional libraries.</p>
</div>
<div class="section level3">
<h3 id="comparison-custom-script-vs--built-in-image">Comparison: Custom Script vs. Built-in Image<a class="anchor" aria-label="anchor" href="#comparison-custom-script-vs--built-in-image"></a>
</h3>
<table class="table">
<colgroup>
<col width="20%">
<col width="41%">
<col width="38%">
</colgroup>
<thead><tr class="header">
<th>Feature</th>
<th>Custom Script (<code>XGBoost</code> with
<code>entry_point</code>)</th>
<th>Built-in XGBoost Image</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><strong>Flexibility</strong></td>
<td>Allows for custom preprocessing, data transformation, or advanced
parameterization. Requires a Python script and custom dependencies can
be added through <code>requirements.txt</code>.</td>
<td>Limited to XGBoost’s built-in functionality, no custom preprocessing
steps without additional customization.</td>
</tr>
<tr class="even">
<td><strong>Simplicity</strong></td>
<td>Requires setting up a script with <code>entry_point</code> and
managing dependencies. Ideal for specific needs but requires
configuration.</td>
<td>Streamlined for fast deployment without custom code. Simple setup
and no need for custom scripts.</td>
</tr>
<tr class="odd">
<td><strong>Performance</strong></td>
<td>Similar performance, though potential for overhead with additional
preprocessing.</td>
<td>Optimized for typical XGBoost tasks with faster startup. May offer
marginally faster time-to-first-train.</td>
</tr>
<tr class="even">
<td><strong>Use Cases</strong></td>
<td>Ideal for complex workflows requiring unique preprocessing steps or
when adding specific libraries or functionalities.</td>
<td>Best for quick experiments, standard workflows, or initial testing
on datasets without complex preprocessing.</td>
</tr>
</tbody>
</table>
<p><strong>When to Use Each Approach</strong>: - <strong>Custom
Script</strong>: Recommended if you need to implement custom data
preprocessing, advanced feature engineering, or specific workflow steps
that require more control over training. - <strong>Built-in
Image</strong>: Ideal when running standard XGBoost training, especially
for quick experiments or production deployments where default
configurations suffice.</p>
<p>Both methods offer powerful and flexible approaches to model training
on SageMaker, allowing you to select the approach best suited to your
needs. Below is an example of training using the built-in XGBoost
Image.</p>
<div class="section level4">
<h4 id="setting-up-the-data-path">Setting up the data path<a class="anchor" aria-label="anchor" href="#setting-up-the-data-path"></a>
</h4>
<p>In this approach, using <code>TrainingInput</code> directly with
SageMaker’s built-in XGBoost container contrasts with our previous
method, where we specified a custom script with argument inputs
(specified in hyperparameters) for data paths and settings. With
<code>TrainingInput</code>, data paths and formats are managed as
structured inputs (<code>{'train': train_input}</code>) rather than
passed as arguments in a script. This setup simplifies and standardizes
data handling in SageMaker’s built-in algorithms, keeping the data
configuration separate from hyperparameters.</p>
<div class="codewrapper sourceCode" id="cb30">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a>train_s3_path</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb31">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a><span class="st">'s3://titanic-dataset-test/data/titanic_train.csv'</span></span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb32">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="im">from</span> sagemaker.estimator <span class="im">import</span> Estimator <span class="co"># when using images, we use the general Estimator class</span></span>
<span id="cb32-2"><a href="#cb32-2" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" tabindex="-1"></a><span class="co"># Define instance type/count we'll use for training</span></span>
<span id="cb32-4"><a href="#cb32-4" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb32-5"><a href="#cb32-5" tabindex="-1"></a>instance_count<span class="op">=</span><span class="dv">1</span> <span class="co"># always start with 1. Rarely is parallelized training justified with data &lt; 50 GB.</span></span>
<span id="cb32-6"><a href="#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" tabindex="-1"></a><span class="co"># Use Estimator directly for built-in container without specifying entry_point</span></span>
<span id="cb32-8"><a href="#cb32-8" tabindex="-1"></a>xgboost_estimator_builtin <span class="op">=</span> Estimator(</span>
<span id="cb32-9"><a href="#cb32-9" tabindex="-1"></a>    image_uri<span class="op">=</span>sagemaker.image_uris.retrieve(<span class="st">"xgboost"</span>, session.boto_region_name, version<span class="op">=</span><span class="st">"1.5-1"</span>),</span>
<span id="cb32-10"><a href="#cb32-10" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb32-11"><a href="#cb32-11" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,</span>
<span id="cb32-12"><a href="#cb32-12" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb32-13"><a href="#cb32-13" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb32-14"><a href="#cb32-14" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb32-15"><a href="#cb32-15" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb32-16"><a href="#cb32-16" tabindex="-1"></a>        <span class="st">'max_depth'</span>: <span class="dv">5</span>,</span>
<span id="cb32-17"><a href="#cb32-17" tabindex="-1"></a>        <span class="st">'eta'</span>: <span class="fl">0.1</span>,</span>
<span id="cb32-18"><a href="#cb32-18" tabindex="-1"></a>        <span class="st">'subsample'</span>: <span class="fl">0.8</span>,</span>
<span id="cb32-19"><a href="#cb32-19" tabindex="-1"></a>        <span class="st">'colsample_bytree'</span>: <span class="fl">0.8</span>,</span>
<span id="cb32-20"><a href="#cb32-20" tabindex="-1"></a>        <span class="st">'num_round'</span>: <span class="dv">100</span></span>
<span id="cb32-21"><a href="#cb32-21" tabindex="-1"></a>    }</span>
<span id="cb32-22"><a href="#cb32-22" tabindex="-1"></a>)</span>
<span id="cb32-23"><a href="#cb32-23" tabindex="-1"></a></span>
<span id="cb32-24"><a href="#cb32-24" tabindex="-1"></a><span class="co"># Define input data</span></span>
<span id="cb32-25"><a href="#cb32-25" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(train_s3_path, content_type<span class="op">=</span><span class="st">"csv"</span>)</span>
<span id="cb32-26"><a href="#cb32-26" tabindex="-1"></a></span>
<span id="cb32-27"><a href="#cb32-27" tabindex="-1"></a><span class="co"># Measure and start training time</span></span>
<span id="cb32-28"><a href="#cb32-28" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb32-29"><a href="#cb32-29" tabindex="-1"></a>xgboost_estimator_builtin.fit({<span class="st">'train'</span>: train_input})</span>
<span id="cb32-30"><a href="#cb32-30" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb32-31"><a href="#cb32-31" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb33">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:16:19 Uploading <span class="at">-</span> Uploading generated training model</span>
<span id="cb33-2"><a href="#cb33-2" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:16:19 Completed <span class="at">-</span> Training job completed</span>
<span id="cb33-3"><a href="#cb33-3" tabindex="-1"></a><span class="ex">Training</span> seconds: 135</span>
<span id="cb33-4"><a href="#cb33-4" tabindex="-1"></a><span class="ex">Billable</span> seconds: 135</span>
<span id="cb33-5"><a href="#cb33-5" tabindex="-1"></a><span class="ex">Runtime</span> for training on SageMaker: 197.50 seconds, instance_type: ml.m5.large, instance_count: 1</span></code></pre>
</div>
</div>
</div>
</section><section><h2 class="section-heading" id="monitoring-training">Monitoring Training<a class="anchor" aria-label="anchor" href="#monitoring-training"></a>
</h2>
<hr class="half-width">
<p>To view and monitor your SageMaker training job, follow these steps
in the AWS Management Console. Since training jobs may be visible to
multiple users in your account, it’s essential to confirm that you’re
interacting with your own job before making any changes.</p>
<ol style="list-style-type: decimal">
<li>
<strong>Navigate to the SageMaker Console</strong>
<ul>
<li>Go to the AWS Management Console and open the
<strong>SageMaker</strong> service (can search for it)</li>
</ul>
</li>
<li>
<strong>View Training Jobs</strong>
<ul>
<li>In the left-hand navigation menu, select <strong>Training
jobs</strong>. You’ll see a list of recent training jobs, which may
include jobs from other users in the account.</li>
</ul>
</li>
<li>
<strong>Verify Your Training Job</strong>
<ul>
<li>Identify your job by looking for the specific name format (e.g.,
<code>sagemaker-xgboost-YYYY-MM-DD-HH-MM-SS-XXX</code>) generated when
you launched the job. Click on its name to access detailed information.
Cross-check the job details, such as the <strong>Instance Type</strong>
and <strong>Input data configuration</strong>, with the parameters you
set in your script.</li>
</ul>
</li>
<li>
<strong>Monitor the Job Status</strong>
<ul>
<li>Once you’ve verified the correct job, click on its name to access
detailed information:
<ul>
<li>
<strong>Status</strong>: Confirms whether the job is
<code>InProgress</code>, <code>Completed</code>, or
<code>Failed</code>.</li>
<li>
<strong>Logs</strong>: Review CloudWatch Logs and Job Metrics for
real-time updates.</li>
<li>
<strong>Output Data</strong>: Shows the S3 location with the trained
model artifacts.</li>
</ul>
</li>
</ul>
</li>
<li>
<strong>Use CloudWatch for In-Depth Monitoring</strong>
<ul>
<li>If additional monitoring is needed, go to <strong>CloudWatch
Logs</strong> to view output logs associated with your training job in
real-time.</li>
</ul>
</li>
<li>
<strong>Stopping a Training Job</strong>
<ul>
<li>Before stopping a job, ensure you’ve selected the correct one by
verifying job details as outlined above.</li>
<li>If you’re certain it’s your job, go to <strong>Training
jobs</strong> in the SageMaker Console, select the job, and choose
<strong>Stop</strong> from the <strong>Actions</strong> menu. Confirm
your selection, as this action will halt the job and release any
associated resources.</li>
<li>
<strong>Important</strong>: Avoid stopping jobs you don’t own, as
this could disrupt other users’ work and may have unintended
consequences.</li>
</ul>
</li>
</ol>
<p>Following these steps helps ensure you only interact with and modify
jobs you own, reducing the risk of impacting other users’ training
processes.</p>
</section><section><h2 class="section-heading" id="when-training-takes-too-long">When Training Takes Too Long<a class="anchor" aria-label="anchor" href="#when-training-takes-too-long"></a>
</h2>
<hr class="half-width">
<p>When training time becomes excessive, two main options can improve
efficiency in SageMaker: * <strong>Option 1: Upgrading to a more
powerful instance</strong> * <strong>Option 2: Using multiple instances
for distributed training</strong>.</p>
<p>Generally, <strong>Option 1 is the preferred approach</strong> and
should be explored first.</p>
<div class="section level3">
<h3 id="option-1-upgrade-to-a-more-powerful-instance-preferred-starting-point">Option 1: Upgrade to a More Powerful Instance (Preferred Starting
Point)<a class="anchor" aria-label="anchor" href="#option-1-upgrade-to-a-more-powerful-instance-preferred-starting-point"></a>
</h3>
<p>Upgrading to a more capable instance, particularly one with GPU
capabilities (e.g., for deep learning), is often the simplest and most
cost-effective way to speed up training. Here’s a breakdown of instances
to consider. Check the <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">Instances
for ML spreadsheet</a> for guidance on selecting a better instance.</p>
<p><strong>When to Use a Single Instance Upgrade</strong><br>
Upgrading a single instance works well if: - <strong>Dataset
Size</strong>: The dataset is small to moderate (e.g., &lt;10 GB),
fitting comfortably within the memory of a larger instance. -
<strong>Model Complexity</strong>: The model is not so large that it
requires distribution across multiple devices. - <strong>Training
Time</strong>: Expected training time is within a few hours, but could
benefit from additional power.</p>
<p>Upgrading a single instance is typically the most efficient option in
terms of both cost and setup complexity. It avoids the communication
overhead associated with multi-instance setups (discussed below) and is
well-suited for most small to medium-sized datasets.</p>
</div>
<div class="section level3">
<h3 id="option-2-use-multiple-instances-for-distributed-training">Option 2: Use Multiple Instances for Distributed Training<a class="anchor" aria-label="anchor" href="#option-2-use-multiple-instances-for-distributed-training"></a>
</h3>
<p>If upgrading a single instance doesn’t sufficiently reduce training
time, distributed training across multiple instances may be a viable
alternative, particularly for larger datasets and complex models.
SageMaker supports two primary distributed training techniques:
<strong>data parallelism</strong> and <strong>model
parallelism</strong>.</p>
<div class="section level4">
<h4 id="understanding-data-parallelism-vs--model-parallelism">Understanding Data Parallelism vs. Model Parallelism<a class="anchor" aria-label="anchor" href="#understanding-data-parallelism-vs--model-parallelism"></a>
</h4>
<ul>
<li><p><strong>Data Parallelism</strong>: This approach splits the
dataset across multiple instances, allowing each instance to process a
subset of the data independently. After each batch, gradients are
synchronized across instances to ensure consistent updates to the model.
Data parallelism is effective when the model itself fits within an
instance’s memory, but the data size or desired training speed requires
faster processing through multiple instances.</p></li>
<li><p><strong>Model Parallelism</strong>: Model parallelism divides the
model itself across multiple instances, making it ideal for very large
models (e.g., deep learning models in NLP or image processing) that
cannot fit in memory on a single instance. Each instance processes a
segment of the model, and results are combined during training. This
approach is suitable for memory-intensive models that exceed the
capacity of a single instance.</p></li>
</ul>
</div>
<div class="section level4">
<h4 id="how-sagemaker-chooses-between-data-and-model-parallelism">How SageMaker Chooses Between Data and Model Parallelism<a class="anchor" aria-label="anchor" href="#how-sagemaker-chooses-between-data-and-model-parallelism"></a>
</h4>
<p>In SageMaker, the choice between data and model parallelism is not
entirely automatic. Here’s how it typically works:</p>
<ul>
<li><p><strong>Data Parallelism (Automatic)</strong>: When you set
<code>instance_count &gt; 1</code>, SageMaker will automatically apply
data parallelism. This splits the dataset across instances, allowing
each instance to process a subset independently and synchronize
gradients after each batch. Data parallelism works well when the model
can fit in the memory of a single instance, but the data size or
processing speed needs enhancement with multiple instances.</p></li>
<li><p><strong>Model Parallelism (Manual Setup)</strong>: To enable
model parallelism, you need to configure it explicitly using the
<strong>SageMaker Model Parallel Library</strong>, suitable for deep
learning models in frameworks like PyTorch or TensorFlow. Model
parallelism splits the model itself across multiple instances, which is
useful for memory-intensive models that exceed the capacity of a single
instance. Configuring model parallelism requires setting up a
distribution strategy in SageMaker’s Python SDK.</p></li>
<li><p><strong>Hybrid Parallelism (Manual Setup)</strong>: For extremely
large datasets and models, SageMaker can support both data and model
parallelism together, but this setup requires manual configuration.
Hybrid parallelism is beneficial for workloads that are both data- and
memory-intensive, where both the model and the data need distributed
processing.</p></li>
</ul>
<p><strong>When to Use Distributed Training with Multiple
Instances</strong><br>
Consider multiple instances if: - <strong>Dataset Size</strong>: The
dataset is large (&gt;10 GB) and doesn’t fit comfortably within a single
instance’s memory. - <strong>Model Complexity</strong>: The model is
complex, requiring extensive computation that a single instance cannot
handle in a reasonable time. - <strong>Expected Training Time</strong>:
Training on a single instance takes prohibitively long (e.g., &gt;10
hours), and distributed computing overhead is manageable.</p>
</div>
</div>
<div class="section level3">
<h3 id="cost-of-distributed-computing">Cost of distributed computing<a class="anchor" aria-label="anchor" href="#cost-of-distributed-computing"></a>
</h3>
<p><strong>tl;dr</strong> Use 1 instance unless you are finding that
you’re waiting hours for the training/tuning to complete.</p>
<p>Let’s break down some key points for deciding between <strong>1
instance vs. multiple instances</strong> from a cost perspective:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Instance Cost per Hour</strong>:
<ul>
<li>SageMaker charges per instance-hour. Running <strong>multiple
instances</strong> in parallel can finish training faster, reducing
wall-clock time, but the <strong>cost per hour will increase</strong>
with each added instance.</li>
</ul>
</li>
<li>
<strong>Single Instance vs. Multiple Instance Wall-Clock
Time</strong>:
<ul>
<li>When using a single instance, training will take significantly
longer, especially if your data is large. However, the wall-clock time
difference between 1 instance and 10 instances may not translate to a
direct 10x speedup when using multiple instances due to
<strong>communication overheads</strong>.</li>
<li>For example, with data-parallel training, instances need to
synchronize gradients between batches, which introduces
<strong>communication costs</strong> and may slow down training on
larger clusters.</li>
</ul>
</li>
<li>
<strong>Scaling Efficiency</strong>:
<ul>
<li>Parallelizing training does not scale perfectly due to those
overheads. Adding instances generally provides <strong>diminishing
returns</strong> on training time reduction.</li>
<li>For example, doubling instances from 1 to 2 may reduce training time
by close to 50%, but going from 8 to 16 instances may only reduce
training time by around 20-30%, depending on the model and batch
sizes.</li>
</ul>
</li>
<li>
<strong>Typical Recommendation</strong>:
<ul>
<li>For <strong>small-to-moderate datasets</strong> or cases where
training time isn’t a critical factor, a <strong>single
instance</strong> may be more cost-effective, as it avoids parallel
processing overheads.</li>
<li>For <strong>large datasets</strong> or where training speed is a
high priority (e.g., tuning complex deep learning models), using
<strong>multiple instances</strong> can be beneficial despite the cost
increase due to time savings.</li>
</ul>
</li>
<li>
<strong>Practical Cost Estimation</strong>:
<ul>
<li>Suppose a single instance takes <code>T</code> hours to train and
costs <code>$C</code> per hour. For a 10-instance setup, the cost would
be approximately:
<ul>
<li>
<strong>Single instance:</strong> <code>T * $C</code>
</li>
<li>
<strong>10 instances (parallel):</strong>
<code>(T / k) * (10 * $C)</code>, where <code>k</code> is the speedup
factor (&lt;10 due to overhead).</li>
</ul>
</li>
<li>If the speedup is only about 5x instead of 10x due to communication
overhead, then the cost difference may be minimal, with a slight edge to
a single instance on total cost but at a higher wall-clock time.</li>
</ul>
</li>
</ol>
<blockquote>
<p>In summary: - <strong>Start by upgrading to a more powerful instance
(Option 1)</strong> for datasets up to 10 GB and moderately complex
models. A single, more powerful, instance is usually more cost-effective
for smaller workloads and where time isn’t critical. Running initial
tests with a single instance can also provide a benchmark. You can then
experiment with small increases in instance count to find a balance
between cost and time savings, particularly considering communication
overheads that affect parallel efficiency. - <strong>Consider
distributed training across multiple instances (Option 2)</strong> only
when dataset size, model complexity, or training time demand it.</p>
</blockquote>
</div>
</section><section><h2 class="section-heading" id="xgboosts-distributed-training-mechanism">XGBoost’s Distributed Training Mechanism<a class="anchor" aria-label="anchor" href="#xgboosts-distributed-training-mechanism"></a>
</h2>
<hr class="half-width">
<p>In the event that option 2 explained above really is better for your
use-case (e.g., you have a very large dataset or model that takes a
while to train even with high performance instances), the next example
will demo setting this up. Before we do, though, we should ask what
distributed computing really means for our specific model/setup.
XGBoost’s distributed training relies on a data-parallel approach that
divides the dataset across multiple instances (or workers), enabling
each instance to work on a portion of the data independently. This
strategy enhances efficiency, especially for large datasets and
computationally intensive tasks.</p>
<blockquote>
<p><strong>What about a model parallelism approach?</strong> Unlike deep
learning models with vast neural network layers, XGBoost’s decision
trees are usually small enough to fit in memory on a single instance,
even when the dataset is large. Thus, model parallelism is rarely
necessary. XGBoost does not inherently support model parallelism out of
the box in SageMaker because the model architecture doesn’t typically
exceed memory limits, unlike massive language or image models. Although
model parallelism can be theoretically applied (e.g., splitting large
tree structures across instances), it’s generally not supported natively
in SageMaker for XGBoost, as it would require a custom distribution
framework to split the model itself.</p>
</blockquote>
<p>Here’s how distributed training in XGBoost works, particularly in the
SageMaker environment:</p>
<div class="section level3">
<h3 id="key-steps-in-distributed-training-with-xgboost">Key Steps in Distributed Training with XGBoost<a class="anchor" aria-label="anchor" href="#key-steps-in-distributed-training-with-xgboost"></a>
</h3>
<div class="section level4">
<h4 id="data-partitioning">1. <strong>Data Partitioning</strong>
<a class="anchor" aria-label="anchor" href="#data-partitioning"></a>
</h4>
<ul>
<li>The dataset is divided among multiple instances. For example, with
two instances, each instance may receive half of the dataset.</li>
<li>In SageMaker, data partitioning across instances is handled
automatically via the input channels you specify during training,
reducing manual setup.</li>
</ul>
</div>
<div class="section level4">
<h4 id="parallel-gradient-boosting">2. <strong>Parallel Gradient Boosting</strong>
<a class="anchor" aria-label="anchor" href="#parallel-gradient-boosting"></a>
</h4>
<ul>
<li>XGBoost performs gradient boosting by constructing trees iteratively
based on calculated gradients.</li>
<li>Each instance calculates gradients (first-order derivatives) and
Hessians (second-order derivatives of the loss function) independently
on its subset of data.</li>
<li>This parallel processing allows each instance to determine which
features to split and which trees to add to the model based on its data
portion.</li>
</ul>
</div>
<div class="section level4">
<h4 id="communication-between-instances">3. <strong>Communication Between Instances</strong>
<a class="anchor" aria-label="anchor" href="#communication-between-instances"></a>
</h4>
<ul>
<li>After computing gradients and Hessians locally, instances
synchronize to share and combine these values.</li>
<li>Synchronization keeps the model parameters consistent across
instances. Only computed gradients are communicated, not the raw
dataset, minimizing data transfer overhead.</li>
<li>The combined gradients guide global model updates, ensuring that the
ensemble of trees reflects the entire dataset, despite its division
across multiple instances.</li>
</ul>
</div>
<div class="section level4">
<h4 id="final-model-aggregation">4. <strong>Final Model Aggregation</strong>
<a class="anchor" aria-label="anchor" href="#final-model-aggregation"></a>
</h4>
<ul>
<li>Once training completes, XGBoost aggregates the trained trees from
each instance into a single final model.</li>
<li>This aggregation enables the final model to perform as though it
trained on the entire dataset, even if the dataset couldn’t fit into a
single instance’s memory.</li>
</ul>
<p>SageMaker simplifies these steps by automatically managing the
partitioning, synchronization, and aggregation processes during
distributed training with XGBoost.</p>
</div>
</div>
</section><section><h2 class="section-heading" id="implementing-distributed-training-with-xgboost-in-sagemaker">Implementing Distributed Training with XGBoost in SageMaker<a class="anchor" aria-label="anchor" href="#implementing-distributed-training-with-xgboost-in-sagemaker"></a>
</h2>
<hr class="half-width">
<p>In SageMaker, setting up distributed training for XGBoost can offer
significant time savings as dataset sizes and computational requirements
increase. Here’s how you can configure it:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Select Multiple Instances</strong>: Specify
<code>instance_count &gt; 1</code> in the SageMaker
<code>Estimator</code> to enable distributed training.</li>
<li>
<strong>Optimize Instance Type</strong>: Choose an instance type
suitable for your dataset size and XGBoost requirements</li>
<li>
<strong>Monitor for Speed Improvements</strong>: With larger
datasets, distributed training can yield time savings by scaling
horizontally. However, gains may vary depending on the dataset and
computation per instance.</li>
</ol>
<div class="codewrapper sourceCode" id="cb34">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a><span class="co"># Define instance type/count we'll use for training</span></span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb34-3"><a href="#cb34-3" tabindex="-1"></a>instance_count<span class="op">=</span><span class="dv">1</span> <span class="co"># always start with 1. Rarely is parallelized training justified with data &lt; 50 GB.</span></span>
<span id="cb34-4"><a href="#cb34-4" tabindex="-1"></a></span>
<span id="cb34-5"><a href="#cb34-5" tabindex="-1"></a><span class="co"># Define the XGBoost estimator for distributed training</span></span>
<span id="cb34-6"><a href="#cb34-6" tabindex="-1"></a>xgboost_estimator <span class="op">=</span> Estimator(</span>
<span id="cb34-7"><a href="#cb34-7" tabindex="-1"></a>    image_uri<span class="op">=</span>sagemaker.image_uris.retrieve(<span class="st">"xgboost"</span>, session.boto_region_name, version<span class="op">=</span><span class="st">"1.5-1"</span>),</span>
<span id="cb34-8"><a href="#cb34-8" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb34-9"><a href="#cb34-9" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,  <span class="co"># Start with 1 instance for baseline</span></span>
<span id="cb34-10"><a href="#cb34-10" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb34-11"><a href="#cb34-11" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb34-12"><a href="#cb34-12" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb34-13"><a href="#cb34-13" tabindex="-1"></a>)</span>
<span id="cb34-14"><a href="#cb34-14" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" tabindex="-1"></a><span class="co"># Set hyperparameters</span></span>
<span id="cb34-16"><a href="#cb34-16" tabindex="-1"></a>xgboost_estimator.set_hyperparameters(</span>
<span id="cb34-17"><a href="#cb34-17" tabindex="-1"></a>    max_depth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb34-18"><a href="#cb34-18" tabindex="-1"></a>    eta<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb34-19"><a href="#cb34-19" tabindex="-1"></a>    subsample<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb34-20"><a href="#cb34-20" tabindex="-1"></a>    colsample_bytree<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb34-21"><a href="#cb34-21" tabindex="-1"></a>    num_round<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb34-22"><a href="#cb34-22" tabindex="-1"></a>)</span>
<span id="cb34-23"><a href="#cb34-23" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" tabindex="-1"></a><span class="co"># Specify input data from S3</span></span>
<span id="cb34-25"><a href="#cb34-25" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(train_s3_path, content_type<span class="op">=</span><span class="st">"csv"</span>)</span>
<span id="cb34-26"><a href="#cb34-26" tabindex="-1"></a></span>
<span id="cb34-27"><a href="#cb34-27" tabindex="-1"></a><span class="co"># Run with 1 instance</span></span>
<span id="cb34-28"><a href="#cb34-28" tabindex="-1"></a>start1 <span class="op">=</span> t.time()</span>
<span id="cb34-29"><a href="#cb34-29" tabindex="-1"></a>xgboost_estimator.fit({<span class="st">"train"</span>: train_input})</span>
<span id="cb34-30"><a href="#cb34-30" tabindex="-1"></a>end1 <span class="op">=</span> t.time()</span>
<span id="cb34-31"><a href="#cb34-31" tabindex="-1"></a></span>
<span id="cb34-32"><a href="#cb34-32" tabindex="-1"></a></span>
<span id="cb34-33"><a href="#cb34-33" tabindex="-1"></a><span class="co"># Now run with 2 instances to observe speedup</span></span>
<span id="cb34-34"><a href="#cb34-34" tabindex="-1"></a>xgboost_estimator.instance_count <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb34-35"><a href="#cb34-35" tabindex="-1"></a>start2 <span class="op">=</span> t.time()</span>
<span id="cb34-36"><a href="#cb34-36" tabindex="-1"></a>xgboost_estimator.fit({<span class="st">"train"</span>: train_input})</span>
<span id="cb34-37"><a href="#cb34-37" tabindex="-1"></a>end2 <span class="op">=</span> t.time()</span>
<span id="cb34-38"><a href="#cb34-38" tabindex="-1"></a></span>
<span id="cb34-39"><a href="#cb34-39" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end1 <span class="op">-</span> start1<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb34-40"><a href="#cb34-40" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end2 <span class="op">-</span> start2<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>xgboost_estimator<span class="sc">.</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb35">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" tabindex="-1"></a><span class="ex">INFO:sagemaker.image_uris:Ignoring</span> unnecessary instance type: None.</span>
<span id="cb35-2"><a href="#cb35-2" tabindex="-1"></a><span class="ex">INFO:sagemaker:Creating</span> training-job with name: sagemaker-xgboost-2024-11-03-21-16-39-216</span>
<span id="cb35-3"><a href="#cb35-3" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:16:40 Starting <span class="at">-</span> Starting the training job...</span>
<span id="cb35-6"><a href="#cb35-6" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:16:55 Starting <span class="at">-</span> Preparing the instances for training...</span>
<span id="cb35-7"><a href="#cb35-7" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:17:22 Downloading <span class="at">-</span> Downloading input data...</span>
<span id="cb35-8"><a href="#cb35-8" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:18:07 Downloading <span class="at">-</span> Downloading the training image......</span>
<span id="cb35-9"><a href="#cb35-9" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:19:13 Training <span class="at">-</span> Training image download completed. Training in progress.</span>
<span id="cb35-10"><a href="#cb35-10" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:19:13 Uploading <span class="at">-</span> Uploading generated training model[34m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning:</span>
<span id="cb35-11"><a href="#cb35-11" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:19:32 Completed <span class="at">-</span> Training job completed</span>
<span id="cb35-12"><a href="#cb35-12" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" tabindex="-1"></a><span class="ex">INFO:sagemaker:Creating</span> training-job with name: sagemaker-xgboost-2024-11-03-21-19-57-254</span>
<span id="cb35-14"><a href="#cb35-14" tabindex="-1"></a><span class="ex">Training</span> seconds: 130</span>
<span id="cb35-15"><a href="#cb35-15" tabindex="-1"></a><span class="ex">Billable</span> seconds: 130</span>
<span id="cb35-16"><a href="#cb35-16" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:19:58 Starting <span class="at">-</span> Starting the training job...</span>
<span id="cb35-18"><a href="#cb35-18" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:20:13 Starting <span class="at">-</span> Preparing the instances for training...</span>
<span id="cb35-19"><a href="#cb35-19" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:20:46 Downloading <span class="at">-</span> Downloading input data......</span>
<span id="cb35-20"><a href="#cb35-20" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:21:36 Downloading <span class="at">-</span> Downloading the training image...</span>
<span id="cb35-21"><a href="#cb35-21" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:22:27 Training <span class="at">-</span> Training image download completed. Training in progress..[35m/miniconda3/lib/python3.8/site-packages/xgboost/compat.py:36: </span>
<span id="cb35-22"><a href="#cb35-22" tabindex="-1"></a></span>
<span id="cb35-23"><a href="#cb35-23" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:23:01 Uploading <span class="at">-</span> Uploading generated training model</span>
<span id="cb35-24"><a href="#cb35-24" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:23:01 Completed <span class="at">-</span> Training job completed</span>
<span id="cb35-25"><a href="#cb35-25" tabindex="-1"></a><span class="ex">Training</span> seconds: 270</span>
<span id="cb35-26"><a href="#cb35-26" tabindex="-1"></a><span class="ex">Billable</span> seconds: 270</span>
<span id="cb35-27"><a href="#cb35-27" tabindex="-1"></a><span class="ex">Runtime</span> for training on SageMaker: 198.04 seconds, instance_type: ml.m5.large, instance_count: 1</span>
<span id="cb35-28"><a href="#cb35-28" tabindex="-1"></a><span class="ex">Runtime</span> for training on SageMaker: 197.66 seconds, instance_type: ml.m5.large, instance_count: 2</span></code></pre>
</div>
<div class="section level3">
<h3 id="why-scaling-instances-might-not-show-speedup-here">Why Scaling Instances Might Not Show Speedup Here<a class="anchor" aria-label="anchor" href="#why-scaling-instances-might-not-show-speedup-here"></a>
</h3>
<ul>
<li><p>Small Dataset: With only 892 rows, the dataset might be too small
to benefit from distributed training. Distributing small datasets often
adds overhead (like network communication between instances), which
outweighs the parallel processing benefits.</p></li>
<li><p>Distributed Overhead: Distributed training introduces
coordination steps that can add latency. For very short training jobs,
this overhead can become a larger portion of the total training time,
reducing the benefit of additional instances.</p></li>
<li><p>Tree-Based Models: Tree-based models, like those in XGBoost,
don’t benefit from distributed scaling as much as deep learning models
when datasets are small. For large datasets, distributed XGBoost can
still offer speedups, but this effect is generally less than with neural
networks, where parallel gradient updates across multiple instances
become efficient.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="when-multi-instance-training-helps">When Multi-Instance Training Helps<a class="anchor" aria-label="anchor" href="#when-multi-instance-training-helps"></a>
</h3>
<ul>
<li><p>Larger Datasets: Multi-instance training shines with larger
datasets, where splitting the data across instances and processing it in
parallel can significantly reduce the training time.</p></li>
<li><p>Complex Models: For highly complex models with many parameters
(like deep learning models or large XGBoost ensembles) and long training
times, distributing the training can help speed up the process as each
instance contributes to the gradient calculation and optimization
steps.</p></li>
<li><p>Distributed Algorithms: XGBoost has a built-in distributed
training capability, but models that perform gradient descent, like deep
neural networks, gain more obvious benefits because each instance can
compute gradients for a batch of data simultaneously, allowing faster
convergence.</p></li>
</ul>
</div>
</section><section><h2 class="section-heading" id="training-a-neural-network-with-sagemaker">Training a neural network with SageMaker<a class="anchor" aria-label="anchor" href="#training-a-neural-network-with-sagemaker"></a>
</h2>
<hr class="half-width">
<p>Let’s see how to do a similar experiment, but this time using PyTorch
neural networks. We will again demonstrate how to test our custom model
train script (train_nn.py) before deploying to SageMaker, and discuss
some strategies (e.g., using a GPU) for improving train time when
needed.</p>
<div class="section level3">
<h3 id="preparing-the-data-compressed-npz-files">Preparing the data (compressed npz files)<a class="anchor" aria-label="anchor" href="#preparing-the-data-compressed-npz-files"></a>
</h3>
<p>When deploying a PyTorch model on SageMaker, it’s helpful to prepare
the input data in a format that’s directly accessible and compatible
with PyTorch’s data handling methods. The next code cell will prep our
npz files from the existing csv versions. Why are we using this
format?</p>
<ol style="list-style-type: decimal">
<li><p><strong>Optimized Data Loading</strong>:<br>
The <code>.npz</code> format stores arrays in a compressed, binary
format, making it efficient for both storage and loading. PyTorch can
easily handle <code>.npz</code> files, especially in batch processing,
without requiring complex data transformations during training.</p></li>
<li><p><strong>Batch Compatibility</strong>:<br>
When training neural networks in PyTorch, it’s common to load data in
batches. By storing data in an <code>.npz</code> file, we can quickly
load the entire dataset or specific parts (e.g., <code>X_train</code>,
<code>y_train</code>) into memory and feed it to the PyTorch
<code>DataLoader</code>, enabling efficient batched data
loading.</p></li>
<li><p><strong>Reduced I/O Overhead in SageMaker</strong>:<br>
Storing data in <code>.npz</code> files minimizes the I/O operations
during training, reducing time spent on data handling. This is
especially beneficial in cloud environments like SageMaker, where
efficient data handling directly impacts training costs and
performance.</p></li>
<li><p><strong>Consistency and Compatibility</strong>:<br>
Using <code>.npz</code> files allows us to ensure consistency between
training and validation datasets. Each file (<code>train_data.npz</code>
and <code>val_data.npz</code>) stores the arrays in a standardized way
that can be easily accessed by keys (<code>X_train</code>,
<code>y_train</code>, <code>X_val</code>, <code>y_val</code>). This
structure is compatible with PyTorch’s <code>Dataset</code> class,
making it straightforward to design custom datasets for
training.</p></li>
<li><p><strong>Support for Multiple Data Types</strong>:<br><code>.npz</code> files support storage of multiple arrays within a
single file. This is helpful for organizing features and labels without
additional code. Here, the <code>train_data.npz</code> file contains
both <code>X_train</code> and <code>y_train</code>, keeping everything
related to training data in one place. Similarly,
<code>val_data.npz</code> organizes validation features and labels,
simplifying file management.</p></li>
</ol>
<p>In summary, saving the data in <code>.npz</code> files ensures a
smooth workflow from data loading to model training in PyTorch,
leveraging SageMaker’s infrastructure for a more efficient, structured
training process.</p>
<div class="codewrapper sourceCode" id="cb36">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb36-2"><a href="#cb36-2" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb36-3"><a href="#cb36-3" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler, LabelEncoder</span>
<span id="cb36-4"><a href="#cb36-4" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb36-5"><a href="#cb36-5" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" tabindex="-1"></a><span class="co"># Load and preprocess the Titanic dataset</span></span>
<span id="cb36-7"><a href="#cb36-7" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(train_filename)</span>
<span id="cb36-8"><a href="#cb36-8" tabindex="-1"></a></span>
<span id="cb36-9"><a href="#cb36-9" tabindex="-1"></a><span class="co"># Encode categorical variables and normalize numerical ones</span></span>
<span id="cb36-10"><a href="#cb36-10" tabindex="-1"></a>df[<span class="st">'Sex'</span>] <span class="op">=</span> LabelEncoder().fit_transform(df[<span class="st">'Sex'</span>])</span>
<span id="cb36-11"><a href="#cb36-11" tabindex="-1"></a>df[<span class="st">'Embarked'</span>] <span class="op">=</span> df[<span class="st">'Embarked'</span>].fillna(<span class="st">'S'</span>)  <span class="co"># Fill missing values in 'Embarked'</span></span>
<span id="cb36-12"><a href="#cb36-12" tabindex="-1"></a>df[<span class="st">'Embarked'</span>] <span class="op">=</span> LabelEncoder().fit_transform(df[<span class="st">'Embarked'</span>])</span>
<span id="cb36-13"><a href="#cb36-13" tabindex="-1"></a></span>
<span id="cb36-14"><a href="#cb36-14" tabindex="-1"></a><span class="co"># Fill missing values for 'Age' and 'Fare' with median</span></span>
<span id="cb36-15"><a href="#cb36-15" tabindex="-1"></a>df[<span class="st">'Age'</span>] <span class="op">=</span> df[<span class="st">'Age'</span>].fillna(df[<span class="st">'Age'</span>].median())</span>
<span id="cb36-16"><a href="#cb36-16" tabindex="-1"></a>df[<span class="st">'Fare'</span>] <span class="op">=</span> df[<span class="st">'Fare'</span>].fillna(df[<span class="st">'Fare'</span>].median())</span>
<span id="cb36-17"><a href="#cb36-17" tabindex="-1"></a></span>
<span id="cb36-18"><a href="#cb36-18" tabindex="-1"></a><span class="co"># Select features and target</span></span>
<span id="cb36-19"><a href="#cb36-19" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'Pclass'</span>, <span class="st">'Sex'</span>, <span class="st">'Age'</span>, <span class="st">'SibSp'</span>, <span class="st">'Parch'</span>, <span class="st">'Fare'</span>, <span class="st">'Embarked'</span>]].values</span>
<span id="cb36-20"><a href="#cb36-20" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'Survived'</span>].values</span>
<span id="cb36-21"><a href="#cb36-21" tabindex="-1"></a></span>
<span id="cb36-22"><a href="#cb36-22" tabindex="-1"></a><span class="co"># Normalize features (helps avoid exploding/vanishing gradients)</span></span>
<span id="cb36-23"><a href="#cb36-23" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb36-24"><a href="#cb36-24" tabindex="-1"></a>X <span class="op">=</span> scaler.fit_transform(X)</span>
<span id="cb36-25"><a href="#cb36-25" tabindex="-1"></a></span>
<span id="cb36-26"><a href="#cb36-26" tabindex="-1"></a><span class="co"># Split the data</span></span>
<span id="cb36-27"><a href="#cb36-27" tabindex="-1"></a>X_train, X_val, y_train, y_val <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb36-28"><a href="#cb36-28" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" tabindex="-1"></a><span class="co"># Save the preprocessed data to our local jupyter environment</span></span>
<span id="cb36-30"><a href="#cb36-30" tabindex="-1"></a>np.savez(<span class="st">'train_data.npz'</span>, X_train<span class="op">=</span>X_train, y_train<span class="op">=</span>y_train)</span>
<span id="cb36-31"><a href="#cb36-31" tabindex="-1"></a>np.savez(<span class="st">'val_data.npz'</span>, X_val<span class="op">=</span>X_val, y_val<span class="op">=</span>y_val)</span></code></pre>
</div>
<p>Next, we will upload our compressed files to our S3 bucket. Storage
is farily cheap on AWS (around $0.023 per GB per month), but be mindful
of uploading too much data. It may be convenient to store a preprocessed
version of the data, just don’t store too many versions that aren’t
being actively used.</p>
<div class="codewrapper sourceCode" id="cb37">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="im">import</span> boto3</span>
<span id="cb37-2"><a href="#cb37-2" tabindex="-1"></a></span>
<span id="cb37-3"><a href="#cb37-3" tabindex="-1"></a>train_file <span class="op">=</span> <span class="st">"train_data.npz"</span>  <span class="co"># Local file path in your notebook environment</span></span>
<span id="cb37-4"><a href="#cb37-4" tabindex="-1"></a>val_file <span class="op">=</span> <span class="st">"val_data.npz"</span>  <span class="co"># Local file path in your notebook environment</span></span>
<span id="cb37-5"><a href="#cb37-5" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" tabindex="-1"></a><span class="co"># Initialize the S3 client</span></span>
<span id="cb37-7"><a href="#cb37-7" tabindex="-1"></a>s3 <span class="op">=</span> boto3.client(<span class="st">'s3'</span>)</span>
<span id="cb37-8"><a href="#cb37-8" tabindex="-1"></a></span>
<span id="cb37-9"><a href="#cb37-9" tabindex="-1"></a><span class="co"># Upload the training and validation files to S3</span></span>
<span id="cb37-10"><a href="#cb37-10" tabindex="-1"></a>s3.upload_file(train_file, bucket, <span class="ss">f"</span><span class="sc">{</span>train_file<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-11"><a href="#cb37-11" tabindex="-1"></a>s3.upload_file(val_file, bucket, <span class="ss">f"</span><span class="sc">{</span>val_file<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb37-12"><a href="#cb37-12" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Files successfully uploaded to S3."</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb38">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb38-1"><a href="#cb38-1" tabindex="-1"></a><span class="ex">Files</span> successfully uploaded to S3.</span></code></pre>
</div>
<div class="section level4">
<h4 id="testing-our-train-script-on-notebook-instance">Testing our train script on notebook instance<a class="anchor" aria-label="anchor" href="#testing-our-train-script-on-notebook-instance"></a>
</h4>
<p>You should always test code thoroughly before scaling up and using
more resources. Here, we will test our script using a small number of
epochs — just to verify our setup is correct.</p>
<div class="codewrapper sourceCode" id="cb39">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb39-2"><a href="#cb39-2" tabindex="-1"></a></span>
<span id="cb39-3"><a href="#cb39-3" tabindex="-1"></a><span class="co"># Measure training time locally</span></span>
<span id="cb39-4"><a href="#cb39-4" tabindex="-1"></a>start_time <span class="op">=</span> t.time()</span>
<span id="cb39-5"><a href="#cb39-5" tabindex="-1"></a><span class="op">%</span>run  test_AWS<span class="op">/</span>scripts<span class="op">/</span>train_nn.py <span class="op">--</span>train train_data.npz <span class="op">--</span>val val_data.npz <span class="op">--</span>epochs <span class="dv">1000</span> <span class="op">--</span>learning_rate <span class="fl">0.001</span></span>
<span id="cb39-6"><a href="#cb39-6" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Local training time: </span><span class="sc">{</span>t<span class="sc">.</span>time() <span class="op">-</span> start_time<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type = </span><span class="sc">{</span>local_instance<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
</div>
</div>
<div class="section level3">
<h3 id="deploying-pytorch-neural-network-via-sagemaker">Deploying PyTorch Neural Network via SageMaker<a class="anchor" aria-label="anchor" href="#deploying-pytorch-neural-network-via-sagemaker"></a>
</h3>
<p>Now that we have tested things locally, we can try to train with a
larger number of epochs and a better instance selected. We can do this
easily by invoking the PyTorch estimator. Our notebook is currently
configured to use ml.m5.large. We can upgrade this to
<code>ml.m5.xlarge</code> with the below code (using our notebook as a
controller).</p>
<p><strong>Should we use a GPU?</strong>: Since this dataset is farily
small, we don’t necessarily need a GPU for training. Considering costs,
the m5.xlarge is <code>$0.17/hour</code>, while the cheapest GPU
instance is <code>$0.75/hour</code>. However, for larger datasets (&gt;
1 GB) and models, we may want to consider a GPU if training time becomes
cumbersome (see <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">Instances
for ML</a>. If that doesn’t work, we can try distributed computing
(setting instance &gt; 1). More on this in the next section.</p>
<div class="codewrapper sourceCode" id="cb40">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" tabindex="-1"></a><span class="im">from</span> sagemaker.pytorch <span class="im">import</span> PyTorch</span>
<span id="cb40-2"><a href="#cb40-2" tabindex="-1"></a><span class="im">from</span> sagemaker.inputs <span class="im">import</span> TrainingInput</span>
<span id="cb40-3"><a href="#cb40-3" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb40-5"><a href="#cb40-5" tabindex="-1"></a>instance_count <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb40-6"><a href="#cb40-6" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.large"</span></span>
<span id="cb40-7"><a href="#cb40-7" tabindex="-1"></a>output_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/output_nn/'</span> <span class="co"># this folder will auto-generate if it doesn't exist already</span></span>
<span id="cb40-8"><a href="#cb40-8" tabindex="-1"></a></span>
<span id="cb40-9"><a href="#cb40-9" tabindex="-1"></a><span class="co"># Define the PyTorch estimator and pass hyperparameters as arguments</span></span>
<span id="cb40-10"><a href="#cb40-10" tabindex="-1"></a>pytorch_estimator <span class="op">=</span> PyTorch(</span>
<span id="cb40-11"><a href="#cb40-11" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">"test_AWS/scripts/train_nn.py"</span>,</span>
<span id="cb40-12"><a href="#cb40-12" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb40-13"><a href="#cb40-13" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type, <span class="co"># with this small dataset, we don't recessarily need a GPU for fast training. </span></span>
<span id="cb40-14"><a href="#cb40-14" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,  <span class="co"># Distributed training with two instances</span></span>
<span id="cb40-15"><a href="#cb40-15" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.9"</span>,</span>
<span id="cb40-16"><a href="#cb40-16" tabindex="-1"></a>    py_version<span class="op">=</span><span class="st">"py38"</span>,</span>
<span id="cb40-17"><a href="#cb40-17" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb40-18"><a href="#cb40-18" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb40-19"><a href="#cb40-19" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb40-20"><a href="#cb40-20" tabindex="-1"></a>        <span class="st">"train"</span>: <span class="st">"/opt/ml/input/data/train/train_data.npz"</span>,  <span class="co"># SageMaker will mount this path</span></span>
<span id="cb40-21"><a href="#cb40-21" tabindex="-1"></a>        <span class="st">"val"</span>: <span class="st">"/opt/ml/input/data/val/val_data.npz"</span>,        <span class="co"># SageMaker will mount this path</span></span>
<span id="cb40-22"><a href="#cb40-22" tabindex="-1"></a>        <span class="st">"epochs"</span>: epochs,</span>
<span id="cb40-23"><a href="#cb40-23" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: <span class="fl">0.001</span></span>
<span id="cb40-24"><a href="#cb40-24" tabindex="-1"></a>    }</span>
<span id="cb40-25"><a href="#cb40-25" tabindex="-1"></a>)</span>
<span id="cb40-26"><a href="#cb40-26" tabindex="-1"></a></span>
<span id="cb40-27"><a href="#cb40-27" tabindex="-1"></a><span class="co"># Define input paths</span></span>
<span id="cb40-28"><a href="#cb40-28" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(<span class="ss">f"s3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/train_data.npz"</span>, content_type<span class="op">=</span><span class="st">"application/x-npz"</span>)</span>
<span id="cb40-29"><a href="#cb40-29" tabindex="-1"></a>val_input <span class="op">=</span> TrainingInput(<span class="ss">f"s3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/val_data.npz"</span>, content_type<span class="op">=</span><span class="st">"application/x-npz"</span>)</span>
<span id="cb40-30"><a href="#cb40-30" tabindex="-1"></a></span>
<span id="cb40-31"><a href="#cb40-31" tabindex="-1"></a><span class="co"># Start the training job and time it</span></span>
<span id="cb40-32"><a href="#cb40-32" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb40-33"><a href="#cb40-33" tabindex="-1"></a>pytorch_estimator.fit({<span class="st">"train"</span>: train_input, <span class="st">"val"</span>: val_input})</span>
<span id="cb40-34"><a href="#cb40-34" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb40-35"><a href="#cb40-35" tabindex="-1"></a></span>
<span id="cb40-36"><a href="#cb40-36" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb41">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:27:03 Uploading <span class="at">-</span> Uploading generated training model</span>
<span id="cb41-2"><a href="#cb41-2" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:27:03 Completed <span class="at">-</span> Training job completed</span>
<span id="cb41-3"><a href="#cb41-3" tabindex="-1"></a><span class="ex">Training</span> seconds: 135</span>
<span id="cb41-4"><a href="#cb41-4" tabindex="-1"></a><span class="ex">Billable</span> seconds: 135</span>
<span id="cb41-5"><a href="#cb41-5" tabindex="-1"></a><span class="ex">Runtime</span> for training on SageMaker: 197.62 seconds, instance_type: ml.m5.large, instance_count: 1</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="deploying-pytorch-neural-network-via-sagemaker-with-a-gpu-instance">Deploying PyTorch Neural Network via SageMaker with a GPU
Instance<a class="anchor" aria-label="anchor" href="#deploying-pytorch-neural-network-via-sagemaker-with-a-gpu-instance"></a>
</h3>
<p>In this section, we’ll implement the same procedure as above, but
using a GPU-enabled instance for potentially faster training. While GPU
instances are more expensive, they can be cost-effective for larger
datasets or more complex models that require significant computational
power.</p>
<div class="section level4">
<h4 id="selecting-a-gpu-instance">Selecting a GPU Instance<a class="anchor" aria-label="anchor" href="#selecting-a-gpu-instance"></a>
</h4>
<p>For a small dataset like ours, we don’t strictly need a GPU, but for
larger datasets or more complex models, a GPU can reduce training time.
Here, we’ll select an <code>ml.g4dn.xlarge</code> instance, which
provides a single GPU and costs approximately <code>$0.75/hour</code>
(check <a href="https://docs.google.com/spreadsheets/d/1uPT4ZAYl_onIl7zIjv5oEAdwy4Hdn6eiA9wVfOBbHmY/edit?usp=sharing" class="external-link">Instances
for ML</a> for detailed pricing).</p>
</div>
<div class="section level4">
<h4 id="code-modifications-for-gpu-use">Code Modifications for GPU Use<a class="anchor" aria-label="anchor" href="#code-modifications-for-gpu-use"></a>
</h4>
<p>Using a GPU requires minor changes in your training script
(<code>train_nn.py</code>). Specifically, you’ll need to: 1. Check for
GPU availability in PyTorch. 2. Move the model and tensors to the GPU
device if available.</p>
</div>
<div class="section level4">
<h4 id="enabling-pytorch-to-use-gpu-in-train_nn-py">Enabling PyTorch to use GPU in <code>train_nn.py</code>
<a class="anchor" aria-label="anchor" href="#enabling-pytorch-to-use-gpu-in-train_nn-py"></a>
</h4>
<p>The following code snippet to enables GPU support in
<code>train_nn.py</code>:</p>
<div class="codewrapper sourceCode" id="cb42">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb42-2"><a href="#cb42-2" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" tabindex="-1"></a><span class="co"># Set device</span></span>
<span id="cb42-4"><a href="#cb42-4" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb43">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" tabindex="-1"></a><span class="im">from</span> sagemaker.pytorch <span class="im">import</span> PyTorch</span>
<span id="cb43-2"><a href="#cb43-2" tabindex="-1"></a><span class="im">from</span> sagemaker.inputs <span class="im">import</span> TrainingInput</span>
<span id="cb43-3"><a href="#cb43-3" tabindex="-1"></a><span class="im">import</span> time <span class="im">as</span> t</span>
<span id="cb43-4"><a href="#cb43-4" tabindex="-1"></a></span>
<span id="cb43-5"><a href="#cb43-5" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb43-6"><a href="#cb43-6" tabindex="-1"></a>instance_count <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb43-7"><a href="#cb43-7" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.g4dn.xlarge"</span></span>
<span id="cb43-8"><a href="#cb43-8" tabindex="-1"></a>output_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/output_nn/'</span></span>
<span id="cb43-9"><a href="#cb43-9" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" tabindex="-1"></a><span class="co"># Define the PyTorch estimator and pass hyperparameters as arguments</span></span>
<span id="cb43-11"><a href="#cb43-11" tabindex="-1"></a>pytorch_estimator_gpu <span class="op">=</span> PyTorch(</span>
<span id="cb43-12"><a href="#cb43-12" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">"test_AWS/scripts/train_nn.py"</span>,</span>
<span id="cb43-13"><a href="#cb43-13" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb43-14"><a href="#cb43-14" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type,</span>
<span id="cb43-15"><a href="#cb43-15" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,</span>
<span id="cb43-16"><a href="#cb43-16" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.9"</span>,</span>
<span id="cb43-17"><a href="#cb43-17" tabindex="-1"></a>    py_version<span class="op">=</span><span class="st">"py38"</span>,</span>
<span id="cb43-18"><a href="#cb43-18" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb43-19"><a href="#cb43-19" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb43-20"><a href="#cb43-20" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb43-21"><a href="#cb43-21" tabindex="-1"></a>        <span class="st">"train"</span>: <span class="st">"/opt/ml/input/data/train/train_data.npz"</span>,</span>
<span id="cb43-22"><a href="#cb43-22" tabindex="-1"></a>        <span class="st">"val"</span>: <span class="st">"/opt/ml/input/data/val/val_data.npz"</span>,</span>
<span id="cb43-23"><a href="#cb43-23" tabindex="-1"></a>        <span class="st">"epochs"</span>: epochs,</span>
<span id="cb43-24"><a href="#cb43-24" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: <span class="fl">0.001</span></span>
<span id="cb43-25"><a href="#cb43-25" tabindex="-1"></a>    }</span>
<span id="cb43-26"><a href="#cb43-26" tabindex="-1"></a>)</span>
<span id="cb43-27"><a href="#cb43-27" tabindex="-1"></a></span>
<span id="cb43-28"><a href="#cb43-28" tabindex="-1"></a><span class="co"># Define input paths</span></span>
<span id="cb43-29"><a href="#cb43-29" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(<span class="ss">f"s3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/train_data.npz"</span>, content_type<span class="op">=</span><span class="st">"application/x-npz"</span>)</span>
<span id="cb43-30"><a href="#cb43-30" tabindex="-1"></a>val_input <span class="op">=</span> TrainingInput(<span class="ss">f"s3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/val_data.npz"</span>, content_type<span class="op">=</span><span class="st">"application/x-npz"</span>)</span>
<span id="cb43-31"><a href="#cb43-31" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" tabindex="-1"></a><span class="co"># Start the training job and time it</span></span>
<span id="cb43-33"><a href="#cb43-33" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb43-34"><a href="#cb43-34" tabindex="-1"></a>pytorch_estimator_gpu.fit({<span class="st">"train"</span>: train_input, <span class="st">"val"</span>: val_input})</span>
<span id="cb43-35"><a href="#cb43-35" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb43-36"><a href="#cb43-36" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb44">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:33:56 Uploading <span class="at">-</span> Uploading generated training model</span>
<span id="cb44-2"><a href="#cb44-2" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:33:56 Completed <span class="at">-</span> Training job completed</span>
<span id="cb44-3"><a href="#cb44-3" tabindex="-1"></a><span class="ex">Training</span> seconds: 350</span>
<span id="cb44-4"><a href="#cb44-4" tabindex="-1"></a><span class="ex">Billable</span> seconds: 350</span>
<span id="cb44-5"><a href="#cb44-5" tabindex="-1"></a><span class="ex">Runtime</span> for training on SageMaker: 409.68 seconds, instance_type: ml.g4dn.xlarge, instance_count: 1</span></code></pre>
</div>
</div>
<div class="section level4">
<h4 id="gpus-can-be-slow-for-small-datasetsmodels">GPUs can be slow for small datasets/models<a class="anchor" aria-label="anchor" href="#gpus-can-be-slow-for-small-datasetsmodels"></a>
</h4>
<blockquote>
<p>This performance discrepancy might be due to the following
factors:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Small Dataset/Model Size</strong>: When datasets and
models are small, the overhead of transferring data between the CPU and
GPU, as well as managing the GPU, can actually slow things down. For
very small models and datasets, CPUs are often faster since there’s
minimal data to process.</p></li>
<li><p><strong>GPU Initialization Overhead</strong>: Every time a
training job starts on a GPU, there’s a small overhead for initializing
CUDA libraries. For short jobs, this setup time can make the GPU appear
slower overall.</p></li>
<li><p><strong>Batch Size</strong>: GPUs perform best with larger batch
sizes since they can process many data points in parallel. If the batch
size is too small, the GPU is underutilized, leading to suboptimal
performance. You may want to try increasing the batch size to see if
this reduces training time.</p></li>
<li><p><strong>Instance Type</strong>: Some GPU instances, like the
<code>ml.g4dn</code> series, have less computational power than the
larger <code>p3</code> series. They’re better suited for inference or
lightweight tasks rather than intense training, so a more powerful
instance (e.g., <code>ml.p3.2xlarge</code>) could help for larger
tasks.</p></li>
</ol>
<p>If training time continues to be critical, sticking with a CPU
instance may be the best approach for smaller datasets. For larger, more
complex models and datasets, the GPU’s advantages should become more
apparent.</p>
</blockquote>
</div>
</div>
<div class="section level3">
<h3 id="distributed-training-for-neural-networks-in-sagemaker">Distributed Training for Neural Networks in SageMaker<a class="anchor" aria-label="anchor" href="#distributed-training-for-neural-networks-in-sagemaker"></a>
</h3>
<p>In the event that you do need distributed computing to achieve
reasonable train times (remember to try an upgraded instance first!),
simply adjust the instance count to a number between 2 and 5. Beyond 5
instances, you’ll see diminishing returns and may be needlessly spending
extra money/compute-energy.</p>
<div class="codewrapper sourceCode" id="cb45">
<h3 class="code-label">PYTHON<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode python" tabindex="0"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" tabindex="-1"></a><span class="im">from</span> sagemaker.pytorch <span class="im">import</span> PyTorch</span>
<span id="cb45-2"><a href="#cb45-2" tabindex="-1"></a><span class="im">from</span> sagemaker.inputs <span class="im">import</span> TrainingInput</span>
<span id="cb45-3"><a href="#cb45-3" tabindex="-1"></a><span class="im">import</span> time <span class="im">as</span> t</span>
<span id="cb45-4"><a href="#cb45-4" tabindex="-1"></a></span>
<span id="cb45-5"><a href="#cb45-5" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb45-6"><a href="#cb45-6" tabindex="-1"></a>instance_count <span class="op">=</span> <span class="dv">2</span> <span class="co"># increasing to 2 to see if it has any benefit (likely won't see any with this small dataset)</span></span>
<span id="cb45-7"><a href="#cb45-7" tabindex="-1"></a>instance_type<span class="op">=</span><span class="st">"ml.m5.xlarge"</span></span>
<span id="cb45-8"><a href="#cb45-8" tabindex="-1"></a>output_path <span class="op">=</span> <span class="ss">f's3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/output_nn/'</span></span>
<span id="cb45-9"><a href="#cb45-9" tabindex="-1"></a></span>
<span id="cb45-10"><a href="#cb45-10" tabindex="-1"></a><span class="co"># Define the PyTorch estimator and pass hyperparameters as arguments</span></span>
<span id="cb45-11"><a href="#cb45-11" tabindex="-1"></a>pytorch_estimator <span class="op">=</span> PyTorch(</span>
<span id="cb45-12"><a href="#cb45-12" tabindex="-1"></a>    entry_point<span class="op">=</span><span class="st">"test_AWS/scripts/train_nn.py"</span>,</span>
<span id="cb45-13"><a href="#cb45-13" tabindex="-1"></a>    role<span class="op">=</span>role,</span>
<span id="cb45-14"><a href="#cb45-14" tabindex="-1"></a>    instance_type<span class="op">=</span>instance_type, <span class="co"># with this small dataset, we don't recessarily need a GPU for fast training. </span></span>
<span id="cb45-15"><a href="#cb45-15" tabindex="-1"></a>    instance_count<span class="op">=</span>instance_count,  <span class="co"># Distributed training with two instances</span></span>
<span id="cb45-16"><a href="#cb45-16" tabindex="-1"></a>    framework_version<span class="op">=</span><span class="st">"1.9"</span>,</span>
<span id="cb45-17"><a href="#cb45-17" tabindex="-1"></a>    py_version<span class="op">=</span><span class="st">"py38"</span>,</span>
<span id="cb45-18"><a href="#cb45-18" tabindex="-1"></a>    output_path<span class="op">=</span>output_path,</span>
<span id="cb45-19"><a href="#cb45-19" tabindex="-1"></a>    sagemaker_session<span class="op">=</span>session,</span>
<span id="cb45-20"><a href="#cb45-20" tabindex="-1"></a>    hyperparameters<span class="op">=</span>{</span>
<span id="cb45-21"><a href="#cb45-21" tabindex="-1"></a>        <span class="st">"train"</span>: <span class="st">"/opt/ml/input/data/train/train_data.npz"</span>,  <span class="co"># SageMaker will mount this path</span></span>
<span id="cb45-22"><a href="#cb45-22" tabindex="-1"></a>        <span class="st">"val"</span>: <span class="st">"/opt/ml/input/data/val/val_data.npz"</span>,        <span class="co"># SageMaker will mount this path</span></span>
<span id="cb45-23"><a href="#cb45-23" tabindex="-1"></a>        <span class="st">"epochs"</span>: epochs,</span>
<span id="cb45-24"><a href="#cb45-24" tabindex="-1"></a>        <span class="st">"learning_rate"</span>: <span class="fl">0.001</span></span>
<span id="cb45-25"><a href="#cb45-25" tabindex="-1"></a>    }</span>
<span id="cb45-26"><a href="#cb45-26" tabindex="-1"></a>)</span>
<span id="cb45-27"><a href="#cb45-27" tabindex="-1"></a></span>
<span id="cb45-28"><a href="#cb45-28" tabindex="-1"></a><span class="co"># Define input paths</span></span>
<span id="cb45-29"><a href="#cb45-29" tabindex="-1"></a>train_input <span class="op">=</span> TrainingInput(<span class="ss">f"s3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/train_data.npz"</span>, content_type<span class="op">=</span><span class="st">"application/x-npz"</span>)</span>
<span id="cb45-30"><a href="#cb45-30" tabindex="-1"></a>val_input <span class="op">=</span> TrainingInput(<span class="ss">f"s3://</span><span class="sc">{</span>bucket<span class="sc">}</span><span class="ss">/val_data.npz"</span>, content_type<span class="op">=</span><span class="st">"application/x-npz"</span>)</span>
<span id="cb45-31"><a href="#cb45-31" tabindex="-1"></a></span>
<span id="cb45-32"><a href="#cb45-32" tabindex="-1"></a><span class="co"># Start the training job and time it</span></span>
<span id="cb45-33"><a href="#cb45-33" tabindex="-1"></a>start <span class="op">=</span> t.time()</span>
<span id="cb45-34"><a href="#cb45-34" tabindex="-1"></a>pytorch_estimator.fit({<span class="st">"train"</span>: train_input, <span class="st">"val"</span>: val_input})</span>
<span id="cb45-35"><a href="#cb45-35" tabindex="-1"></a>end <span class="op">=</span> t.time()</span>
<span id="cb45-36"><a href="#cb45-36" tabindex="-1"></a></span>
<span id="cb45-37"><a href="#cb45-37" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Runtime for training on SageMaker: </span><span class="sc">{</span>end <span class="op">-</span> start<span class="sc">:.2f}</span><span class="ss"> seconds, instance_type: </span><span class="sc">{</span>instance_type<span class="sc">}</span><span class="ss">, instance_count: </span><span class="sc">{</span>instance_count<span class="sc">}</span><span class="ss">"</span>)</span></code></pre>
</div>
<div class="codewrapper sourceCode" id="cb46">
<h3 class="code-label">SH<i aria-hidden="true" data-feather="chevron-left"></i><i aria-hidden="true" data-feather="chevron-right"></i>
</h3>
<pre class="sourceCode sh" tabindex="0"><code class="sourceCode bash"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:36:35 Uploading <span class="at">-</span> Uploading generated training model</span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a><span class="ex">2024-11-03</span> 21:36:47 Completed <span class="at">-</span> Training job completed</span>
<span id="cb46-3"><a href="#cb46-3" tabindex="-1"></a><span class="ex">Training</span> seconds: 228</span>
<span id="cb46-4"><a href="#cb46-4" tabindex="-1"></a><span class="ex">Billable</span> seconds: 228</span>
<span id="cb46-5"><a href="#cb46-5" tabindex="-1"></a><span class="ex">Runtime</span> for training on SageMaker: 198.36 seconds, instance_type: ml.m5.xlarge, instance_count: 2</span></code></pre>
</div>
</div>
<div class="section level3">
<h3 id="distributed-training-for-neural-networks-in-sagemaker-understanding-training-strategies-and-how-epochs-are-managed">Distributed Training for Neural Networks in SageMaker: Understanding
Training Strategies and How Epochs Are Managed<a class="anchor" aria-label="anchor" href="#distributed-training-for-neural-networks-in-sagemaker-understanding-training-strategies-and-how-epochs-are-managed"></a>
</h3>
<p>Amazon SageMaker provides two main strategies for distributed
training: <strong>data parallelism</strong> and <strong>model
parallelism</strong>. Understanding which strategy will be used depends
on the model size and the configuration of your SageMaker training job,
as well as the default settings of the specific SageMaker Estimator you
are using.</p>
<div class="section level4">
<h4 id="data-parallelism-most-common-for-mini-batch-sgd">1. <strong>Data Parallelism (Most Common for Mini-batch
SGD)</strong>
<a class="anchor" aria-label="anchor" href="#data-parallelism-most-common-for-mini-batch-sgd"></a>
</h4>
<ul>
<li>
<strong>How it Works</strong>: In data parallelism, each instance in
the cluster (e.g., multiple <code>ml.m5.xlarge</code> instances)
maintains a <strong>complete copy of the model</strong>. The
<strong>training dataset is split across instances</strong>, and each
instance processes a different subset of data simultaneously. This
enables multiple instances to complete forward and backward passes on
different data batches independently.</li>
<li>
<strong>Epoch Distribution</strong>: Even though each instance
processes all the specified epochs, they only work on a portion of the
dataset for each epoch. After each batch, instances synchronize their
gradient updates across all instances using a method such as
<em>all-reduce</em>. This ensures that while each instance is working
with a unique data batch, the model weights remain consistent across
instances.</li>
<li>
<strong>Key Insight</strong>: Because all instances process the
specified number of epochs and synchronize weight updates between
batches, each instance’s training contributes to a cohesive, shared
model. The <strong>effective epoch count across instances appears to be
shared</strong> because data parallelism allows each instance to handle
a fraction of the data per epoch, not the epochs themselves. Data
parallelism is well-suited for models that can fit into a single
instance’s memory and benefit from increased data throughput.</li>
</ul>
</div>
<div class="section level4">
<h4 id="model-parallelism-best-for-large-models">2. <strong>Model Parallelism (Best for Large Models)</strong>
<a class="anchor" aria-label="anchor" href="#model-parallelism-best-for-large-models"></a>
</h4>
<ul>
<li>
<strong>How it Works</strong>: Model parallelism divides the model
itself across multiple instances, not the data. This approach is best
suited for very large models that cannot fit into a single GPU or
instance’s memory (e.g., large language models).</li>
<li>
<strong>Epoch Distribution</strong>: The model is partitioned so
that each instance is responsible for specific layers or components.
Data flows sequentially through these partitions, where each instance
processes a part of each batch and passes it to the next instance.</li>
<li>
<strong>Key Insight</strong>: This approach is more complex due to
the dependency between model components, so <strong>synchronization
occurs across the model layers rather than across data batches</strong>.
Model parallelism generally suits scenarios with exceptionally large
model architectures that exceed memory limits of typical instances.</li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="determining-which-distributed-training-strategy-is-used">Determining Which Distributed Training Strategy is Used<a class="anchor" aria-label="anchor" href="#determining-which-distributed-training-strategy-is-used"></a>
</h3>
<p>SageMaker will select the distributed strategy based on: -
<strong>Framework and Estimator Configuration</strong>: Most deep
learning frameworks in SageMaker default to data parallelism, especially
when using PyTorch or TensorFlow with standard configurations. -
<strong>Model and Data Size</strong>: If you specify a model that
exceeds a single instance’s memory capacity, SageMaker may switch to
model parallelism if configured for it. - <strong>Instance
Count</strong>: When you specify <code>instance_count &gt; 1</code> in
your Estimator with a deep learning model, SageMaker will use data
parallelism by default unless explicitly configured for model
parallelism.</p>
<p>You observed that each instance ran all epochs with
<code>instance_count=2</code> and 10,000 epochs, which aligns with data
parallelism. Here, each instance processed the full set of epochs
independently, but each batch of data was different, and the gradient
updates were synchronized across instances.</p>
</div>
<div class="section level3">
<h3 id="summary-of-key-points">Summary of Key Points<a class="anchor" aria-label="anchor" href="#summary-of-key-points"></a>
</h3>
<ul>
<li>
<strong>Data Parallelism</strong> is the default distributed
training strategy and splits the dataset across instances, allowing each
instance to work on different data batches.
<ul>
<li>Each instance runs all specified epochs, but the weight updates are
synchronized, so <strong>epoch workload is shared across the
data</strong> rather than by reducing epoch count per instance.</li>
</ul>
</li>
<li>
<strong>Model Parallelism</strong> splits the model itself across
instances, typically only needed for very large models that exceed the
memory capacity of single instances.</li>
<li>
<strong>Choosing Between Distributed Strategies</strong>: Data
parallelism is suitable for most neural network models, especially those
that fit in memory, while model parallelism is intended for
exceptionally large models with memory constraints.</li>
</ul>
<p>For cost optimization: - <strong>Single-instance training</strong> is
typically more cost-effective for small or moderately sized datasets,
while <strong>multi-instance setups</strong> can reduce wall-clock time
for larger datasets and complex models, at a higher instance cost. - For
<strong>initial testing</strong>, start with data parallelism on a
single instance, and increase instance count if training time becomes
prohibitive, while being mindful of communication overhead and scaling
efficiency.</p>
<div id="keypoints1" class="callout keypoints">
<div class="callout-square">
<i class="callout-icon" data-feather="key"></i>
</div>
<div class="callout-inner">
<h3 class="callout-title">Key Points</h3>
<div class="callout-content">
<ul>
<li>
<strong>Environment Initialization</strong>: Setting up a SageMaker
session, defining roles, and specifying the S3 bucket are essential for
managing data and running jobs in SageMaker.</li>
<li>
<strong>Local vs. Managed Training</strong>: Local training in
SageMaker notebooks can be useful for quick tests but lacks the
scalability and resource management available in SageMaker-managed
training.</li>
<li>
<strong>Estimator Classes</strong>: SageMaker provides
framework-specific Estimator classes (e.g., XGBoost, PyTorch, SKLearn)
to streamline training setups, each suited to different model types and
workflows.</li>
<li>
<strong>Custom Scripts vs. Built-in Images</strong>: Custom training
scripts offer flexibility with preprocessing and custom logic, while
built-in images are optimized for rapid deployment and simpler
setups.</li>
<li>
<strong>Training Data Channels</strong>: Using
<code>TrainingInput</code> ensures SageMaker manages data efficiently,
especially for distributed setups where data needs to be synchronized
across multiple instances.</li>
<li>
<strong>Distributed Training Options</strong>: Data parallelism
(splitting data across instances) is common for many models, while model
parallelism (splitting the model across instances) is useful for very
large models that exceed instance memory.</li>
</ul>
</div>
</div>
</div>
<!--
Place links that you need to refer to multiple times across pages here. Delete
any links that you are not going to use.
 -->
</div>
</section></section>
</div>
    </main>
</div>
<!-- END  : inst/pkgdown/templates/content-extra.html -->

      </div>
<!--/div.row-->
      		<footer class="row footer mx-md-3"><hr>
<div class="col-md-6">
        <p>This lesson is subject to the <a href="CODE_OF_CONDUCT.html">Code of Conduct</a></p>
        <p>

        <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/edit/main/README.md" class="external-link">Edit on GitHub</a>

	
        | <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/blob/main/CONTRIBUTING.md" class="external-link">Contributing</a>
        | <a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/" class="external-link">Source</a></p>
				<p><a href="https://github.com/UW-Madison-DataScience/ml-with-aws-sagemaker/blob/main/CITATION.cff" class="external-link">Cite</a> | <a href="mailto:endemann@wisc.edu">Contact</a> | <a href="https://carpentries.org/about/" class="external-link">About</a></p>
			</div>
			<div class="col-md-6">

        <p>Materials licensed under <a href="LICENSE.html">CC-BY 4.0</a> by the authors</p>

        <p>Template licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/" class="external-link">CC-BY 4.0</a> by <a href="https://carpentries.org/" class="external-link">The Carpentries</a></p>
        <p>Built with <a href="https://github.com/carpentries/sandpaper/tree/0.16.9" class="external-link">sandpaper (0.16.9)</a>, <a href="https://github.com/carpentries/pegboard/tree/0.7.6" class="external-link">pegboard (0.7.6)</a>, and <a href="https://github.com/carpentries/varnish/tree/1.0.4" class="external-link">varnish (1.0.4)</a></p>
			</div>
		</footer>
</div> <!-- / div.container -->
	<div id="to-top">
		<a href="#top">
      <i class="search-icon" data-feather="arrow-up" role="img" aria-label="Back To Top"></i><br><!-- <span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top --><span class="d-none d-sm-none d-md-none d-lg-none d-xl-block">Back</span> To Top
		</a>
	</div>
  <script type="application/ld+json">
    {
  "@context": "https://schema.org",
  "@type": "TrainingMaterial",
  "@id": "https://UW-Madison-DataScience.github.io/ml-with-aws-sagemaker/instructor/aio.html",
  "inLanguage": "en",
  "dct:conformsTo": "https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE",
  "description": "A Carpentries Lesson teaching foundational data and coding skills to researchers worldwide",
  "keywords": "AWS, SageMaker, Cloud Computing, Machine Learning, AI",
  "name": "All in One View",
  "creativeWorkStatus": "active",
  "url": "https://UW-Madison-DataScience.github.io/ml-with-aws-sagemaker/instructor/aio.html",
  "identifier": "https://UW-Madison-DataScience.github.io/ml-with-aws-sagemaker/instructor/aio.html",
  "dateCreated": "2024-10-31",
  "dateModified": "2024-11-06",
  "datePublished": "2024-11-06"
}

  </script><script>
		feather.replace();
	</script>
</body>
</html><!-- END:   inst/pkgdown/templates/layout.html-->

